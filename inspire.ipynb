{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from DeepPurpose.dataset import *\n",
    "import DTI_inspire as models\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from utils_inspire import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Processing...\n",
      "Beginning to extract zip file...\n",
      "Default set to logspace (nM -> p) for easier regression\n",
      "Done!\n",
      "Beginning to download dataset...\n",
      "Beginning to extract zip file...\n",
      "Done!\n",
      "Loading Dataset from path...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 772572: expected 193 fields, saw 205\\nSkipping line 772598: expected 193 fields, saw 205\\n'\n",
      "b'Skipping line 805291: expected 193 fields, saw 205\\n'\n",
      "b'Skipping line 827961: expected 193 fields, saw 265\\n'\n",
      "b'Skipping line 1231688: expected 193 fields, saw 241\\n'\n",
      "b'Skipping line 1345591: expected 193 fields, saw 241\\nSkipping line 1345592: expected 193 fields, saw 241\\nSkipping line 1345593: expected 193 fields, saw 241\\nSkipping line 1345594: expected 193 fields, saw 241\\nSkipping line 1345595: expected 193 fields, saw 241\\nSkipping line 1345596: expected 193 fields, saw 241\\nSkipping line 1345597: expected 193 fields, saw 241\\nSkipping line 1345598: expected 193 fields, saw 241\\nSkipping line 1345599: expected 193 fields, saw 241\\n'\n",
      "b'Skipping line 1358864: expected 193 fields, saw 205\\n'\n",
      "b'Skipping line 1378087: expected 193 fields, saw 241\\nSkipping line 1378088: expected 193 fields, saw 241\\nSkipping line 1378089: expected 193 fields, saw 241\\nSkipping line 1378090: expected 193 fields, saw 241\\nSkipping line 1378091: expected 193 fields, saw 241\\nSkipping line 1378092: expected 193 fields, saw 241\\nSkipping line 1378093: expected 193 fields, saw 241\\nSkipping line 1378094: expected 193 fields, saw 241\\nSkipping line 1378095: expected 193 fields, saw 241\\n'\n",
      "b'Skipping line 1417264: expected 193 fields, saw 205\\n'\n",
      "/home/william/anaconda3/envs/DeepPurpose/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (8,9,10,11,12,13,15,17,19,20,26,27,31,32,34,35,46,49,50,51,52,53,54,61,62,63,64,65,66,73,74,75,76,77,78,85,86,87,88,89,90,97,98,99,100,101,102,109,110,111,112,113,114,121,122,123,124,125,126,133,134,135,136,137,138,145,147,148,149,150,157,158,159,160,161,162,169,171,172,173,174) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Processing...\n",
      "There are 66444 drug target pairs.\n",
      "Default set to logspace (nM -> p) for easier regression\n"
     ]
    }
   ],
   "source": [
    "X_drug, X_target, y  = load_process_DAVIS(path = './data', binary = False, convert_to_log = True, threshold = 30)\n",
    "\n",
    "if not os.path.isfile('./data/BindingDB_All.tsv'):\n",
    "    bdb_path = download_BindingDB(path = './data')\n",
    "else:\n",
    "    bdb_path = './data/BindingDB_All.tsv'\n",
    "X_drug2, X_target2, y2  = process_BindingDB(path = bdb_path, df = None, y = 'Kd', binary = False, convert_to_log = True, threshold = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_drug2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_drug = np.concatenate((X_drug, X_drug2), axis=0)\n",
    "X_target = np.concatenate((X_target, X_target2), axis=0)\n",
    "y = np.concatenate((y, y2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug Target Interaction Prediction Mode...\n",
      "in total: 96500 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 10733\n",
      "encoding protein...\n",
      "unique target sequence: 1449\n",
      "splitting dataset...\n",
      "Done.\n",
      "First SMILE representation: \n",
      "(array([1124, 1823,  229, 1925,  606,   38,  282,  248,   85,  258,  549,\n",
      "       1860,    6,  248,  248,   85,  333,  621, 1578,  650,  222, 1456,\n",
      "         52,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0]))\n",
      "First target representation: \n",
      "[11 18 18 ...  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "drug_encoding, target_encoding = 'Transformer', 'CNN_inspire'\n",
    "#ensure none of test smiles appears in set\n",
    "train, val, test  = data_process(X_drug, X_target, y, \n",
    "                      drug_encoding, target_encoding, \n",
    "                      split_method='cold_drug',frac=[0.8,0.1,0.1],\n",
    "                      random_seed = 42)\n",
    "# train.head()\n",
    "print(f\"First SMILE representation: \\n{train.drug_encoding.iloc[0]}\")\n",
    "print(f\"First target representation: \\n{train.target_encoding.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_pickle('data/G35_data_train.pkl')\n",
    "val.to_pickle('data/G35_data_vali.pkl')\n",
    "test.to_pickle('data/G35_data_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/ML/DTI/inspire_encoder.py:43: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.emb.weight)\n",
      "/home/william/ML/DTI/inspire_encoder.py:60: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(network[0].weight)\n",
      "/home/william/ML/DTI/inspire_encoder.py:83: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(network[0].weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 1.2491, 1.2353, 0.0000, 1.0360, 1.2484, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1016, 0.0000, 0.0000,\n",
       "         0.0000, 1.2245, 0.0000, 0.0000, 0.0000, 1.2481, 1.2483, 0.0000, 0.0000,\n",
       "         1.2437, 0.0000, 0.0000, 1.2441, 0.0000, 0.0000, 0.0000, 0.0000, 1.1707,\n",
       "         0.0000, 0.0000, 0.0000, 0.5771, 0.0000, 1.2461, 1.2488, 0.0000, 0.0000,\n",
       "         1.2463, 1.2486, 1.2452, 0.0000, 0.0000, 0.0000, 0.0000, 1.2427, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 1.2494, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2476, 1.2484, 1.2293, 1.2451,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 1.2426, 0.0000, 1.2285, 0.0000, 1.2467,\n",
       "         0.0000, 0.0000, 1.2424, 0.0000, 0.0000, 1.2397, 0.0000, 0.0000, 1.2486,\n",
       "         1.2428, 0.0000, 0.0000, 1.1842, 0.0000, 1.2463, 1.2480, 1.2422, 1.2473,\n",
       "         1.2457, 1.2473, 0.0000, 0.0000, 1.2430, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 1.2447, 0.0000, 0.0000, 0.0000, 1.2295, 0.0000,\n",
       "         1.2406, 1.2465, 0.0000, 0.0000, 1.1982, 0.0000, 1.2396, 1.2351, 1.2451,\n",
       "         0.0000, 0.0000],\n",
       "        [1.2485, 1.2490, 0.7668, 0.0000, 0.0000, 1.0547, 0.0000, 0.0000, 0.0000,\n",
       "         1.2451, 1.2326, 1.2363, 1.2486, 0.0000, 1.2385, 0.0000, 1.1914, 1.2450,\n",
       "         1.2455, 0.0000, 1.2486, 1.2481, 0.0000, 0.0000, 0.0000, 1.2444, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 1.2402, 1.0291, 1.2100, 0.0000, 0.0000,\n",
       "         1.2280, 0.0000, 1.2383, 0.0000, 1.1786, 0.0000, 0.0000, 0.0000, 1.2463,\n",
       "         0.0000, 0.0000, 0.0000, 1.2476, 1.2458, 1.2246, 0.0000, 0.0000, 1.2478,\n",
       "         1.2321, 0.0000, 0.0000, 1.2437, 0.0000, 0.0000, 1.2292, 1.2487, 1.2485,\n",
       "         0.0000, 1.1918, 1.2395, 1.2251, 1.2420, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         1.2481, 0.0000, 1.2101, 1.2489, 0.0000, 0.0000, 0.0000, 0.7085, 0.0000,\n",
       "         0.5646, 1.2311, 0.0000, 1.1889, 1.2281, 0.0000, 0.9098, 1.2472, 0.0000,\n",
       "         0.0000, 1.2370, 1.2119, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 1.2491, 0.0000, 1.2399, 1.2492, 1.2436, 1.2099,\n",
       "         1.2162, 1.2486, 1.2472, 0.0000, 1.2436, 1.2490, 1.2450, 0.0000, 1.2452,\n",
       "         0.0000, 0.0000, 1.2352, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 1.2496]], device='cuda:0', grad_fn=<FusedDropoutBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspire_encoder import InspireEncoder\n",
    "config = {'inspire_activation': 'relu',\n",
    "            'CNN_inspire_filters': 128,\n",
    "            'protein_strides': [10, 15, 20, 25, 30],\n",
    "            'inspire_dropout': 0.2,\n",
    "            'protein_layers': [128] }\n",
    "model = InspireEncoder(**config)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "model(torch.from_numpy(np.vstack(train.target_encoding.values[0:2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = generate_config(drug_encoding = drug_encoding, \n",
    "                         target_encoding = target_encoding, \n",
    "                         cls_hidden_dims = [1024,1024,512],  \n",
    "                         train_epoch = 100, \n",
    "                         LR = 0.0001, \n",
    "                         batch_size = 32,\n",
    "                         inspire_activation = 'elu',\n",
    "                         CNN_inspire_filters = 128,\n",
    "                         protein_strides = [10, 15, 20, 25, 30],\n",
    "                         inspire_dropout =  0,\n",
    "                         protein_layers =  [128],\n",
    "                         transformer_emb_size_drug = 128,\n",
    "                         transformer_intermediate_size_drug = 512,\n",
    "                         transformer_num_attention_heads_drug = 8,\n",
    "                         transformer_n_layer_drug = 8,\n",
    "                         transformer_dropout_rate = 0.1,\n",
    "                         transformer_attention_probs_dropout = 0.1,\n",
    "                         transformer_hidden_dropout_rate = 0.1,\n",
    "                         num_workers = 8\n",
    "                        )\n",
    "model = models.model_initialize(**config)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 1 GPU!\n",
      "--- Data Preparation ---\n",
      "--- Go for Training ---\n",
      "Training at Epoch 1 iteration 0 with loss 32.6424. Total time 0.0 hours\n",
      "Training at Epoch 1 iteration 100 with loss 1.37346. Total time 0.00277 hours\n",
      "Training at Epoch 1 iteration 200 with loss 1.01125. Total time 0.00555 hours\n",
      "Training at Epoch 1 iteration 300 with loss 1.10193. Total time 0.00833 hours\n",
      "Training at Epoch 1 iteration 400 with loss 1.40587. Total time 0.01111 hours\n",
      "Training at Epoch 1 iteration 500 with loss 1.56093. Total time 0.01388 hours\n",
      "Training at Epoch 1 iteration 600 with loss 1.38338. Total time 0.01666 hours\n",
      "Training at Epoch 1 iteration 700 with loss 1.03448. Total time 0.01944 hours\n",
      "Training at Epoch 1 iteration 800 with loss 1.25069. Total time 0.02222 hours\n",
      "Training at Epoch 1 iteration 900 with loss 0.80213. Total time 0.025 hours\n",
      "Training at Epoch 1 iteration 1000 with loss 1.51212. Total time 0.0275 hours\n",
      "Training at Epoch 1 iteration 1100 with loss 1.03688. Total time 0.03027 hours\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4686be42c026>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ML/DTI/DTI_inspire.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train, val, test, verbose)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m                                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DeepPurpose/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DeepPurpose/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(train, val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('models/second_trial_run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
