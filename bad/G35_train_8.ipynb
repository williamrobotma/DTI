{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "G35_train_8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lca4TEitJVqn"
      },
      "source": [
        "# Assignment 3 - G35 Training Notebook\n",
        "\n",
        "**Course:** SYSC 4906 - Introduction to Machine Learning\n",
        "\n",
        "**Semester:** Fall 2020\n",
        "\n",
        "**Student Name(s):** William Ma, James Green\n",
        "\n",
        "**Student Number(s):**  101004624, 1010101010\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgMDvWs9JNrx"
      },
      "source": [
        "#DeepPurpose Collab Setup\n",
        "\n",
        "Before implementing and answering the questions below, ensure that these DeepPurpose Collab Setup steps are run correctly to avoid problems later on.\n",
        "\n",
        "**Note:** Second to last step (rdkit installation) requires user input. Enter `y` and press `Enter`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5vIyQpd_RFz"
      },
      "source": [
        "# Run first time on colab, then restart runtime\n",
        "# Must be done because colab uses latest pytorch but DeepPurpose uses 1.4.0 - \n",
        "# could cause compatibility issues when moving pkls between environments\n",
        "# !pip install torch==1.4.0 torchvision==0.5.0"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nX4QXKoH_m8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f66fb2-02f4-4b52-81ff-71e9f66abe0e"
      },
      "source": [
        "!echo $PYTHONPATH"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/env/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5WkEDCXH6Rs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95bf94d0-168b-4c47-d927-7f437471b00a"
      },
      "source": [
        "%env PYTHONPATH="
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: PYTHONPATH=\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53VDj5UEIMBW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38840522-2a1d-48a2-fcc5-54fb858bf3e4"
      },
      "source": [
        "%%bash\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREFIX=/usr/local\n",
            "installing: python-3.6.5-hc3d631a_2 ...\n",
            "installing: ca-certificates-2018.03.07-0 ...\n",
            "installing: conda-env-2.6.0-h36134e3_1 ...\n",
            "installing: libgcc-ng-7.2.0-hdf63c60_3 ...\n",
            "installing: libstdcxx-ng-7.2.0-hdf63c60_3 ...\n",
            "installing: libffi-3.2.1-hd88cf55_4 ...\n",
            "installing: ncurses-6.1-hf484d3e_0 ...\n",
            "installing: openssl-1.0.2o-h20670df_0 ...\n",
            "installing: tk-8.6.7-hc745277_3 ...\n",
            "installing: xz-5.2.4-h14c3975_4 ...\n",
            "installing: yaml-0.1.7-had09818_2 ...\n",
            "installing: zlib-1.2.11-ha838bed_2 ...\n",
            "installing: libedit-3.1.20170329-h6b74fdf_2 ...\n",
            "installing: readline-7.0-ha6073c6_4 ...\n",
            "installing: sqlite-3.23.1-he433501_0 ...\n",
            "installing: asn1crypto-0.24.0-py36_0 ...\n",
            "installing: certifi-2018.4.16-py36_0 ...\n",
            "installing: chardet-3.0.4-py36h0f667ec_1 ...\n",
            "installing: idna-2.6-py36h82fb2a8_1 ...\n",
            "installing: pycosat-0.6.3-py36h0a5515d_0 ...\n",
            "installing: pycparser-2.18-py36hf9f622e_1 ...\n",
            "installing: pysocks-1.6.8-py36_0 ...\n",
            "installing: ruamel_yaml-0.15.37-py36h14c3975_2 ...\n",
            "installing: six-1.11.0-py36h372c433_1 ...\n",
            "installing: cffi-1.11.5-py36h9745a5d_0 ...\n",
            "installing: setuptools-39.2.0-py36_0 ...\n",
            "installing: cryptography-2.2.2-py36h14c3975_0 ...\n",
            "installing: wheel-0.31.1-py36_0 ...\n",
            "installing: pip-10.0.1-py36_0 ...\n",
            "installing: pyopenssl-18.0.0-py36_0 ...\n",
            "installing: urllib3-1.22-py36hbe7ace6_0 ...\n",
            "installing: requests-2.18.4-py36he2e5f8d_1 ...\n",
            "installing: conda-4.5.4-py36_0 ...\n",
            "installation finished.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "--2020-12-09 23:10:37--  https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.200.79, 104.18.201.79, 2606:4700::6812:c84f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.200.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh [following]\n",
            "--2020-12-09 23:10:37--  https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58468498 (56M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-4.5.4-Linux-x86_64.sh’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  0% 49.1M 1s\n",
            "    50K .......... .......... .......... .......... ..........  0% 6.70M 5s\n",
            "   100K .......... .......... .......... .......... ..........  0% 6.82M 6s\n",
            "   150K .......... .......... .......... .......... ..........  0% 6.98M 6s\n",
            "   200K .......... .......... .......... .......... ..........  0% 8.56M 6s\n",
            "   250K .......... .......... .......... .......... ..........  0% 37.2M 6s\n",
            "   300K .......... .......... .......... .......... ..........  0%  249M 5s\n",
            "   350K .......... .......... .......... .......... ..........  0%  102M 4s\n",
            "   400K .......... .......... .......... .......... ..........  0%  183M 4s\n",
            "   450K .......... .......... .......... .......... ..........  0% 10.3M 4s\n",
            "   500K .......... .......... .......... .......... ..........  0%  187M 4s\n",
            "   550K .......... .......... .......... .......... ..........  1% 39.6M 3s\n",
            "   600K .......... .......... .......... .......... ..........  1%  154M 3s\n",
            "   650K .......... .......... .......... .......... ..........  1%  167M 3s\n",
            "   700K .......... .......... .......... .......... ..........  1%  163M 3s\n",
            "   750K .......... .......... .......... .......... ..........  1%  156M 3s\n",
            "   800K .......... .......... .......... .......... ..........  1% 92.8M 3s\n",
            "   850K .......... .......... .......... .......... ..........  1%  210M 2s\n",
            "   900K .......... .......... .......... .......... ..........  1%  123M 2s\n",
            "   950K .......... .......... .......... .......... ..........  1%  117M 2s\n",
            "  1000K .......... .......... .......... .......... ..........  1%  112M 2s\n",
            "  1050K .......... .......... .......... .......... ..........  1%  212M 2s\n",
            "  1100K .......... .......... .......... .......... ..........  2% 19.4M 2s\n",
            "  1150K .......... .......... .......... .......... ..........  2%  125M 2s\n",
            "  1200K .......... .......... .......... .......... ..........  2%  137M 2s\n",
            "  1250K .......... .......... .......... .......... ..........  2% 99.7M 2s\n",
            "  1300K .......... .......... .......... .......... ..........  2%  253M 2s\n",
            "  1350K .......... .......... .......... .......... ..........  2%  110M 2s\n",
            "  1400K .......... .......... .......... .......... ..........  2%  118M 2s\n",
            "  1450K .......... .......... .......... .......... ..........  2%  119M 2s\n",
            "  1500K .......... .......... .......... .......... ..........  2%  227M 2s\n",
            "  1550K .......... .......... .......... .......... ..........  2%  143M 2s\n",
            "  1600K .......... .......... .......... .......... ..........  2%  125M 2s\n",
            "  1650K .......... .......... .......... .......... ..........  2%  138M 2s\n",
            "  1700K .......... .......... .......... .......... ..........  3%  191M 1s\n",
            "  1750K .......... .......... .......... .......... ..........  3%  187M 1s\n",
            "  1800K .......... .......... .......... .......... ..........  3% 86.2M 1s\n",
            "  1850K .......... .......... .......... .......... ..........  3%  262M 1s\n",
            "  1900K .......... .......... .......... .......... ..........  3%  224M 1s\n",
            "  1950K .......... .......... .......... .......... ..........  3%  161M 1s\n",
            "  2000K .......... .......... .......... .......... ..........  3%  172M 1s\n",
            "  2050K .......... .......... .......... .......... ..........  3%  309M 1s\n",
            "  2100K .......... .......... .......... .......... ..........  3%  168M 1s\n",
            "  2150K .......... .......... .......... .......... ..........  3% 33.8M 1s\n",
            "  2200K .......... .......... .......... .......... ..........  3%  175M 1s\n",
            "  2250K .......... .......... .......... .......... ..........  4%  124M 1s\n",
            "  2300K .......... .......... .......... .......... ..........  4%  122M 1s\n",
            "  2350K .......... .......... .......... .......... ..........  4%  101M 1s\n",
            "  2400K .......... .......... .......... .......... ..........  4%  133M 1s\n",
            "  2450K .......... .......... .......... .......... ..........  4%  150M 1s\n",
            "  2500K .......... .......... .......... .......... ..........  4%  141M 1s\n",
            "  2550K .......... .......... .......... .......... ..........  4%  140M 1s\n",
            "  2600K .......... .......... .......... .......... ..........  4%  178M 1s\n",
            "  2650K .......... .......... .......... .......... ..........  4%  148M 1s\n",
            "  2700K .......... .......... .......... .......... ..........  4%  130M 1s\n",
            "  2750K .......... .......... .......... .......... ..........  4%  129M 1s\n",
            "  2800K .......... .......... .......... .......... ..........  4%  147M 1s\n",
            "  2850K .......... .......... .......... .......... ..........  5%  142M 1s\n",
            "  2900K .......... .......... .......... .......... ..........  5%  135M 1s\n",
            "  2950K .......... .......... .......... .......... ..........  5%  139M 1s\n",
            "  3000K .......... .......... .......... .......... ..........  5%  159M 1s\n",
            "  3050K .......... .......... .......... .......... ..........  5%  186M 1s\n",
            "  3100K .......... .......... .......... .......... ..........  5%  166M 1s\n",
            "  3150K .......... .......... .......... .......... ..........  5%  116M 1s\n",
            "  3200K .......... .......... .......... .......... ..........  5%  152M 1s\n",
            "  3250K .......... .......... .......... .......... ..........  5%  167M 1s\n",
            "  3300K .......... .......... .......... .......... ..........  5%  125M 1s\n",
            "  3350K .......... .......... .......... .......... ..........  5%  125M 1s\n",
            "  3400K .......... .......... .......... .......... ..........  6%  123M 1s\n",
            "  3450K .......... .......... .......... .......... ..........  6%  139M 1s\n",
            "  3500K .......... .......... .......... .......... ..........  6%  183M 1s\n",
            "  3550K .......... .......... .......... .......... ..........  6%  109M 1s\n",
            "  3600K .......... .......... .......... .......... ..........  6%  132M 1s\n",
            "  3650K .......... .......... .......... .......... ..........  6%  150M 1s\n",
            "  3700K .......... .......... .......... .......... ..........  6%  154M 1s\n",
            "  3750K .......... .......... .......... .......... ..........  6%  152M 1s\n",
            "  3800K .......... .......... .......... .......... ..........  6%  166M 1s\n",
            "  3850K .......... .......... .......... .......... ..........  6%  166M 1s\n",
            "  3900K .......... .......... .......... .......... ..........  6%  149M 1s\n",
            "  3950K .......... .......... .......... .......... ..........  7%  119M 1s\n",
            "  4000K .......... .......... .......... .......... ..........  7%  145M 1s\n",
            "  4050K .......... .......... .......... .......... ..........  7%  174M 1s\n",
            "  4100K .......... .......... .......... .......... ..........  7%  147M 1s\n",
            "  4150K .......... .......... .......... .......... ..........  7%  134M 1s\n",
            "  4200K .......... .......... .......... .......... ..........  7%  162M 1s\n",
            "  4250K .......... .......... .......... .......... ..........  7%  248M 1s\n",
            "  4300K .......... .......... .......... .......... ..........  7%  299M 1s\n",
            "  4350K .......... .......... .......... .......... ..........  7%  170M 1s\n",
            "  4400K .......... .......... .......... .......... ..........  7%  209M 1s\n",
            "  4450K .......... .......... .......... .......... ..........  7%  216M 1s\n",
            "  4500K .......... .......... .......... .......... ..........  7%  194M 1s\n",
            "  4550K .......... .......... .......... .......... ..........  8%  139M 1s\n",
            "  4600K .......... .......... .......... .......... ..........  8%  148M 1s\n",
            "  4650K .......... .......... .......... .......... ..........  8%  157M 1s\n",
            "  4700K .......... .......... .......... .......... ..........  8%  160M 1s\n",
            "  4750K .......... .......... .......... .......... ..........  8%  136M 1s\n",
            "  4800K .......... .......... .......... .......... ..........  8%  190M 1s\n",
            "  4850K .......... .......... .......... .......... ..........  8%  170M 1s\n",
            "  4900K .......... .......... .......... .......... ..........  8%  217M 1s\n",
            "  4950K .......... .......... .......... .......... ..........  8%  203M 1s\n",
            "  5000K .......... .......... .......... .......... ..........  8%  254M 1s\n",
            "  5050K .......... .......... .......... .......... ..........  8%  191M 1s\n",
            "  5100K .......... .......... .......... .......... ..........  9%  179M 1s\n",
            "  5150K .......... .......... .......... .......... ..........  9%  233M 1s\n",
            "  5200K .......... .......... .......... .......... ..........  9%  262M 1s\n",
            "  5250K .......... .......... .......... .......... ..........  9%  279M 1s\n",
            "  5300K .......... .......... .......... .......... ..........  9%  182M 1s\n",
            "  5350K .......... .......... .......... .......... ..........  9%  246M 1s\n",
            "  5400K .......... .......... .......... .......... ..........  9%  316M 1s\n",
            "  5450K .......... .......... .......... .......... ..........  9%  248M 1s\n",
            "  5500K .......... .......... .......... .......... ..........  9%  303M 1s\n",
            "  5550K .......... .......... .......... .......... ..........  9%  144M 1s\n",
            "  5600K .......... .......... .......... .......... ..........  9%  230M 1s\n",
            "  5650K .......... .......... .......... .......... ..........  9%  265M 1s\n",
            "  5700K .......... .......... .......... .......... .......... 10%  284M 1s\n",
            "  5750K .......... .......... .......... .......... .......... 10%  222M 1s\n",
            "  5800K .......... .......... .......... .......... .......... 10%  145M 1s\n",
            "  5850K .......... .......... .......... .......... .......... 10%  193M 1s\n",
            "  5900K .......... .......... .......... .......... .......... 10%  198M 1s\n",
            "  5950K .......... .......... .......... .......... .......... 10%  146M 1s\n",
            "  6000K .......... .......... .......... .......... .......... 10%  194M 1s\n",
            "  6050K .......... .......... .......... .......... .......... 10%  184M 1s\n",
            "  6100K .......... .......... .......... .......... .......... 10%  205M 1s\n",
            "  6150K .......... .......... .......... .......... .......... 10%  133M 1s\n",
            "  6200K .......... .......... .......... .......... .......... 10%  202M 1s\n",
            "  6250K .......... .......... .......... .......... .......... 11%  299M 1s\n",
            "  6300K .......... .......... .......... .......... .......... 11%  219M 1s\n",
            "  6350K .......... .......... .......... .......... .......... 11%  153M 1s\n",
            "  6400K .......... .......... .......... .......... .......... 11%  325M 1s\n",
            "  6450K .......... .......... .......... .......... .......... 11%  288M 1s\n",
            "  6500K .......... .......... .......... .......... .......... 11%  210M 1s\n",
            "  6550K .......... .......... .......... .......... .......... 11%  159M 1s\n",
            "  6600K .......... .......... .......... .......... .......... 11%  167M 1s\n",
            "  6650K .......... .......... .......... .......... .......... 11%  181M 1s\n",
            "  6700K .......... .......... .......... .......... .......... 11%  182M 1s\n",
            "  6750K .......... .......... .......... .......... .......... 11%  140M 1s\n",
            "  6800K .......... .......... .......... .......... .......... 11%  185M 1s\n",
            "  6850K .......... .......... .......... .......... .......... 12%  181M 1s\n",
            "  6900K .......... .......... .......... .......... .......... 12%  153M 1s\n",
            "  6950K .......... .......... .......... .......... .......... 12%  147M 1s\n",
            "  7000K .......... .......... .......... .......... .......... 12%  203M 1s\n",
            "  7050K .......... .......... .......... .......... .......... 12%  297M 1s\n",
            "  7100K .......... .......... .......... .......... .......... 12%  284M 1s\n",
            "  7150K .......... .......... .......... .......... .......... 12%  156M 1s\n",
            "  7200K .......... .......... .......... .......... .......... 12%  312M 1s\n",
            "  7250K .......... .......... .......... .......... .......... 12%  315M 1s\n",
            "  7300K .......... .......... .......... .......... .......... 12%  314M 1s\n",
            "  7350K .......... .......... .......... .......... .......... 12%  202M 1s\n",
            "  7400K .......... .......... .......... .......... .......... 13%  235M 1s\n",
            "  7450K .......... .......... .......... .......... .......... 13%  260M 1s\n",
            "  7500K .......... .......... .......... .......... .......... 13%  214M 1s\n",
            "  7550K .......... .......... .......... .......... .......... 13%  132M 1s\n",
            "  7600K .......... .......... .......... .......... .......... 13%  151M 1s\n",
            "  7650K .......... .......... .......... .......... .......... 13%  167M 1s\n",
            "  7700K .......... .......... .......... .......... .......... 13%  169M 1s\n",
            "  7750K .......... .......... .......... .......... .......... 13%  140M 1s\n",
            "  7800K .......... .......... .......... .......... .......... 13%  150M 1s\n",
            "  7850K .......... .......... .......... .......... .......... 13%  199M 1s\n",
            "  7900K .......... .......... .......... .......... .......... 13%  190M 1s\n",
            "  7950K .......... .......... .......... .......... .......... 14%  139M 1s\n",
            "  8000K .......... .......... .......... .......... .......... 14%  171M 1s\n",
            "  8050K .......... .......... .......... .......... .......... 14%  182M 1s\n",
            "  8100K .......... .......... .......... .......... .......... 14%  160M 1s\n",
            "  8150K .......... .......... .......... .......... .......... 14%  163M 1s\n",
            "  8200K .......... .......... .......... .......... .......... 14%  191M 1s\n",
            "  8250K .......... .......... .......... .......... .......... 14%  172M 1s\n",
            "  8300K .......... .......... .......... .......... .......... 14%  196M 0s\n",
            "  8350K .......... .......... .......... .......... .......... 14%  135M 0s\n",
            "  8400K .......... .......... .......... .......... .......... 14%  188M 0s\n",
            "  8450K .......... .......... .......... .......... .......... 14%  192M 0s\n",
            "  8500K .......... .......... .......... .......... .......... 14%  282M 0s\n",
            "  8550K .......... .......... .......... .......... .......... 15%  287M 0s\n",
            "  8600K .......... .......... .......... .......... .......... 15%  252M 0s\n",
            "  8650K .......... .......... .......... .......... .......... 15%  262M 0s\n",
            "  8700K .......... .......... .......... .......... .......... 15%  199M 0s\n",
            "  8750K .......... .......... .......... .......... .......... 15%  160M 0s\n",
            "  8800K .......... .......... .......... .......... .......... 15%  190M 0s\n",
            "  8850K .......... .......... .......... .......... .......... 15%  168M 0s\n",
            "  8900K .......... .......... .......... .......... .......... 15%  176M 0s\n",
            "  8950K .......... .......... .......... .......... .......... 15%  151M 0s\n",
            "  9000K .......... .......... .......... .......... .......... 15%  158M 0s\n",
            "  9050K .......... .......... .......... .......... .......... 15%  157M 0s\n",
            "  9100K .......... .......... .......... .......... .......... 16%  166M 0s\n",
            "  9150K .......... .......... .......... .......... .......... 16%  110M 0s\n",
            "  9200K .......... .......... .......... .......... .......... 16%  142M 0s\n",
            "  9250K .......... .......... .......... .......... .......... 16%  149M 0s\n",
            "  9300K .......... .......... .......... .......... .......... 16%  153M 0s\n",
            "  9350K .......... .......... .......... .......... .......... 16%  148M 0s\n",
            "  9400K .......... .......... .......... .......... .......... 16%  153M 0s\n",
            "  9450K .......... .......... .......... .......... .......... 16% 63.0M 0s\n",
            "  9500K .......... .......... .......... .......... .......... 16% 34.1M 0s\n",
            "  9550K .......... .......... .......... .......... .......... 16% 27.7M 0s\n",
            "  9600K .......... .......... .......... .......... .......... 16% 56.4M 0s\n",
            "  9650K .......... .......... .......... .......... .......... 16% 42.4M 0s\n",
            "  9700K .......... .......... .......... .......... .......... 17% 34.5M 0s\n",
            "  9750K .......... .......... .......... .......... .......... 17% 37.3M 0s\n",
            "  9800K .......... .......... .......... .......... .......... 17%  165M 0s\n",
            "  9850K .......... .......... .......... .......... .......... 17%  168M 0s\n",
            "  9900K .......... .......... .......... .......... .......... 17%  153M 0s\n",
            "  9950K .......... .......... .......... .......... .......... 17%  120M 0s\n",
            " 10000K .......... .......... .......... .......... .......... 17%  174M 0s\n",
            " 10050K .......... .......... .......... .......... .......... 17%  121M 0s\n",
            " 10100K .......... .......... .......... .......... .......... 17%  149M 0s\n",
            " 10150K .......... .......... .......... .......... .......... 17%  104M 0s\n",
            " 10200K .......... .......... .......... .......... .......... 17%  150M 0s\n",
            " 10250K .......... .......... .......... .......... .......... 18%  163M 0s\n",
            " 10300K .......... .......... .......... .......... .......... 18%  127M 0s\n",
            " 10350K .......... .......... .......... .......... .......... 18%  134M 0s\n",
            " 10400K .......... .......... .......... .......... .......... 18%  187M 0s\n",
            " 10450K .......... .......... .......... .......... .......... 18%  239M 0s\n",
            " 10500K .......... .......... .......... .......... .......... 18%  167M 0s\n",
            " 10550K .......... .......... .......... .......... .......... 18%  131M 0s\n",
            " 10600K .......... .......... .......... .......... .......... 18%  159M 0s\n",
            " 10650K .......... .......... .......... .......... .......... 18%  151M 0s\n",
            " 10700K .......... .......... .......... .......... .......... 18%  163M 0s\n",
            " 10750K .......... .......... .......... .......... .......... 18%  133M 0s\n",
            " 10800K .......... .......... .......... .......... .......... 19%  176M 0s\n",
            " 10850K .......... .......... .......... .......... .......... 19%  162M 0s\n",
            " 10900K .......... .......... .......... .......... .......... 19%  165M 0s\n",
            " 10950K .......... .......... .......... .......... .......... 19%  132M 0s\n",
            " 11000K .......... .......... .......... .......... .......... 19%  184M 0s\n",
            " 11050K .......... .......... .......... .......... .......... 19%  264M 0s\n",
            " 11100K .......... .......... .......... .......... .......... 19%  250M 0s\n",
            " 11150K .......... .......... .......... .......... .......... 19%  133M 0s\n",
            " 11200K .......... .......... .......... .......... .......... 19%  127M 0s\n",
            " 11250K .......... .......... .......... .......... .......... 19%  153M 0s\n",
            " 11300K .......... .......... .......... .......... .......... 19%  137M 0s\n",
            " 11350K .......... .......... .......... .......... .......... 19%  146M 0s\n",
            " 11400K .......... .......... .......... .......... .......... 20%  154M 0s\n",
            " 11450K .......... .......... .......... .......... .......... 20%  152M 0s\n",
            " 11500K .......... .......... .......... .......... .......... 20%  251M 0s\n",
            " 11550K .......... .......... .......... .......... .......... 20%  187M 0s\n",
            " 11600K .......... .......... .......... .......... .......... 20%  154M 0s\n",
            " 11650K .......... .......... .......... .......... .......... 20%  116M 0s\n",
            " 11700K .......... .......... .......... .......... .......... 20%  151M 0s\n",
            " 11750K .......... .......... .......... .......... .......... 20%  143M 0s\n",
            " 11800K .......... .......... .......... .......... .......... 20%  143M 0s\n",
            " 11850K .......... .......... .......... .......... .......... 20%  175M 0s\n",
            " 11900K .......... .......... .......... .......... .......... 20%  163M 0s\n",
            " 11950K .......... .......... .......... .......... .......... 21%  170M 0s\n",
            " 12000K .......... .......... .......... .......... .......... 21%  261M 0s\n",
            " 12050K .......... .......... .......... .......... .......... 21%  230M 0s\n",
            " 12100K .......... .......... .......... .......... .......... 21%  227M 0s\n",
            " 12150K .......... .......... .......... .......... .......... 21%  153M 0s\n",
            " 12200K .......... .......... .......... .......... .......... 21%  157M 0s\n",
            " 12250K .......... .......... .......... .......... .......... 21%  171M 0s\n",
            " 12300K .......... .......... .......... .......... .......... 21%  159M 0s\n",
            " 12350K .......... .......... .......... .......... .......... 21%  113M 0s\n",
            " 12400K .......... .......... .......... .......... .......... 21%  131M 0s\n",
            " 12450K .......... .......... .......... .......... .......... 21%  160M 0s\n",
            " 12500K .......... .......... .......... .......... .......... 21%  189M 0s\n",
            " 12550K .......... .......... .......... .......... .......... 22%  234M 0s\n",
            " 12600K .......... .......... .......... .......... .......... 22%  236M 0s\n",
            " 12650K .......... .......... .......... .......... .......... 22%  162M 0s\n",
            " 12700K .......... .......... .......... .......... .......... 22%  177M 0s\n",
            " 12750K .......... .......... .......... .......... .......... 22%  118M 0s\n",
            " 12800K .......... .......... .......... .......... .......... 22% 44.3M 0s\n",
            " 12850K .......... .......... .......... .......... .......... 22% 83.1M 0s\n",
            " 12900K .......... .......... .......... .......... .......... 22% 66.1M 0s\n",
            " 12950K .......... .......... .......... .......... .......... 22%  221M 0s\n",
            " 13000K .......... .......... .......... .......... .......... 22%  206M 0s\n",
            " 13050K .......... .......... .......... .......... .......... 22%  226M 0s\n",
            " 13100K .......... .......... .......... .......... .......... 23%  215M 0s\n",
            " 13150K .......... .......... .......... .......... .......... 23%  176M 0s\n",
            " 13200K .......... .......... .......... .......... .......... 23%  171M 0s\n",
            " 13250K .......... .......... .......... .......... .......... 23%  252M 0s\n",
            " 13300K .......... .......... .......... .......... .......... 23%  145M 0s\n",
            " 13350K .......... .......... .......... .......... .......... 23%  119M 0s\n",
            " 13400K .......... .......... .......... .......... .......... 23%  141M 0s\n",
            " 13450K .......... .......... .......... .......... .......... 23%  163M 0s\n",
            " 13500K .......... .......... .......... .......... .......... 23%  167M 0s\n",
            " 13550K .......... .......... .......... .......... .......... 23%  120M 0s\n",
            " 13600K .......... .......... .......... .......... .......... 23%  161M 0s\n",
            " 13650K .......... .......... .......... .......... .......... 23%  157M 0s\n",
            " 13700K .......... .......... .......... .......... .......... 24%  145M 0s\n",
            " 13750K .......... .......... .......... .......... .......... 24%  145M 0s\n",
            " 13800K .......... .......... .......... .......... .......... 24%  164M 0s\n",
            " 13850K .......... .......... .......... .......... .......... 24%  157M 0s\n",
            " 13900K .......... .......... .......... .......... .......... 24%  147M 0s\n",
            " 13950K .......... .......... .......... .......... .......... 24%  136M 0s\n",
            " 14000K .......... .......... .......... .......... .......... 24%  159M 0s\n",
            " 14050K .......... .......... .......... .......... .......... 24%  133M 0s\n",
            " 14100K .......... .......... .......... .......... .......... 24%  138M 0s\n",
            " 14150K .......... .......... .......... .......... .......... 24%  123M 0s\n",
            " 14200K .......... .......... .......... .......... .......... 24%  181M 0s\n",
            " 14250K .......... .......... .......... .......... .......... 25%  266M 0s\n",
            " 14300K .......... .......... .......... .......... .......... 25%  237M 0s\n",
            " 14350K .......... .......... .......... .......... .......... 25%  152M 0s\n",
            " 14400K .......... .......... .......... .......... .......... 25%  156M 0s\n",
            " 14450K .......... .......... .......... .......... .......... 25%  163M 0s\n",
            " 14500K .......... .......... .......... .......... .......... 25%  205M 0s\n",
            " 14550K .......... .......... .......... .......... .......... 25%  152M 0s\n",
            " 14600K .......... .......... .......... .......... .......... 25%  154M 0s\n",
            " 14650K .......... .......... .......... .......... .......... 25%  173M 0s\n",
            " 14700K .......... .......... .......... .......... .......... 25%  134M 0s\n",
            " 14750K .......... .......... .......... .......... .......... 25%  113M 0s\n",
            " 14800K .......... .......... .......... .......... .......... 26%  137M 0s\n",
            " 14850K .......... .......... .......... .......... .......... 26%  152M 0s\n",
            " 14900K .......... .......... .......... .......... .......... 26%  164M 0s\n",
            " 14950K .......... .......... .......... .......... .......... 26%  191M 0s\n",
            " 15000K .......... .......... .......... .......... .......... 26%  177M 0s\n",
            " 15050K .......... .......... .......... .......... .......... 26%  151M 0s\n",
            " 15100K .......... .......... .......... .......... .......... 26%  161M 0s\n",
            " 15150K .......... .......... .......... .......... .......... 26%  128M 0s\n",
            " 15200K .......... .......... .......... .......... .......... 26%  153M 0s\n",
            " 15250K .......... .......... .......... .......... .......... 26%  148M 0s\n",
            " 15300K .......... .......... .......... .......... .......... 26%  182M 0s\n",
            " 15350K .......... .......... .......... .......... .......... 26%  144M 0s\n",
            " 15400K .......... .......... .......... .......... .......... 27%  163M 0s\n",
            " 15450K .......... .......... .......... .......... .......... 27%  141M 0s\n",
            " 15500K .......... .......... .......... .......... .......... 27%  161M 0s\n",
            " 15550K .......... .......... .......... .......... .......... 27%  196M 0s\n",
            " 15600K .......... .......... .......... .......... .......... 27%  262M 0s\n",
            " 15650K .......... .......... .......... .......... .......... 27%  179M 0s\n",
            " 15700K .......... .......... .......... .......... .......... 27%  148M 0s\n",
            " 15750K .......... .......... .......... .......... .......... 27%  228M 0s\n",
            " 15800K .......... .......... .......... .......... .......... 27%  157M 0s\n",
            " 15850K .......... .......... .......... .......... .......... 27%  128M 0s\n",
            " 15900K .......... .......... .......... .......... .......... 27%  134M 0s\n",
            " 15950K .......... .......... .......... .......... .......... 28%  128M 0s\n",
            " 16000K .......... .......... .......... .......... .......... 28%  195M 0s\n",
            " 16050K .......... .......... .......... .......... .......... 28%  237M 0s\n",
            " 16100K .......... .......... .......... .......... .......... 28%  238M 0s\n",
            " 16150K .......... .......... .......... .......... .......... 28%  215M 0s\n",
            " 16200K .......... .......... .......... .......... .......... 28%  150M 0s\n",
            " 16250K .......... .......... .......... .......... .......... 28%  161M 0s\n",
            " 16300K .......... .......... .......... .......... .......... 28%  174M 0s\n",
            " 16350K .......... .......... .......... .......... .......... 28%  189M 0s\n",
            " 16400K .......... .......... .......... .......... .......... 28%  209M 0s\n",
            " 16450K .......... .......... .......... .......... .......... 28%  131M 0s\n",
            " 16500K .......... .......... .......... .......... .......... 28%  220M 0s\n",
            " 16550K .......... .......... .......... .......... .......... 29%  160M 0s\n",
            " 16600K .......... .......... .......... .......... .......... 29%  147M 0s\n",
            " 16650K .......... .......... .......... .......... .......... 29%  219M 0s\n",
            " 16700K .......... .......... .......... .......... .......... 29%  239M 0s\n",
            " 16750K .......... .......... .......... .......... .......... 29%  171M 0s\n",
            " 16800K .......... .......... .......... .......... .......... 29%  258M 0s\n",
            " 16850K .......... .......... .......... .......... .......... 29%  248M 0s\n",
            " 16900K .......... .......... .......... .......... .......... 29%  214M 0s\n",
            " 16950K .......... .......... .......... .......... .......... 29%  120M 0s\n",
            " 17000K .......... .......... .......... .......... .......... 29%  140M 0s\n",
            " 17050K .......... .......... .......... .......... .......... 29%  126M 0s\n",
            " 17100K .......... .......... .......... .......... .......... 30%  144M 0s\n",
            " 17150K .......... .......... .......... .......... .......... 30%  127M 0s\n",
            " 17200K .......... .......... .......... .......... .......... 30%  134M 0s\n",
            " 17250K .......... .......... .......... .......... .......... 30%  158M 0s\n",
            " 17300K .......... .......... .......... .......... .......... 30%  163M 0s\n",
            " 17350K .......... .......... .......... .......... .......... 30%  135M 0s\n",
            " 17400K .......... .......... .......... .......... .......... 30%  158M 0s\n",
            " 17450K .......... .......... .......... .......... .......... 30%  181M 0s\n",
            " 17500K .......... .......... .......... .......... .......... 30%  248M 0s\n",
            " 17550K .......... .......... .......... .......... .......... 30%  160M 0s\n",
            " 17600K .......... .......... .......... .......... .......... 30%  164M 0s\n",
            " 17650K .......... .......... .......... .......... .......... 30%  133M 0s\n",
            " 17700K .......... .......... .......... .......... .......... 31%  147M 0s\n",
            " 17750K .......... .......... .......... .......... .......... 31%  224M 0s\n",
            " 17800K .......... .......... .......... .......... .......... 31%  191M 0s\n",
            " 17850K .......... .......... .......... .......... .......... 31%  189M 0s\n",
            " 17900K .......... .......... .......... .......... .......... 31%  171M 0s\n",
            " 17950K .......... .......... .......... .......... .......... 31%  202M 0s\n",
            " 18000K .......... .......... .......... .......... .......... 31%  227M 0s\n",
            " 18050K .......... .......... .......... .......... .......... 31%  222M 0s\n",
            " 18100K .......... .......... .......... .......... .......... 31%  178M 0s\n",
            " 18150K .......... .......... .......... .......... .......... 31%  145M 0s\n",
            " 18200K .......... .......... .......... .......... .......... 31%  157M 0s\n",
            " 18250K .......... .......... .......... .......... .......... 32%  249M 0s\n",
            " 18300K .......... .......... .......... .......... .......... 32%  144M 0s\n",
            " 18350K .......... .......... .......... .......... .......... 32%  129M 0s\n",
            " 18400K .......... .......... .......... .......... .......... 32%  120M 0s\n",
            " 18450K .......... .......... .......... .......... .......... 32%  156M 0s\n",
            " 18500K .......... .......... .......... .......... .......... 32%  162M 0s\n",
            " 18550K .......... .......... .......... .......... .......... 32%  132M 0s\n",
            " 18600K .......... .......... .......... .......... .......... 32%  156M 0s\n",
            " 18650K .......... .......... .......... .......... .......... 32%  152M 0s\n",
            " 18700K .......... .......... .......... .......... .......... 32%  146M 0s\n",
            " 18750K .......... .......... .......... .......... .......... 32%  134M 0s\n",
            " 18800K .......... .......... .......... .......... .......... 33%  123M 0s\n",
            " 18850K .......... .......... .......... .......... .......... 33%  120M 0s\n",
            " 18900K .......... .......... .......... .......... .......... 33%  145M 0s\n",
            " 18950K .......... .......... .......... .......... .......... 33%  130M 0s\n",
            " 19000K .......... .......... .......... .......... .......... 33%  154M 0s\n",
            " 19050K .......... .......... .......... .......... .......... 33%  164M 0s\n",
            " 19100K .......... .......... .......... .......... .......... 33%  153M 0s\n",
            " 19150K .......... .......... .......... .......... .......... 33%  129M 0s\n",
            " 19200K .......... .......... .......... .......... .......... 33%  178M 0s\n",
            " 19250K .......... .......... .......... .......... .......... 33%  152M 0s\n",
            " 19300K .......... .......... .......... .......... .......... 33%  128M 0s\n",
            " 19350K .......... .......... .......... .......... .......... 33%  143M 0s\n",
            " 19400K .......... .......... .......... .......... .......... 34%  171M 0s\n",
            " 19450K .......... .......... .......... .......... .......... 34%  168M 0s\n",
            " 19500K .......... .......... .......... .......... .......... 34%  176M 0s\n",
            " 19550K .......... .......... .......... .......... .......... 34%  111M 0s\n",
            " 19600K .......... .......... .......... .......... .......... 34%  158M 0s\n",
            " 19650K .......... .......... .......... .......... .......... 34%  248M 0s\n",
            " 19700K .......... .......... .......... .......... .......... 34%  237M 0s\n",
            " 19750K .......... .......... .......... .......... .......... 34%  196M 0s\n",
            " 19800K .......... .......... .......... .......... .......... 34%  186M 0s\n",
            " 19850K .......... .......... .......... .......... .......... 34%  157M 0s\n",
            " 19900K .......... .......... .......... .......... .......... 34%  143M 0s\n",
            " 19950K .......... .......... .......... .......... .......... 35%  118M 0s\n",
            " 20000K .......... .......... .......... .......... .......... 35%  160M 0s\n",
            " 20050K .......... .......... .......... .......... .......... 35%  171M 0s\n",
            " 20100K .......... .......... .......... .......... .......... 35%  181M 0s\n",
            " 20150K .......... .......... .......... .......... .......... 35%  209M 0s\n",
            " 20200K .......... .......... .......... .......... .......... 35%  257M 0s\n",
            " 20250K .......... .......... .......... .......... .......... 35%  246M 0s\n",
            " 20300K .......... .......... .......... .......... .......... 35%  206M 0s\n",
            " 20350K .......... .......... .......... .......... .......... 35%  144M 0s\n",
            " 20400K .......... .......... .......... .......... .......... 35%  129M 0s\n",
            " 20450K .......... .......... .......... .......... .......... 35% 99.6M 0s\n",
            " 20500K .......... .......... .......... .......... .......... 35%  239M 0s\n",
            " 20550K .......... .......... .......... .......... .......... 36%  238M 0s\n",
            " 20600K .......... .......... .......... .......... .......... 36%  302M 0s\n",
            " 20650K .......... .......... .......... .......... .......... 36%  314M 0s\n",
            " 20700K .......... .......... .......... .......... .......... 36%  247M 0s\n",
            " 20750K .......... .......... .......... .......... .......... 36%  164M 0s\n",
            " 20800K .......... .......... .......... .......... .......... 36%  203M 0s\n",
            " 20850K .......... .......... .......... .......... .......... 36%  177M 0s\n",
            " 20900K .......... .......... .......... .......... .......... 36%  175M 0s\n",
            " 20950K .......... .......... .......... .......... .......... 36%  213M 0s\n",
            " 21000K .......... .......... .......... .......... .......... 36%  308M 0s\n",
            " 21050K .......... .......... .......... .......... .......... 36%  237M 0s\n",
            " 21100K .......... .......... .......... .......... .......... 37%  175M 0s\n",
            " 21150K .......... .......... .......... .......... .......... 37%  121M 0s\n",
            " 21200K .......... .......... .......... .......... .......... 37%  193M 0s\n",
            " 21250K .......... .......... .......... .......... .......... 37%  188M 0s\n",
            " 21300K .......... .......... .......... .......... .......... 37%  179M 0s\n",
            " 21350K .......... .......... .......... .......... .......... 37%  168M 0s\n",
            " 21400K .......... .......... .......... .......... .......... 37%  203M 0s\n",
            " 21450K .......... .......... .......... .......... .......... 37%  182M 0s\n",
            " 21500K .......... .......... .......... .......... .......... 37%  159M 0s\n",
            " 21550K .......... .......... .......... .......... .......... 37%  160M 0s\n",
            " 21600K .......... .......... .......... .......... .......... 37%  191M 0s\n",
            " 21650K .......... .......... .......... .......... .......... 38%  173M 0s\n",
            " 21700K .......... .......... .......... .......... .......... 38%  173M 0s\n",
            " 21750K .......... .......... .......... .......... .......... 38%  178M 0s\n",
            " 21800K .......... .......... .......... .......... .......... 38%  309M 0s\n",
            " 21850K .......... .......... .......... .......... .......... 38%  214M 0s\n",
            " 21900K .......... .......... .......... .......... .......... 38%  199M 0s\n",
            " 21950K .......... .......... .......... .......... .......... 38%  162M 0s\n",
            " 22000K .......... .......... .......... .......... .......... 38%  193M 0s\n",
            " 22050K .......... .......... .......... .......... .......... 38%  206M 0s\n",
            " 22100K .......... .......... .......... .......... .......... 38%  303M 0s\n",
            " 22150K .......... .......... .......... .......... .......... 38%  272M 0s\n",
            " 22200K .......... .......... .......... .......... .......... 38%  201M 0s\n",
            " 22250K .......... .......... .......... .......... .......... 39%  228M 0s\n",
            " 22300K .......... .......... .......... .......... .......... 39%  216M 0s\n",
            " 22350K .......... .......... .......... .......... .......... 39%  147M 0s\n",
            " 22400K .......... .......... .......... .......... .......... 39%  155M 0s\n",
            " 22450K .......... .......... .......... .......... .......... 39%  169M 0s\n",
            " 22500K .......... .......... .......... .......... .......... 39%  280M 0s\n",
            " 22550K .......... .......... .......... .......... .......... 39%  290M 0s\n",
            " 22600K .......... .......... .......... .......... .......... 39%  318M 0s\n",
            " 22650K .......... .......... .......... .......... .......... 39%  308M 0s\n",
            " 22700K .......... .......... .......... .......... .......... 39%  264M 0s\n",
            " 22750K .......... .......... .......... .......... .......... 39%  178M 0s\n",
            " 22800K .......... .......... .......... .......... .......... 40%  216M 0s\n",
            " 22850K .......... .......... .......... .......... .......... 40%  248M 0s\n",
            " 22900K .......... .......... .......... .......... .......... 40%  264M 0s\n",
            " 22950K .......... .......... .......... .......... .......... 40%  164M 0s\n",
            " 23000K .......... .......... .......... .......... .......... 40%  203M 0s\n",
            " 23050K .......... .......... .......... .......... .......... 40%  208M 0s\n",
            " 23100K .......... .......... .......... .......... .......... 40%  211M 0s\n",
            " 23150K .......... .......... .......... .......... .......... 40%  144M 0s\n",
            " 23200K .......... .......... .......... .......... .......... 40%  161M 0s\n",
            " 23250K .......... .......... .......... .......... .......... 40%  170M 0s\n",
            " 23300K .......... .......... .......... .......... .......... 40%  168M 0s\n",
            " 23350K .......... .......... .......... .......... .......... 40%  165M 0s\n",
            " 23400K .......... .......... .......... .......... .......... 41%  221M 0s\n",
            " 23450K .......... .......... .......... .......... .......... 41%  190M 0s\n",
            " 23500K .......... .......... .......... .......... .......... 41%  203M 0s\n",
            " 23550K .......... .......... .......... .......... .......... 41%  179M 0s\n",
            " 23600K .......... .......... .......... .......... .......... 41%  339M 0s\n",
            " 23650K .......... .......... .......... .......... .......... 41%  313M 0s\n",
            " 23700K .......... .......... .......... .......... .......... 41%  271M 0s\n",
            " 23750K .......... .......... .......... .......... .......... 41%  161M 0s\n",
            " 23800K .......... .......... .......... .......... .......... 41%  199M 0s\n",
            " 23850K .......... .......... .......... .......... .......... 41%  216M 0s\n",
            " 23900K .......... .......... .......... .......... .......... 41%  260M 0s\n",
            " 23950K .......... .......... .......... .......... .......... 42%  178M 0s\n",
            " 24000K .......... .......... .......... .......... .......... 42%  173M 0s\n",
            " 24050K .......... .......... .......... .......... .......... 42%  291M 0s\n",
            " 24100K .......... .......... .......... .......... .......... 42%  365M 0s\n",
            " 24150K .......... .......... .......... .......... .......... 42%  276M 0s\n",
            " 24200K .......... .......... .......... .......... .......... 42%  284M 0s\n",
            " 24250K .......... .......... .......... .......... .......... 42%  287M 0s\n",
            " 24300K .......... .......... .......... .......... .......... 42%  308M 0s\n",
            " 24350K .......... .......... .......... .......... .......... 42%  199M 0s\n",
            " 24400K .......... .......... .......... .......... .......... 42%  320M 0s\n",
            " 24450K .......... .......... .......... .......... .......... 42%  299M 0s\n",
            " 24500K .......... .......... .......... .......... .......... 42%  231M 0s\n",
            " 24550K .......... .......... .......... .......... .......... 43%  174M 0s\n",
            " 24600K .......... .......... .......... .......... .......... 43%  210M 0s\n",
            " 24650K .......... .......... .......... .......... .......... 43%  340M 0s\n",
            " 24700K .......... .......... .......... .......... .......... 43%  354M 0s\n",
            " 24750K .......... .......... .......... .......... .......... 43%  186M 0s\n",
            " 24800K .......... .......... .......... .......... .......... 43%  326M 0s\n",
            " 24850K .......... .......... .......... .......... .......... 43%  267M 0s\n",
            " 24900K .......... .......... .......... .......... .......... 43%  266M 0s\n",
            " 24950K .......... .......... .......... .......... .......... 43%  253M 0s\n",
            " 25000K .......... .......... .......... .......... .......... 43%  315M 0s\n",
            " 25050K .......... .......... .......... .......... .......... 43%  191M 0s\n",
            " 25100K .......... .......... .......... .......... .......... 44%  195M 0s\n",
            " 25150K .......... .......... .......... .......... .......... 44%  160M 0s\n",
            " 25200K .......... .......... .......... .......... .......... 44%  187M 0s\n",
            " 25250K .......... .......... .......... .......... .......... 44%  214M 0s\n",
            " 25300K .......... .......... .......... .......... .......... 44%  221M 0s\n",
            " 25350K .......... .......... .......... .......... .......... 44%  183M 0s\n",
            " 25400K .......... .......... .......... .......... .......... 44%  222M 0s\n",
            " 25450K .......... .......... .......... .......... .......... 44%  141M 0s\n",
            " 25500K .......... .......... .......... .......... .......... 44%  208M 0s\n",
            " 25550K .......... .......... .......... .......... .......... 44%  168M 0s\n",
            " 25600K .......... .......... .......... .......... .......... 44%  180M 0s\n",
            " 25650K .......... .......... .......... .......... .......... 45%  206M 0s\n",
            " 25700K .......... .......... .......... .......... .......... 45%  201M 0s\n",
            " 25750K .......... .......... .......... .......... .......... 45%  223M 0s\n",
            " 25800K .......... .......... .......... .......... .......... 45%  248M 0s\n",
            " 25850K .......... .......... .......... .......... .......... 45%  206M 0s\n",
            " 25900K .......... .......... .......... .......... .......... 45%  227M 0s\n",
            " 25950K .......... .......... .......... .......... .......... 45%  169M 0s\n",
            " 26000K .......... .......... .......... .......... .......... 45%  225M 0s\n",
            " 26050K .......... .......... .......... .......... .......... 45%  221M 0s\n",
            " 26100K .......... .......... .......... .......... .......... 45%  271M 0s\n",
            " 26150K .......... .......... .......... .......... .......... 45%  301M 0s\n",
            " 26200K .......... .......... .......... .......... .......... 45%  346M 0s\n",
            " 26250K .......... .......... .......... .......... .......... 46%  293M 0s\n",
            " 26300K .......... .......... .......... .......... .......... 46%  352M 0s\n",
            " 26350K .......... .......... .......... .......... .......... 46%  182M 0s\n",
            " 26400K .......... .......... .......... .......... .......... 46%  229M 0s\n",
            " 26450K .......... .......... .......... .......... .......... 46%  207M 0s\n",
            " 26500K .......... .......... .......... .......... .......... 46%  205M 0s\n",
            " 26550K .......... .......... .......... .......... .......... 46%  156M 0s\n",
            " 26600K .......... .......... .......... .......... .......... 46%  162M 0s\n",
            " 26650K .......... .......... .......... .......... .......... 46%  167M 0s\n",
            " 26700K .......... .......... .......... .......... .......... 46%  156M 0s\n",
            " 26750K .......... .......... .......... .......... .......... 46%  134M 0s\n",
            " 26800K .......... .......... .......... .......... .......... 47%  157M 0s\n",
            " 26850K .......... .......... .......... .......... .......... 47%  156M 0s\n",
            " 26900K .......... .......... .......... .......... .......... 47%  165M 0s\n",
            " 26950K .......... .......... .......... .......... .......... 47%  177M 0s\n",
            " 27000K .......... .......... .......... .......... .......... 47%  202M 0s\n",
            " 27050K .......... .......... .......... .......... .......... 47%  173M 0s\n",
            " 27100K .......... .......... .......... .......... .......... 47%  246M 0s\n",
            " 27150K .......... .......... .......... .......... .......... 47%  166M 0s\n",
            " 27200K .......... .......... .......... .......... .......... 47%  219M 0s\n",
            " 27250K .......... .......... .......... .......... .......... 47%  173M 0s\n",
            " 27300K .......... .......... .......... .......... .......... 47%  217M 0s\n",
            " 27350K .......... .......... .......... .......... .......... 47%  222M 0s\n",
            " 27400K .......... .......... .......... .......... .......... 48%  286M 0s\n",
            " 27450K .......... .......... .......... .......... .......... 48%  290M 0s\n",
            " 27500K .......... .......... .......... .......... .......... 48%  251M 0s\n",
            " 27550K .......... .......... .......... .......... .......... 48%  241M 0s\n",
            " 27600K .......... .......... .......... .......... .......... 48%  321M 0s\n",
            " 27650K .......... .......... .......... .......... .......... 48%  227M 0s\n",
            " 27700K .......... .......... .......... .......... .......... 48%  224M 0s\n",
            " 27750K .......... .......... .......... .......... .......... 48%  197M 0s\n",
            " 27800K .......... .......... .......... .......... .......... 48%  218M 0s\n",
            " 27850K .......... .......... .......... .......... .......... 48%  212M 0s\n",
            " 27900K .......... .......... .......... .......... .......... 48%  221M 0s\n",
            " 27950K .......... .......... .......... .......... .......... 49%  213M 0s\n",
            " 28000K .......... .......... .......... .......... .......... 49%  315M 0s\n",
            " 28050K .......... .......... .......... .......... .......... 49%  215M 0s\n",
            " 28100K .......... .......... .......... .......... .......... 49%  329M 0s\n",
            " 28150K .......... .......... .......... .......... .......... 49%  326M 0s\n",
            " 28200K .......... .......... .......... .......... .......... 49%  230M 0s\n",
            " 28250K .......... .......... .......... .......... .......... 49%  338M 0s\n",
            " 28300K .......... .......... .......... .......... .......... 49%  206M 0s\n",
            " 28350K .......... .......... .......... .......... .......... 49%  168M 0s\n",
            " 28400K .......... .......... .......... .......... .......... 49%  250M 0s\n",
            " 28450K .......... .......... .......... .......... .......... 49%  182M 0s\n",
            " 28500K .......... .......... .......... .......... .......... 50%  190M 0s\n",
            " 28550K .......... .......... .......... .......... .......... 50%  183M 0s\n",
            " 28600K .......... .......... .......... .......... .......... 50%  229M 0s\n",
            " 28650K .......... .......... .......... .......... .......... 50%  189M 0s\n",
            " 28700K .......... .......... .......... .......... .......... 50%  225M 0s\n",
            " 28750K .......... .......... .......... .......... .......... 50%  160M 0s\n",
            " 28800K .......... .......... .......... .......... .......... 50%  210M 0s\n",
            " 28850K .......... .......... .......... .......... .......... 50%  172M 0s\n",
            " 28900K .......... .......... .......... .......... .......... 50%  187M 0s\n",
            " 28950K .......... .......... .......... .......... .......... 50%  192M 0s\n",
            " 29000K .......... .......... .......... .......... .......... 50%  235M 0s\n",
            " 29050K .......... .......... .......... .......... .......... 50%  231M 0s\n",
            " 29100K .......... .......... .......... .......... .......... 51%  256M 0s\n",
            " 29150K .......... .......... .......... .......... .......... 51%  210M 0s\n",
            " 29200K .......... .......... .......... .......... .......... 51%  341M 0s\n",
            " 29250K .......... .......... .......... .......... .......... 51%  295M 0s\n",
            " 29300K .......... .......... .......... .......... .......... 51%  257M 0s\n",
            " 29350K .......... .......... .......... .......... .......... 51%  210M 0s\n",
            " 29400K .......... .......... .......... .......... .......... 51%  270M 0s\n",
            " 29450K .......... .......... .......... .......... .......... 51%  357M 0s\n",
            " 29500K .......... .......... .......... .......... .......... 51%  326M 0s\n",
            " 29550K .......... .......... .......... .......... .......... 51%  230M 0s\n",
            " 29600K .......... .......... .......... .......... .......... 51%  290M 0s\n",
            " 29650K .......... .......... .......... .......... .......... 52%  330M 0s\n",
            " 29700K .......... .......... .......... .......... .......... 52%  258M 0s\n",
            " 29750K .......... .......... .......... .......... .......... 52%  208M 0s\n",
            " 29800K .......... .......... .......... .......... .......... 52%  208M 0s\n",
            " 29850K .......... .......... .......... .......... .......... 52%  203M 0s\n",
            " 29900K .......... .......... .......... .......... .......... 52%  215M 0s\n",
            " 29950K .......... .......... .......... .......... .......... 52%  176M 0s\n",
            " 30000K .......... .......... .......... .......... .......... 52%  261M 0s\n",
            " 30050K .......... .......... .......... .......... .......... 52%  143M 0s\n",
            " 30100K .......... .......... .......... .......... .......... 52%  172M 0s\n",
            " 30150K .......... .......... .......... .......... .......... 52%  179M 0s\n",
            " 30200K .......... .......... .......... .......... .......... 52%  193M 0s\n",
            " 30250K .......... .......... .......... .......... .......... 53%  168M 0s\n",
            " 30300K .......... .......... .......... .......... .......... 53%  204M 0s\n",
            " 30350K .......... .......... .......... .......... .......... 53%  152M 0s\n",
            " 30400K .......... .......... .......... .......... .......... 53%  157M 0s\n",
            " 30450K .......... .......... .......... .......... .......... 53%  210M 0s\n",
            " 30500K .......... .......... .......... .......... .......... 53%  196M 0s\n",
            " 30550K .......... .......... .......... .......... .......... 53%  183M 0s\n",
            " 30600K .......... .......... .......... .......... .......... 53%  193M 0s\n",
            " 30650K .......... .......... .......... .......... .......... 53%  210M 0s\n",
            " 30700K .......... .......... .......... .......... .......... 53%  206M 0s\n",
            " 30750K .......... .......... .......... .......... .......... 53%  161M 0s\n",
            " 30800K .......... .......... .......... .......... .......... 54%  189M 0s\n",
            " 30850K .......... .......... .......... .......... .......... 54%  195M 0s\n",
            " 30900K .......... .......... .......... .......... .......... 54%  213M 0s\n",
            " 30950K .......... .......... .......... .......... .......... 54%  186M 0s\n",
            " 31000K .......... .......... .......... .......... .......... 54%  233M 0s\n",
            " 31050K .......... .......... .......... .......... .......... 54%  322M 0s\n",
            " 31100K .......... .......... .......... .......... .......... 54%  299M 0s\n",
            " 31150K .......... .......... .......... .......... .......... 54%  170M 0s\n",
            " 31200K .......... .......... .......... .......... .......... 54%  188M 0s\n",
            " 31250K .......... .......... .......... .......... .......... 54%  225M 0s\n",
            " 31300K .......... .......... .......... .......... .......... 54%  172M 0s\n",
            " 31350K .......... .......... .......... .......... .......... 54%  183M 0s\n",
            " 31400K .......... .......... .......... .......... .......... 55%  197M 0s\n",
            " 31450K .......... .......... .......... .......... .......... 55%  188M 0s\n",
            " 31500K .......... .......... .......... .......... .......... 55%  180M 0s\n",
            " 31550K .......... .......... .......... .......... .......... 55%  170M 0s\n",
            " 31600K .......... .......... .......... .......... .......... 55%  164M 0s\n",
            " 31650K .......... .......... .......... .......... .......... 55%  215M 0s\n",
            " 31700K .......... .......... .......... .......... .......... 55%  184M 0s\n",
            " 31750K .......... .......... .......... .......... .......... 55%  191M 0s\n",
            " 31800K .......... .......... .......... .......... .......... 55%  168M 0s\n",
            " 31850K .......... .......... .......... .......... .......... 55%  194M 0s\n",
            " 31900K .......... .......... .......... .......... .......... 55%  244M 0s\n",
            " 31950K .......... .......... .......... .......... .......... 56%  236M 0s\n",
            " 32000K .......... .......... .......... .......... .......... 56%  215M 0s\n",
            " 32050K .......... .......... .......... .......... .......... 56%  227M 0s\n",
            " 32100K .......... .......... .......... .......... .......... 56%  236M 0s\n",
            " 32150K .......... .......... .......... .......... .......... 56%  199M 0s\n",
            " 32200K .......... .......... .......... .......... .......... 56%  207M 0s\n",
            " 32250K .......... .......... .......... .......... .......... 56%  224M 0s\n",
            " 32300K .......... .......... .......... .......... .......... 56%  223M 0s\n",
            " 32350K .......... .......... .......... .......... .......... 56%  180M 0s\n",
            " 32400K .......... .......... .......... .......... .......... 56%  296M 0s\n",
            " 32450K .......... .......... .......... .......... .......... 56%  247M 0s\n",
            " 32500K .......... .......... .......... .......... .......... 57%  326M 0s\n",
            " 32550K .......... .......... .......... .......... .......... 57%  332M 0s\n",
            " 32600K .......... .......... .......... .......... .......... 57%  336M 0s\n",
            " 32650K .......... .......... .......... .......... .......... 57%  264M 0s\n",
            " 32700K .......... .......... .......... .......... .......... 57%  215M 0s\n",
            " 32750K .......... .......... .......... .......... .......... 57%  169M 0s\n",
            " 32800K .......... .......... .......... .......... .......... 57%  236M 0s\n",
            " 32850K .......... .......... .......... .......... .......... 57%  360M 0s\n",
            " 32900K .......... .......... .......... .......... .......... 57%  373M 0s\n",
            " 32950K .......... .......... .......... .......... .......... 57%  288M 0s\n",
            " 33000K .......... .......... .......... .......... .......... 57%  230M 0s\n",
            " 33050K .......... .......... .......... .......... .......... 57%  227M 0s\n",
            " 33100K .......... .......... .......... .......... .......... 58%  205M 0s\n",
            " 33150K .......... .......... .......... .......... .......... 58%  179M 0s\n",
            " 33200K .......... .......... .......... .......... .......... 58%  176M 0s\n",
            " 33250K .......... .......... .......... .......... .......... 58%  177M 0s\n",
            " 33300K .......... .......... .......... .......... .......... 58%  174M 0s\n",
            " 33350K .......... .......... .......... .......... .......... 58%  155M 0s\n",
            " 33400K .......... .......... .......... .......... .......... 58%  190M 0s\n",
            " 33450K .......... .......... .......... .......... .......... 58%  329M 0s\n",
            " 33500K .......... .......... .......... .......... .......... 58%  275M 0s\n",
            " 33550K .......... .......... .......... .......... .......... 58%  277M 0s\n",
            " 33600K .......... .......... .......... .......... .......... 58%  348M 0s\n",
            " 33650K .......... .......... .......... .......... .......... 59%  263M 0s\n",
            " 33700K .......... .......... .......... .......... .......... 59%  207M 0s\n",
            " 33750K .......... .......... .......... .......... .......... 59%  185M 0s\n",
            " 33800K .......... .......... .......... .......... .......... 59%  193M 0s\n",
            " 33850K .......... .......... .......... .......... .......... 59%  192M 0s\n",
            " 33900K .......... .......... .......... .......... .......... 59%  198M 0s\n",
            " 33950K .......... .......... .......... .......... .......... 59%  157M 0s\n",
            " 34000K .......... .......... .......... .......... .......... 59%  205M 0s\n",
            " 34050K .......... .......... .......... .......... .......... 59%  155M 0s\n",
            " 34100K .......... .......... .......... .......... .......... 59%  225M 0s\n",
            " 34150K .......... .......... .......... .......... .......... 59%  309M 0s\n",
            " 34200K .......... .......... .......... .......... .......... 59%  299M 0s\n",
            " 34250K .......... .......... .......... .......... .......... 60%  240M 0s\n",
            " 34300K .......... .......... .......... .......... .......... 60%  132M 0s\n",
            " 34350K .......... .......... .......... .......... .......... 60%  182M 0s\n",
            " 34400K .......... .......... .......... .......... .......... 60%  326M 0s\n",
            " 34450K .......... .......... .......... .......... .......... 60%  359M 0s\n",
            " 34500K .......... .......... .......... .......... .......... 60%  352M 0s\n",
            " 34550K .......... .......... .......... .......... .......... 60%  256M 0s\n",
            " 34600K .......... .......... .......... .......... .......... 60%  288M 0s\n",
            " 34650K .......... .......... .......... .......... .......... 60%  303M 0s\n",
            " 34700K .......... .......... .......... .......... .......... 60%  350M 0s\n",
            " 34750K .......... .......... .......... .......... .......... 60%  302M 0s\n",
            " 34800K .......... .......... .......... .......... .......... 61%  258M 0s\n",
            " 34850K .......... .......... .......... .......... .......... 61%  205M 0s\n",
            " 34900K .......... .......... .......... .......... .......... 61%  191M 0s\n",
            " 34950K .......... .......... .......... .......... .......... 61%  177M 0s\n",
            " 35000K .......... .......... .......... .......... .......... 61%  223M 0s\n",
            " 35050K .......... .......... .......... .......... .......... 61%  192M 0s\n",
            " 35100K .......... .......... .......... .......... .......... 61%  309M 0s\n",
            " 35150K .......... .......... .......... .......... .......... 61%  253M 0s\n",
            " 35200K .......... .......... .......... .......... .......... 61%  330M 0s\n",
            " 35250K .......... .......... .......... .......... .......... 61%  348M 0s\n",
            " 35300K .......... .......... .......... .......... .......... 61%  287M 0s\n",
            " 35350K .......... .......... .......... .......... .......... 61%  189M 0s\n",
            " 35400K .......... .......... .......... .......... .......... 62%  230M 0s\n",
            " 35450K .......... .......... .......... .......... .......... 62%  227M 0s\n",
            " 35500K .......... .......... .......... .......... .......... 62%  231M 0s\n",
            " 35550K .......... .......... .......... .......... .......... 62%  172M 0s\n",
            " 35600K .......... .......... .......... .......... .......... 62%  237M 0s\n",
            " 35650K .......... .......... .......... .......... .......... 62%  206M 0s\n",
            " 35700K .......... .......... .......... .......... .......... 62%  211M 0s\n",
            " 35750K .......... .......... .......... .......... .......... 62%  158M 0s\n",
            " 35800K .......... .......... .......... .......... .......... 62%  178M 0s\n",
            " 35850K .......... .......... .......... .......... .......... 62%  186M 0s\n",
            " 35900K .......... .......... .......... .......... .......... 62%  150M 0s\n",
            " 35950K .......... .......... .......... .......... .......... 63%  158M 0s\n",
            " 36000K .......... .......... .......... .......... .......... 63%  196M 0s\n",
            " 36050K .......... .......... .......... .......... .......... 63%  208M 0s\n",
            " 36100K .......... .......... .......... .......... .......... 63%  144M 0s\n",
            " 36150K .......... .......... .......... .......... .......... 63%  176M 0s\n",
            " 36200K .......... .......... .......... .......... .......... 63%  209M 0s\n",
            " 36250K .......... .......... .......... .......... .......... 63%  241M 0s\n",
            " 36300K .......... .......... .......... .......... .......... 63%  238M 0s\n",
            " 36350K .......... .......... .......... .......... .......... 63%  170M 0s\n",
            " 36400K .......... .......... .......... .......... .......... 63%  202M 0s\n",
            " 36450K .......... .......... .......... .......... .......... 63%  208M 0s\n",
            " 36500K .......... .......... .......... .......... .......... 64%  211M 0s\n",
            " 36550K .......... .......... .......... .......... .......... 64%  205M 0s\n",
            " 36600K .......... .......... .......... .......... .......... 64%  353M 0s\n",
            " 36650K .......... .......... .......... .......... .......... 64%  359M 0s\n",
            " 36700K .......... .......... .......... .......... .......... 64%  390M 0s\n",
            " 36750K .......... .......... .......... .......... .......... 64%  264M 0s\n",
            " 36800K .......... .......... .......... .......... .......... 64%  281M 0s\n",
            " 36850K .......... .......... .......... .......... .......... 64%  248M 0s\n",
            " 36900K .......... .......... .......... .......... .......... 64%  229M 0s\n",
            " 36950K .......... .......... .......... .......... .......... 64%  264M 0s\n",
            " 37000K .......... .......... .......... .......... .......... 64%  343M 0s\n",
            " 37050K .......... .......... .......... .......... .......... 64%  257M 0s\n",
            " 37100K .......... .......... .......... .......... .......... 65%  329M 0s\n",
            " 37150K .......... .......... .......... .......... .......... 65%  295M 0s\n",
            " 37200K .......... .......... .......... .......... .......... 65%  247M 0s\n",
            " 37250K .......... .......... .......... .......... .......... 65%  330M 0s\n",
            " 37300K .......... .......... .......... .......... .......... 65%  269M 0s\n",
            " 37350K .......... .......... .......... .......... .......... 65%  143M 0s\n",
            " 37400K .......... .......... .......... .......... .......... 65%  198M 0s\n",
            " 37450K .......... .......... .......... .......... .......... 65%  231M 0s\n",
            " 37500K .......... .......... .......... .......... .......... 65%  197M 0s\n",
            " 37550K .......... .......... .......... .......... .......... 65%  162M 0s\n",
            " 37600K .......... .......... .......... .......... .......... 65%  203M 0s\n",
            " 37650K .......... .......... .......... .......... .......... 66%  183M 0s\n",
            " 37700K .......... .......... .......... .......... .......... 66%  145M 0s\n",
            " 37750K .......... .......... .......... .......... .......... 66%  158M 0s\n",
            " 37800K .......... .......... .......... .......... .......... 66%  227M 0s\n",
            " 37850K .......... .......... .......... .......... .......... 66%  181M 0s\n",
            " 37900K .......... .......... .......... .......... .......... 66%  170M 0s\n",
            " 37950K .......... .......... .......... .......... .......... 66%  162M 0s\n",
            " 38000K .......... .......... .......... .......... .......... 66%  166M 0s\n",
            " 38050K .......... .......... .......... .......... .......... 66%  243M 0s\n",
            " 38100K .......... .......... .......... .......... .......... 66%  211M 0s\n",
            " 38150K .......... .......... .......... .......... .......... 66%  182M 0s\n",
            " 38200K .......... .......... .......... .......... .......... 66%  178M 0s\n",
            " 38250K .......... .......... .......... .......... .......... 67%  233M 0s\n",
            " 38300K .......... .......... .......... .......... .......... 67%  261M 0s\n",
            " 38350K .......... .......... .......... .......... .......... 67%  301M 0s\n",
            " 38400K .......... .......... .......... .......... .......... 67%  326M 0s\n",
            " 38450K .......... .......... .......... .......... .......... 67%  219M 0s\n",
            " 38500K .......... .......... .......... .......... .......... 67%  321M 0s\n",
            " 38550K .......... .......... .......... .......... .......... 67%  273M 0s\n",
            " 38600K .......... .......... .......... .......... .......... 67%  210M 0s\n",
            " 38650K .......... .......... .......... .......... .......... 67%  197M 0s\n",
            " 38700K .......... .......... .......... .......... .......... 67%  248M 0s\n",
            " 38750K .......... .......... .......... .......... .......... 67%  260M 0s\n",
            " 38800K .......... .......... .......... .......... .......... 68%  203M 0s\n",
            " 38850K .......... .......... .......... .......... .......... 68%  263M 0s\n",
            " 38900K .......... .......... .......... .......... .......... 68%  302M 0s\n",
            " 38950K .......... .......... .......... .......... .......... 68%  312M 0s\n",
            " 39000K .......... .......... .......... .......... .......... 68%  300M 0s\n",
            " 39050K .......... .......... .......... .......... .......... 68%  303M 0s\n",
            " 39100K .......... .......... .......... .......... .......... 68%  223M 0s\n",
            " 39150K .......... .......... .......... .......... .......... 68%  215M 0s\n",
            " 39200K .......... .......... .......... .......... .......... 68%  358M 0s\n",
            " 39250K .......... .......... .......... .......... .......... 68%  363M 0s\n",
            " 39300K .......... .......... .......... .......... .......... 68%  321M 0s\n",
            " 39350K .......... .......... .......... .......... .......... 69%  309M 0s\n",
            " 39400K .......... .......... .......... .......... .......... 69%  193M 0s\n",
            " 39450K .......... .......... .......... .......... .......... 69%  222M 0s\n",
            " 39500K .......... .......... .......... .......... .......... 69%  252M 0s\n",
            " 39550K .......... .......... .......... .......... .......... 69%  195M 0s\n",
            " 39600K .......... .......... .......... .......... .......... 69%  303M 0s\n",
            " 39650K .......... .......... .......... .......... .......... 69%  238M 0s\n",
            " 39700K .......... .......... .......... .......... .......... 69%  289M 0s\n",
            " 39750K .......... .......... .......... .......... .......... 69%  328M 0s\n",
            " 39800K .......... .......... .......... .......... .......... 69%  251M 0s\n",
            " 39850K .......... .......... .......... .......... .......... 69%  259M 0s\n",
            " 39900K .......... .......... .......... .......... .......... 69%  254M 0s\n",
            " 39950K .......... .......... .......... .......... .......... 70%  211M 0s\n",
            " 40000K .......... .......... .......... .......... .......... 70%  308M 0s\n",
            " 40050K .......... .......... .......... .......... .......... 70%  296M 0s\n",
            " 40100K .......... .......... .......... .......... .......... 70%  247M 0s\n",
            " 40150K .......... .......... .......... .......... .......... 70%  241M 0s\n",
            " 40200K .......... .......... .......... .......... .......... 70%  221M 0s\n",
            " 40250K .......... .......... .......... .......... .......... 70%  293M 0s\n",
            " 40300K .......... .......... .......... .......... .......... 70%  363M 0s\n",
            " 40350K .......... .......... .......... .......... .......... 70%  289M 0s\n",
            " 40400K .......... .......... .......... .......... .......... 70%  275M 0s\n",
            " 40450K .......... .......... .......... .......... .......... 70%  248M 0s\n",
            " 40500K .......... .......... .......... .......... .......... 71%  301M 0s\n",
            " 40550K .......... .......... .......... .......... .......... 71%  260M 0s\n",
            " 40600K .......... .......... .......... .......... .......... 71%  220M 0s\n",
            " 40650K .......... .......... .......... .......... .......... 71%  209M 0s\n",
            " 40700K .......... .......... .......... .......... .......... 71%  146M 0s\n",
            " 40750K .......... .......... .......... .......... .......... 71%  173M 0s\n",
            " 40800K .......... .......... .......... .......... .......... 71%  216M 0s\n",
            " 40850K .......... .......... .......... .......... .......... 71%  209M 0s\n",
            " 40900K .......... .......... .......... .......... .......... 71%  181M 0s\n",
            " 40950K .......... .......... .......... .......... .......... 71%  218M 0s\n",
            " 41000K .......... .......... .......... .......... .......... 71%  238M 0s\n",
            " 41050K .......... .......... .......... .......... .......... 71%  293M 0s\n",
            " 41100K .......... .......... .......... .......... .......... 72%  219M 0s\n",
            " 41150K .......... .......... .......... .......... .......... 72%  236M 0s\n",
            " 41200K .......... .......... .......... .......... .......... 72%  336M 0s\n",
            " 41250K .......... .......... .......... .......... .......... 72% 27.1M 0s\n",
            " 41300K .......... .......... .......... .......... .......... 72%  154M 0s\n",
            " 41350K .......... .......... .......... .......... .......... 72%  165M 0s\n",
            " 41400K .......... .......... .......... .......... .......... 72%  176M 0s\n",
            " 41450K .......... .......... .......... .......... .......... 72%  181M 0s\n",
            " 41500K .......... .......... .......... .......... .......... 72%  183M 0s\n",
            " 41550K .......... .......... .......... .......... .......... 72%  174M 0s\n",
            " 41600K .......... .......... .......... .......... .......... 72%  204M 0s\n",
            " 41650K .......... .......... .......... .......... .......... 73%  225M 0s\n",
            " 41700K .......... .......... .......... .......... .......... 73%  257M 0s\n",
            " 41750K .......... .......... .......... .......... .......... 73%  193M 0s\n",
            " 41800K .......... .......... .......... .......... .......... 73%  214M 0s\n",
            " 41850K .......... .......... .......... .......... .......... 73%  210M 0s\n",
            " 41900K .......... .......... .......... .......... .......... 73%  195M 0s\n",
            " 41950K .......... .......... .......... .......... .......... 73%  141M 0s\n",
            " 42000K .......... .......... .......... .......... .......... 73%  185M 0s\n",
            " 42050K .......... .......... .......... .......... .......... 73%  174M 0s\n",
            " 42100K .......... .......... .......... .......... .......... 73%  206M 0s\n",
            " 42150K .......... .......... .......... .......... .......... 73%  190M 0s\n",
            " 42200K .......... .......... .......... .......... .......... 73%  171M 0s\n",
            " 42250K .......... .......... .......... .......... .......... 74%  193M 0s\n",
            " 42300K .......... .......... .......... .......... .......... 74%  244M 0s\n",
            " 42350K .......... .......... .......... .......... .......... 74%  163M 0s\n",
            " 42400K .......... .......... .......... .......... .......... 74%  195M 0s\n",
            " 42450K .......... .......... .......... .......... .......... 74%  224M 0s\n",
            " 42500K .......... .......... .......... .......... .......... 74%  298M 0s\n",
            " 42550K .......... .......... .......... .......... .......... 74%  246M 0s\n",
            " 42600K .......... .......... .......... .......... .......... 74%  246M 0s\n",
            " 42650K .......... .......... .......... .......... .......... 74%  221M 0s\n",
            " 42700K .......... .......... .......... .......... .......... 74%  203M 0s\n",
            " 42750K .......... .......... .......... .......... .......... 74%  184M 0s\n",
            " 42800K .......... .......... .......... .......... .......... 75%  159M 0s\n",
            " 42850K .......... .......... .......... .......... .......... 75%  204M 0s\n",
            " 42900K .......... .......... .......... .......... .......... 75%  208M 0s\n",
            " 42950K .......... .......... .......... .......... .......... 75%  179M 0s\n",
            " 43000K .......... .......... .......... .......... .......... 75%  185M 0s\n",
            " 43050K .......... .......... .......... .......... .......... 75%  272M 0s\n",
            " 43100K .......... .......... .......... .......... .......... 75%  366M 0s\n",
            " 43150K .......... .......... .......... .......... .......... 75%  304M 0s\n",
            " 43200K .......... .......... .......... .......... .......... 75%  241M 0s\n",
            " 43250K .......... .......... .......... .......... .......... 75%  275M 0s\n",
            " 43300K .......... .......... .......... .......... .......... 75%  250M 0s\n",
            " 43350K .......... .......... .......... .......... .......... 76%  208M 0s\n",
            " 43400K .......... .......... .......... .......... .......... 76%  239M 0s\n",
            " 43450K .......... .......... .......... .......... .......... 76%  218M 0s\n",
            " 43500K .......... .......... .......... .......... .......... 76%  223M 0s\n",
            " 43550K .......... .......... .......... .......... .......... 76%  156M 0s\n",
            " 43600K .......... .......... .......... .......... .......... 76%  227M 0s\n",
            " 43650K .......... .......... .......... .......... .......... 76%  222M 0s\n",
            " 43700K .......... .......... .......... .......... .......... 76%  179M 0s\n",
            " 43750K .......... .......... .......... .......... .......... 76%  164M 0s\n",
            " 43800K .......... .......... .......... .......... .......... 76%  210M 0s\n",
            " 43850K .......... .......... .......... .......... .......... 76%  255M 0s\n",
            " 43900K .......... .......... .......... .......... .......... 76%  286M 0s\n",
            " 43950K .......... .......... .......... .......... .......... 77%  238M 0s\n",
            " 44000K .......... .......... .......... .......... .......... 77%  366M 0s\n",
            " 44050K .......... .......... .......... .......... .......... 77%  303M 0s\n",
            " 44100K .......... .......... .......... .......... .......... 77%  317M 0s\n",
            " 44150K .......... .......... .......... .......... .......... 77%  291M 0s\n",
            " 44200K .......... .......... .......... .......... .......... 77%  282M 0s\n",
            " 44250K .......... .......... .......... .......... .......... 77%  193M 0s\n",
            " 44300K .......... .......... .......... .......... .......... 77%  357M 0s\n",
            " 44350K .......... .......... .......... .......... .......... 77%  261M 0s\n",
            " 44400K .......... .......... .......... .......... .......... 77%  345M 0s\n",
            " 44450K .......... .......... .......... .......... .......... 77%  373M 0s\n",
            " 44500K .......... .......... .......... .......... .......... 78%  370M 0s\n",
            " 44550K .......... .......... .......... .......... .......... 78%  182M 0s\n",
            " 44600K .......... .......... .......... .......... .......... 78%  194M 0s\n",
            " 44650K .......... .......... .......... .......... .......... 78%  180M 0s\n",
            " 44700K .......... .......... .......... .......... .......... 78%  166M 0s\n",
            " 44750K .......... .......... .......... .......... .......... 78%  134M 0s\n",
            " 44800K .......... .......... .......... .......... .......... 78%  199M 0s\n",
            " 44850K .......... .......... .......... .......... .......... 78%  187M 0s\n",
            " 44900K .......... .......... .......... .......... .......... 78%  188M 0s\n",
            " 44950K .......... .......... .......... .......... .......... 78%  169M 0s\n",
            " 45000K .......... .......... .......... .......... .......... 78%  189M 0s\n",
            " 45050K .......... .......... .......... .......... .......... 78%  207M 0s\n",
            " 45100K .......... .......... .......... .......... .......... 79%  194M 0s\n",
            " 45150K .......... .......... .......... .......... .......... 79%  201M 0s\n",
            " 45200K .......... .......... .......... .......... .......... 79%  247M 0s\n",
            " 45250K .......... .......... .......... .......... .......... 79%  251M 0s\n",
            " 45300K .......... .......... .......... .......... .......... 79%  238M 0s\n",
            " 45350K .......... .......... .......... .......... .......... 79% 43.8M 0s\n",
            " 45400K .......... .......... .......... .......... .......... 79% 50.7M 0s\n",
            " 45450K .......... .......... .......... .......... .......... 79% 60.9M 0s\n",
            " 45500K .......... .......... .......... .......... .......... 79% 55.1M 0s\n",
            " 45550K .......... .......... .......... .......... .......... 79% 49.1M 0s\n",
            " 45600K .......... .......... .......... .......... .......... 79% 55.4M 0s\n",
            " 45650K .......... .......... .......... .......... .......... 80%  188M 0s\n",
            " 45700K .......... .......... .......... .......... .......... 80%  169M 0s\n",
            " 45750K .......... .......... .......... .......... .......... 80%  186M 0s\n",
            " 45800K .......... .......... .......... .......... .......... 80%  157M 0s\n",
            " 45850K .......... .......... .......... .......... .......... 80%  167M 0s\n",
            " 45900K .......... .......... .......... .......... .......... 80%  205M 0s\n",
            " 45950K .......... .......... .......... .......... .......... 80%  260M 0s\n",
            " 46000K .......... .......... .......... .......... .......... 80%  282M 0s\n",
            " 46050K .......... .......... .......... .......... .......... 80%  294M 0s\n",
            " 46100K .......... .......... .......... .......... .......... 80%  324M 0s\n",
            " 46150K .......... .......... .......... .......... .......... 80%  197M 0s\n",
            " 46200K .......... .......... .......... .......... .......... 81%  259M 0s\n",
            " 46250K .......... .......... .......... .......... .......... 81%  291M 0s\n",
            " 46300K .......... .......... .......... .......... .......... 81%  346M 0s\n",
            " 46350K .......... .......... .......... .......... .......... 81%  219M 0s\n",
            " 46400K .......... .......... .......... .......... .......... 81% 35.5M 0s\n",
            " 46450K .......... .......... .......... .......... .......... 81%  147M 0s\n",
            " 46500K .......... .......... .......... .......... .......... 81%  163M 0s\n",
            " 46550K .......... .......... .......... .......... .......... 81%  101M 0s\n",
            " 46600K .......... .......... .......... .......... .......... 81%  165M 0s\n",
            " 46650K .......... .......... .......... .......... .......... 81%  179M 0s\n",
            " 46700K .......... .......... .......... .......... .......... 81%  214M 0s\n",
            " 46750K .......... .......... .......... .......... .......... 81%  131M 0s\n",
            " 46800K .......... .......... .......... .......... .......... 82%  230M 0s\n",
            " 46850K .......... .......... .......... .......... .......... 82%  154M 0s\n",
            " 46900K .......... .......... .......... .......... .......... 82%  226M 0s\n",
            " 46950K .......... .......... .......... .......... .......... 82%  153M 0s\n",
            " 47000K .......... .......... .......... .......... .......... 82%  225M 0s\n",
            " 47050K .......... .......... .......... .......... .......... 82% 70.7M 0s\n",
            " 47100K .......... .......... .......... .......... .......... 82% 61.6M 0s\n",
            " 47150K .......... .......... .......... .......... .......... 82%  154M 0s\n",
            " 47200K .......... .......... .......... .......... .......... 82%  224M 0s\n",
            " 47250K .......... .......... .......... .......... .......... 82%  180M 0s\n",
            " 47300K .......... .......... .......... .......... .......... 82%  197M 0s\n",
            " 47350K .......... .......... .......... .......... .......... 83%  143M 0s\n",
            " 47400K .......... .......... .......... .......... .......... 83%  115M 0s\n",
            " 47450K .......... .......... .......... .......... .......... 83%  131M 0s\n",
            " 47500K .......... .......... .......... .......... .......... 83%  133M 0s\n",
            " 47550K .......... .......... .......... .......... .......... 83%  180M 0s\n",
            " 47600K .......... .......... .......... .......... .......... 83%  243M 0s\n",
            " 47650K .......... .......... .......... .......... .......... 83%  304M 0s\n",
            " 47700K .......... .......... .......... .......... .......... 83%  324M 0s\n",
            " 47750K .......... .......... .......... .......... .......... 83%  301M 0s\n",
            " 47800K .......... .......... .......... .......... .......... 83%  242M 0s\n",
            " 47850K .......... .......... .......... .......... .......... 83%  304M 0s\n",
            " 47900K .......... .......... .......... .......... .......... 83%  367M 0s\n",
            " 47950K .......... .......... .......... .......... .......... 84%  217M 0s\n",
            " 48000K .......... .......... .......... .......... .......... 84%  236M 0s\n",
            " 48050K .......... .......... .......... .......... .......... 84%  318M 0s\n",
            " 48100K .......... .......... .......... .......... .......... 84%  221M 0s\n",
            " 48150K .......... .......... .......... .......... .......... 84%  302M 0s\n",
            " 48200K .......... .......... .......... .......... .......... 84%  357M 0s\n",
            " 48250K .......... .......... .......... .......... .......... 84%  307M 0s\n",
            " 48300K .......... .......... .......... .......... .......... 84%  318M 0s\n",
            " 48350K .......... .......... .......... .......... .......... 84%  263M 0s\n",
            " 48400K .......... .......... .......... .......... .......... 84%  227M 0s\n",
            " 48450K .......... .......... .......... .......... .......... 84%  212M 0s\n",
            " 48500K .......... .......... .......... .......... .......... 85%  188M 0s\n",
            " 48550K .......... .......... .......... .......... .......... 85%  197M 0s\n",
            " 48600K .......... .......... .......... .......... .......... 85%  222M 0s\n",
            " 48650K .......... .......... .......... .......... .......... 85%  321M 0s\n",
            " 48700K .......... .......... .......... .......... .......... 85%  305M 0s\n",
            " 48750K .......... .......... .......... .......... .......... 85%  243M 0s\n",
            " 48800K .......... .......... .......... .......... .......... 85%  335M 0s\n",
            " 48850K .......... .......... .......... .......... .......... 85%  324M 0s\n",
            " 48900K .......... .......... .......... .......... .......... 85%  193M 0s\n",
            " 48950K .......... .......... .......... .......... .......... 85%  174M 0s\n",
            " 49000K .......... .......... .......... .......... .......... 85%  166M 0s\n",
            " 49050K .......... .......... .......... .......... .......... 85%  140M 0s\n",
            " 49100K .......... .......... .......... .......... .......... 86%  109M 0s\n",
            " 49150K .......... .......... .......... .......... .......... 86%  153M 0s\n",
            " 49200K .......... .......... .......... .......... .......... 86%  127M 0s\n",
            " 49250K .......... .......... .......... .......... .......... 86%  178M 0s\n",
            " 49300K .......... .......... .......... .......... .......... 86%  163M 0s\n",
            " 49350K .......... .......... .......... .......... .......... 86%  139M 0s\n",
            " 49400K .......... .......... .......... .......... .......... 86%  166M 0s\n",
            " 49450K .......... .......... .......... .......... .......... 86%  169M 0s\n",
            " 49500K .......... .......... .......... .......... .......... 86%  181M 0s\n",
            " 49550K .......... .......... .......... .......... .......... 86%  139M 0s\n",
            " 49600K .......... .......... .......... .......... .......... 86%  171M 0s\n",
            " 49650K .......... .......... .......... .......... .......... 87%  218M 0s\n",
            " 49700K .......... .......... .......... .......... .......... 87%  215M 0s\n",
            " 49750K .......... .......... .......... .......... .......... 87%  182M 0s\n",
            " 49800K .......... .......... .......... .......... .......... 87%  183M 0s\n",
            " 49850K .......... .......... .......... .......... .......... 87%  182M 0s\n",
            " 49900K .......... .......... .......... .......... .......... 87%  160M 0s\n",
            " 49950K .......... .......... .......... .......... .......... 87%  150M 0s\n",
            " 50000K .......... .......... .......... .......... .......... 87%  208M 0s\n",
            " 50050K .......... .......... .......... .......... .......... 87%  204M 0s\n",
            " 50100K .......... .......... .......... .......... .......... 87%  178M 0s\n",
            " 50150K .......... .......... .......... .......... .......... 87%  156M 0s\n",
            " 50200K .......... .......... .......... .......... .......... 88%  115M 0s\n",
            " 50250K .......... .......... .......... .......... .......... 88%  182M 0s\n",
            " 50300K .......... .......... .......... .......... .......... 88%  167M 0s\n",
            " 50350K .......... .......... .......... .......... .......... 88%  138M 0s\n",
            " 50400K .......... .......... .......... .......... .......... 88%  141M 0s\n",
            " 50450K .......... .......... .......... .......... .......... 88%  175M 0s\n",
            " 50500K .......... .......... .......... .......... .......... 88%  150M 0s\n",
            " 50550K .......... .......... .......... .......... .......... 88%  108M 0s\n",
            " 50600K .......... .......... .......... .......... .......... 88%  146M 0s\n",
            " 50650K .......... .......... .......... .......... .......... 88% 90.0M 0s\n",
            " 50700K .......... .......... .......... .......... .......... 88%  138M 0s\n",
            " 50750K .......... .......... .......... .......... .......... 88% 85.6M 0s\n",
            " 50800K .......... .......... .......... .......... .......... 89%  194M 0s\n",
            " 50850K .......... .......... .......... .......... .......... 89%  244M 0s\n",
            " 50900K .......... .......... .......... .......... .......... 89%  337M 0s\n",
            " 50950K .......... .......... .......... .......... .......... 89%  272M 0s\n",
            " 51000K .......... .......... .......... .......... .......... 89%  261M 0s\n",
            " 51050K .......... .......... .......... .......... .......... 89%  282M 0s\n",
            " 51100K .......... .......... .......... .......... .......... 89%  297M 0s\n",
            " 51150K .......... .......... .......... .......... .......... 89%  278M 0s\n",
            " 51200K .......... .......... .......... .......... .......... 89%  261M 0s\n",
            " 51250K .......... .......... .......... .......... .......... 89%  202M 0s\n",
            " 51300K .......... .......... .......... .......... .......... 89%  202M 0s\n",
            " 51350K .......... .......... .......... .......... .......... 90%  183M 0s\n",
            " 51400K .......... .......... .......... .......... .......... 90%  233M 0s\n",
            " 51450K .......... .......... .......... .......... .......... 90%  114M 0s\n",
            " 51500K .......... .......... .......... .......... .......... 90%  148M 0s\n",
            " 51550K .......... .......... .......... .......... .......... 90%  211M 0s\n",
            " 51600K .......... .......... .......... .......... .......... 90%  285M 0s\n",
            " 51650K .......... .......... .......... .......... .......... 90%  244M 0s\n",
            " 51700K .......... .......... .......... .......... .......... 90%  338M 0s\n",
            " 51750K .......... .......... .......... .......... .......... 90%  372M 0s\n",
            " 51800K .......... .......... .......... .......... .......... 90%  343M 0s\n",
            " 51850K .......... .......... .......... .......... .......... 90%  306M 0s\n",
            " 51900K .......... .......... .......... .......... .......... 90%  281M 0s\n",
            " 51950K .......... .......... .......... .......... .......... 91%  175M 0s\n",
            " 52000K .......... .......... .......... .......... .......... 91%  283M 0s\n",
            " 52050K .......... .......... .......... .......... .......... 91%  298M 0s\n",
            " 52100K .......... .......... .......... .......... .......... 91%  264M 0s\n",
            " 52150K .......... .......... .......... .......... .......... 91%  241M 0s\n",
            " 52200K .......... .......... .......... .......... .......... 91%  300M 0s\n",
            " 52250K .......... .......... .......... .......... .......... 91%  300M 0s\n",
            " 52300K .......... .......... .......... .......... .......... 91%  123M 0s\n",
            " 52350K .......... .......... .......... .......... .......... 91%  241M 0s\n",
            " 52400K .......... .......... .......... .......... .......... 91%  268M 0s\n",
            " 52450K .......... .......... .......... .......... .......... 91%  177M 0s\n",
            " 52500K .......... .......... .......... .......... .......... 92%  188M 0s\n",
            " 52550K .......... .......... .......... .......... .......... 92%  189M 0s\n",
            " 52600K .......... .......... .......... .......... .......... 92%  165M 0s\n",
            " 52650K .......... .......... .......... .......... .......... 92%  213M 0s\n",
            " 52700K .......... .......... .......... .......... .......... 92%  215M 0s\n",
            " 52750K .......... .......... .......... .......... .......... 92%  264M 0s\n",
            " 52800K .......... .......... .......... .......... .......... 92%  287M 0s\n",
            " 52850K .......... .......... .......... .......... .......... 92%  202M 0s\n",
            " 52900K .......... .......... .......... .......... .......... 92%  220M 0s\n",
            " 52950K .......... .......... .......... .......... .......... 92%  190M 0s\n",
            " 53000K .......... .......... .......... .......... .......... 92%  340M 0s\n",
            " 53050K .......... .......... .......... .......... .......... 92%  274M 0s\n",
            " 53100K .......... .......... .......... .......... .......... 93%  205M 0s\n",
            " 53150K .......... .......... .......... .......... .......... 93%  280M 0s\n",
            " 53200K .......... .......... .......... .......... .......... 93%  331M 0s\n",
            " 53250K .......... .......... .......... .......... .......... 93%  295M 0s\n",
            " 53300K .......... .......... .......... .......... .......... 93%  281M 0s\n",
            " 53350K .......... .......... .......... .......... .......... 93%  200M 0s\n",
            " 53400K .......... .......... .......... .......... .......... 93%  110M 0s\n",
            " 53450K .......... .......... .......... .......... .......... 93%  166M 0s\n",
            " 53500K .......... .......... .......... .......... .......... 93%  188M 0s\n",
            " 53550K .......... .......... .......... .......... .......... 93%  275M 0s\n",
            " 53600K .......... .......... .......... .......... .......... 93%  354M 0s\n",
            " 53650K .......... .......... .......... .......... .......... 94%  304M 0s\n",
            " 53700K .......... .......... .......... .......... .......... 94%  292M 0s\n",
            " 53750K .......... .......... .......... .......... .......... 94%  281M 0s\n",
            " 53800K .......... .......... .......... .......... .......... 94%  230M 0s\n",
            " 53850K .......... .......... .......... .......... .......... 94%  236M 0s\n",
            " 53900K .......... .......... .......... .......... .......... 94%  300M 0s\n",
            " 53950K .......... .......... .......... .......... .......... 94%  163M 0s\n",
            " 54000K .......... .......... .......... .......... .......... 94%  157M 0s\n",
            " 54050K .......... .......... .......... .......... .......... 94%  182M 0s\n",
            " 54100K .......... .......... .......... .......... .......... 94%  215M 0s\n",
            " 54150K .......... .......... .......... .......... .......... 94%  313M 0s\n",
            " 54200K .......... .......... .......... .......... .......... 95%  267M 0s\n",
            " 54250K .......... .......... .......... .......... .......... 95%  122M 0s\n",
            " 54300K .......... .......... .......... .......... .......... 95%  133M 0s\n",
            " 54350K .......... .......... .......... .......... .......... 95%  105M 0s\n",
            " 54400K .......... .......... .......... .......... .......... 95%  145M 0s\n",
            " 54450K .......... .......... .......... .......... .......... 95%  211M 0s\n",
            " 54500K .......... .......... .......... .......... .......... 95%  188M 0s\n",
            " 54550K .......... .......... .......... .......... .......... 95%  111M 0s\n",
            " 54600K .......... .......... .......... .......... .......... 95%  204M 0s\n",
            " 54650K .......... .......... .......... .......... .......... 95%  217M 0s\n",
            " 54700K .......... .......... .......... .......... .......... 95%  173M 0s\n",
            " 54750K .......... .......... .......... .......... .......... 95%  185M 0s\n",
            " 54800K .......... .......... .......... .......... .......... 96%  323M 0s\n",
            " 54850K .......... .......... .......... .......... .......... 96%  345M 0s\n",
            " 54900K .......... .......... .......... .......... .......... 96%  293M 0s\n",
            " 54950K .......... .......... .......... .......... .......... 96%  140M 0s\n",
            " 55000K .......... .......... .......... .......... .......... 96%  297M 0s\n",
            " 55050K .......... .......... .......... .......... .......... 96%  240M 0s\n",
            " 55100K .......... .......... .......... .......... .......... 96%  335M 0s\n",
            " 55150K .......... .......... .......... .......... .......... 96%  288M 0s\n",
            " 55200K .......... .......... .......... .......... .......... 96%  295M 0s\n",
            " 55250K .......... .......... .......... .......... .......... 96%  273M 0s\n",
            " 55300K .......... .......... .......... .......... .......... 96%  314M 0s\n",
            " 55350K .......... .......... .......... .......... .......... 97%  297M 0s\n",
            " 55400K .......... .......... .......... .......... .......... 97%  283M 0s\n",
            " 55450K .......... .......... .......... .......... .......... 97%  309M 0s\n",
            " 55500K .......... .......... .......... .......... .......... 97%  284M 0s\n",
            " 55550K .......... .......... .......... .......... .......... 97%  255M 0s\n",
            " 55600K .......... .......... .......... .......... .......... 97%  304M 0s\n",
            " 55650K .......... .......... .......... .......... .......... 97%  327M 0s\n",
            " 55700K .......... .......... .......... .......... .......... 97%  352M 0s\n",
            " 55750K .......... .......... .......... .......... .......... 97%  217M 0s\n",
            " 55800K .......... .......... .......... .......... .......... 97%  205M 0s\n",
            " 55850K .......... .......... .......... .......... .......... 97%  209M 0s\n",
            " 55900K .......... .......... .......... .......... .......... 97%  210M 0s\n",
            " 55950K .......... .......... .......... .......... .......... 98%  179M 0s\n",
            " 56000K .......... .......... .......... .......... .......... 98%  131M 0s\n",
            " 56050K .......... .......... .......... .......... .......... 98%  189M 0s\n",
            " 56100K .......... .......... .......... .......... .......... 98%  223M 0s\n",
            " 56150K .......... .......... .......... .......... .......... 98%  173M 0s\n",
            " 56200K .......... .......... .......... .......... .......... 98%  303M 0s\n",
            " 56250K .......... .......... .......... .......... .......... 98%  362M 0s\n",
            " 56300K .......... .......... .......... .......... .......... 98%  263M 0s\n",
            " 56350K .......... .......... .......... .......... .......... 98%  281M 0s\n",
            " 56400K .......... .......... .......... .......... .......... 98%  317M 0s\n",
            " 56450K .......... .......... .......... .......... .......... 98%  249M 0s\n",
            " 56500K .......... .......... .......... .......... .......... 99%  228M 0s\n",
            " 56550K .......... .......... .......... .......... .......... 99%  203M 0s\n",
            " 56600K .......... .......... .......... .......... .......... 99%  260M 0s\n",
            " 56650K .......... .......... .......... .......... .......... 99%  359M 0s\n",
            " 56700K .......... .......... .......... .......... .......... 99%  270M 0s\n",
            " 56750K .......... .......... .......... .......... .......... 99%  189M 0s\n",
            " 56800K .......... .......... .......... .......... .......... 99%  189M 0s\n",
            " 56850K .......... .......... .......... .......... .......... 99%  207M 0s\n",
            " 56900K .......... .......... .......... .......... .......... 99%  338M 0s\n",
            " 56950K .......... .......... .......... .......... .......... 99%  231M 0s\n",
            " 57000K .......... .......... .......... .......... .......... 99%  335M 0s\n",
            " 57050K .......... .......... .......... .......... ........  100%  336M=0.3s\n",
            "\n",
            "2020-12-09 23:10:37 (162 MB/s) - ‘Miniconda3-4.5.4-Linux-x86_64.sh’ saved [58468498/58468498]\n",
            "\n",
            "Python 3.6.5 :: Anaconda, Inc.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSMo73o3IOTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3be7a589-4eb5-4d7d-d6c6-59840e50a493"
      },
      "source": [
        "%%bash\n",
        "conda install --channel defaults conda python=3.6 --yes\n",
        "conda update --channel defaults --all --yes"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs: \n",
            "    - conda\n",
            "    - python=3.6\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    yaml-0.2.5                 |       h7b6447c_0          87 KB\n",
            "    pyopenssl-20.0.0           |     pyhd3eb1b0_1          48 KB\n",
            "    openssl-1.1.1i             |       h27cfd23_0         3.8 MB\n",
            "    python-3.6.12              |       hcff3b4d_2        34.0 MB\n",
            "    setuptools-51.0.0          |   py36h06a4308_2         936 KB\n",
            "    pycosat-0.6.3              |   py36h27cfd23_0         107 KB\n",
            "    sqlite-3.33.0              |       h62c20be_0         2.0 MB\n",
            "    zlib-1.2.11                |       h7b6447c_3         120 KB\n",
            "    cryptography-3.3           |   py36h3c74f83_0         626 KB\n",
            "    idna-2.10                  |             py_0          56 KB\n",
            "    xz-5.2.5                   |       h7b6447c_0         438 KB\n",
            "    readline-8.0               |       h7b6447c_0         428 KB\n",
            "    ld_impl_linux-64-2.33.1    |       h53a641e_7         645 KB\n",
            "    requests-2.25.0            |     pyhd3eb1b0_0          51 KB\n",
            "    tqdm-4.54.1                |     pyhd3eb1b0_0          54 KB\n",
            "    pycparser-2.20             |             py_2          94 KB\n",
            "    pip-20.3.1                 |   py36h06a4308_0         2.0 MB\n",
            "    libgcc-ng-9.1.0            |       hdf63c60_0         8.1 MB\n",
            "    ruamel_yaml-0.15.87        |   py36h7b6447c_1         256 KB\n",
            "    pysocks-1.7.1              |   py36h06a4308_0          30 KB\n",
            "    conda-package-handling-1.7.2|   py36h03888b9_0         967 KB\n",
            "    ca-certificates-2020.12.8  |       h06a4308_0         128 KB\n",
            "    tk-8.6.10                  |       hbc83047_0         3.2 MB\n",
            "    urllib3-1.25.11            |             py_0          93 KB\n",
            "    libedit-3.1.20191231       |       h14c3975_1         121 KB\n",
            "    brotlipy-0.7.0             |py36h27cfd23_1003         349 KB\n",
            "    six-1.15.0                 |   py36h06a4308_0          27 KB\n",
            "    cffi-1.14.4                |   py36h261ae71_0         224 KB\n",
            "    _libgcc_mutex-0.1          |             main           3 KB\n",
            "    certifi-2020.12.5          |   py36h06a4308_0         144 KB\n",
            "    libstdcxx-ng-9.1.0         |       hdf63c60_0         4.0 MB\n",
            "    conda-4.9.2                |   py36h06a4308_0         3.1 MB\n",
            "    wheel-0.36.1               |     pyhd3eb1b0_0          31 KB\n",
            "    ncurses-6.2                |       he6710b0_1         1.1 MB\n",
            "    libffi-3.3                 |       he6710b0_2          54 KB\n",
            "    chardet-3.0.4              |py36h06a4308_1003         197 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        67.6 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "    _libgcc_mutex:          0.1-main               \n",
            "    brotlipy:               0.7.0-py36h27cfd23_1003\n",
            "    conda-package-handling: 1.7.2-py36h03888b9_0   \n",
            "    ld_impl_linux-64:       2.33.1-h53a641e_7      \n",
            "    tqdm:                   4.54.1-pyhd3eb1b0_0    \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    ca-certificates:        2018.03.07-0            --> 2020.12.8-h06a4308_0    \n",
            "    certifi:                2018.4.16-py36_0        --> 2020.12.5-py36h06a4308_0\n",
            "    cffi:                   1.11.5-py36h9745a5d_0   --> 1.14.4-py36h261ae71_0   \n",
            "    chardet:                3.0.4-py36h0f667ec_1    --> 3.0.4-py36h06a4308_1003 \n",
            "    conda:                  4.5.4-py36_0            --> 4.9.2-py36h06a4308_0    \n",
            "    cryptography:           2.2.2-py36h14c3975_0    --> 3.3-py36h3c74f83_0      \n",
            "    idna:                   2.6-py36h82fb2a8_1      --> 2.10-py_0               \n",
            "    libedit:                3.1.20170329-h6b74fdf_2 --> 3.1.20191231-h14c3975_1 \n",
            "    libffi:                 3.2.1-hd88cf55_4        --> 3.3-he6710b0_2          \n",
            "    libgcc-ng:              7.2.0-hdf63c60_3        --> 9.1.0-hdf63c60_0        \n",
            "    libstdcxx-ng:           7.2.0-hdf63c60_3        --> 9.1.0-hdf63c60_0        \n",
            "    ncurses:                6.1-hf484d3e_0          --> 6.2-he6710b0_1          \n",
            "    openssl:                1.0.2o-h20670df_0       --> 1.1.1i-h27cfd23_0       \n",
            "    pip:                    10.0.1-py36_0           --> 20.3.1-py36h06a4308_0   \n",
            "    pycosat:                0.6.3-py36h0a5515d_0    --> 0.6.3-py36h27cfd23_0    \n",
            "    pycparser:              2.18-py36hf9f622e_1     --> 2.20-py_2               \n",
            "    pyopenssl:              18.0.0-py36_0           --> 20.0.0-pyhd3eb1b0_1     \n",
            "    pysocks:                1.6.8-py36_0            --> 1.7.1-py36h06a4308_0    \n",
            "    python:                 3.6.5-hc3d631a_2        --> 3.6.12-hcff3b4d_2       \n",
            "    readline:               7.0-ha6073c6_4          --> 8.0-h7b6447c_0          \n",
            "    requests:               2.18.4-py36he2e5f8d_1   --> 2.25.0-pyhd3eb1b0_0     \n",
            "    ruamel_yaml:            0.15.37-py36h14c3975_2  --> 0.15.87-py36h7b6447c_1  \n",
            "    setuptools:             39.2.0-py36_0           --> 51.0.0-py36h06a4308_2   \n",
            "    six:                    1.11.0-py36h372c433_1   --> 1.15.0-py36h06a4308_0   \n",
            "    sqlite:                 3.23.1-he433501_0       --> 3.33.0-h62c20be_0       \n",
            "    tk:                     8.6.7-hc745277_3        --> 8.6.10-hbc83047_0       \n",
            "    urllib3:                1.22-py36hbe7ace6_0     --> 1.25.11-py_0            \n",
            "    wheel:                  0.31.1-py36_0           --> 0.36.1-pyhd3eb1b0_0     \n",
            "    xz:                     5.2.4-h14c3975_4        --> 5.2.5-h7b6447c_0        \n",
            "    yaml:                   0.1.7-had09818_2        --> 0.2.5-h7b6447c_0        \n",
            "    zlib:                   1.2.11-ha838bed_2       --> 1.2.11-h7b6447c_3       \n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "\n",
            "The following packages will be REMOVED:\n",
            "\n",
            "  asn1crypto-0.24.0-py36_0\n",
            "  conda-env-2.6.0-h36134e3_1\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\ryaml-0.2.5           |   87 KB |            |   0% \ryaml-0.2.5           |   87 KB | ########## | 100% \n",
            "\rpyopenssl-20.0.0     |   48 KB |            |   0% \rpyopenssl-20.0.0     |   48 KB | ########## | 100% \n",
            "\ropenssl-1.1.1i       |  3.8 MB |            |   0% \ropenssl-1.1.1i       |  3.8 MB | ##8        |  29% \ropenssl-1.1.1i       |  3.8 MB | #######8   |  78% \ropenssl-1.1.1i       |  3.8 MB | #########7 |  98% \ropenssl-1.1.1i       |  3.8 MB | ########## | 100% \n",
            "\rpython-3.6.12        | 34.0 MB |            |   0% \rpython-3.6.12        | 34.0 MB | ##3        |  23% \rpython-3.6.12        | 34.0 MB | #####8     |  58% \rpython-3.6.12        | 34.0 MB | #######5   |  75% \rpython-3.6.12        | 34.0 MB | #########  |  90% \rpython-3.6.12        | 34.0 MB | ########## | 100% \n",
            "\rsetuptools-51.0.0    |  936 KB |            |   0% \rsetuptools-51.0.0    |  936 KB | ########1  |  82% \rsetuptools-51.0.0    |  936 KB | ########## | 100% \n",
            "\rpycosat-0.6.3        |  107 KB |            |   0% \rpycosat-0.6.3        |  107 KB | ########## | 100% \n",
            "\rsqlite-3.33.0        |  2.0 MB |            |   0% \rsqlite-3.33.0        |  2.0 MB | ########1  |  81% \rsqlite-3.33.0        |  2.0 MB | ########## | 100% \n",
            "\rzlib-1.2.11          |  120 KB |            |   0% \rzlib-1.2.11          |  120 KB | ########## | 100% \n",
            "\rcryptography-3.3     |  626 KB |            |   0% \rcryptography-3.3     |  626 KB | ########   |  81% \rcryptography-3.3     |  626 KB | ########## | 100% \n",
            "\ridna-2.10            |   56 KB |            |   0% \ridna-2.10            |   56 KB | ########## | 100% \n",
            "\rxz-5.2.5             |  438 KB |            |   0% \rxz-5.2.5             |  438 KB | ########## | 100% \n",
            "\rreadline-8.0         |  428 KB |            |   0% \rreadline-8.0         |  428 KB | ########## | 100% \n",
            "\rld_impl_linux-64-2.3 |  645 KB |            |   0% \rld_impl_linux-64-2.3 |  645 KB | #########6 |  96% \rld_impl_linux-64-2.3 |  645 KB | ########## | 100% \n",
            "\rrequests-2.25.0      |   51 KB |            |   0% \rrequests-2.25.0      |   51 KB | ########## | 100% \n",
            "\rtqdm-4.54.1          |   54 KB |            |   0% \rtqdm-4.54.1          |   54 KB | ########## | 100% \n",
            "\rpycparser-2.20       |   94 KB |            |   0% \rpycparser-2.20       |   94 KB | ########## | 100% \n",
            "\rpip-20.3.1           |  2.0 MB |            |   0% \rpip-20.3.1           |  2.0 MB | #######7   |  78% \rpip-20.3.1           |  2.0 MB | #########3 |  94% \rpip-20.3.1           |  2.0 MB | ########## | 100% \n",
            "\rlibgcc-ng-9.1.0      |  8.1 MB |            |   0% \rlibgcc-ng-9.1.0      |  8.1 MB | #######5   |  75% \rlibgcc-ng-9.1.0      |  8.1 MB | #########7 |  98% \rlibgcc-ng-9.1.0      |  8.1 MB | ########## | 100% \n",
            "\rruamel_yaml-0.15.87  |  256 KB |            |   0% \rruamel_yaml-0.15.87  |  256 KB | #########6 |  97% \rruamel_yaml-0.15.87  |  256 KB | ########## | 100% \n",
            "\rpysocks-1.7.1        |   30 KB |            |   0% \rpysocks-1.7.1        |   30 KB | ########## | 100% \n",
            "\rconda-package-handli |  967 KB |            |   0% \rconda-package-handli |  967 KB | ########7  |  88% \rconda-package-handli |  967 KB | ########## | 100% \n",
            "\rca-certificates-2020 |  128 KB |            |   0% \rca-certificates-2020 |  128 KB | ########## | 100% \n",
            "\rtk-8.6.10            |  3.2 MB |            |   0% \rtk-8.6.10            |  3.2 MB | #######8   |  79% \rtk-8.6.10            |  3.2 MB | ########## | 100% \n",
            "\rurllib3-1.25.11      |   93 KB |            |   0% \rurllib3-1.25.11      |   93 KB | ########## | 100% \n",
            "\rlibedit-3.1.20191231 |  121 KB |            |   0% \rlibedit-3.1.20191231 |  121 KB | ########## | 100% \n",
            "\rbrotlipy-0.7.0       |  349 KB |            |   0% \rbrotlipy-0.7.0       |  349 KB | ########## | 100% \n",
            "\rsix-1.15.0           |   27 KB |            |   0% \rsix-1.15.0           |   27 KB | ########## | 100% \n",
            "\rcffi-1.14.4          |  224 KB |            |   0% \rcffi-1.14.4          |  224 KB | ########## | 100% \n",
            "\r_libgcc_mutex-0.1    |    3 KB |            |   0% \r_libgcc_mutex-0.1    |    3 KB | ########## | 100% \n",
            "\rcertifi-2020.12.5    |  144 KB |            |   0% \rcertifi-2020.12.5    |  144 KB | ########## | 100% \n",
            "\rlibstdcxx-ng-9.1.0   |  4.0 MB |            |   0% \rlibstdcxx-ng-9.1.0   |  4.0 MB | #######6   |  77% \rlibstdcxx-ng-9.1.0   |  4.0 MB | #########6 |  97% \rlibstdcxx-ng-9.1.0   |  4.0 MB | ########## | 100% \n",
            "\rconda-4.9.2          |  3.1 MB |            |   0% \rconda-4.9.2          |  3.1 MB | #######9   |  79% \rconda-4.9.2          |  3.1 MB | #########6 |  97% \rconda-4.9.2          |  3.1 MB | ########## | 100% \n",
            "\rwheel-0.36.1         |   31 KB |            |   0% \rwheel-0.36.1         |   31 KB | ########## | 100% \n",
            "\rncurses-6.2          |  1.1 MB |            |   0% \rncurses-6.2          |  1.1 MB | #######8   |  78% \rncurses-6.2          |  1.1 MB | ########## | 100% \n",
            "\rlibffi-3.3           |   54 KB |            |   0% \rlibffi-3.3           |   54 KB | ########## | 100% \n",
            "\rchardet-3.0.4        |  197 KB |            |   0% \rchardet-3.0.4        |  197 KB | ########## | 100% \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yKK3EdlIQnr"
      },
      "source": [
        "import sys\n",
        "_ = (sys.path.append(\"/usr/local/lib/python3.6/site-packages\"))\n",
        "_ = (sys.path.append(\"/content/DeepPurpose/\"))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHH3EKwiISQp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c66c133-7c71-4881-8082-6b0e2088f93b"
      },
      "source": [
        "!conda install --channel conda-forge featuretools --yes"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - featuretools\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    bokeh-2.2.3                |   py36h5fab9bb_0         7.0 MB  conda-forge\n",
            "    ca-certificates-2020.12.5  |       ha878542_0         137 KB  conda-forge\n",
            "    certifi-2020.12.5          |   py36h5fab9bb_0         143 KB  conda-forge\n",
            "    click-7.1.2                |     pyh9f0ad1d_0          64 KB  conda-forge\n",
            "    cloudpickle-1.6.0          |             py_0          22 KB  conda-forge\n",
            "    conda-4.9.2                |   py36h5fab9bb_0         3.0 MB  conda-forge\n",
            "    contextvars-2.4            |             py_0          11 KB  conda-forge\n",
            "    cytoolz-0.11.0             |   py36h1d69622_1         368 KB  conda-forge\n",
            "    dask-2.30.0                |             py_0           4 KB  conda-forge\n",
            "    dask-core-2.30.0           |             py_0         639 KB  conda-forge\n",
            "    distributed-2.30.1         |   py36h5fab9bb_0         1.1 MB  conda-forge\n",
            "    featuretools-0.22.0        |     pyhd8ed1ab_0         256 KB  conda-forge\n",
            "    freetype-2.10.4            |       h7ca028e_0         912 KB  conda-forge\n",
            "    fsspec-0.8.4               |             py_0          65 KB  conda-forge\n",
            "    heapdict-1.0.1             |             py_0           7 KB  conda-forge\n",
            "    immutables-0.14            |   py36h8c4c3a4_1          68 KB  conda-forge\n",
            "    jinja2-2.11.2              |     pyh9f0ad1d_0          93 KB  conda-forge\n",
            "    jpeg-9d                    |       h36c2ea0_0         264 KB  conda-forge\n",
            "    lcms2-2.11                 |       hcbb858e_1         434 KB  conda-forge\n",
            "    libblas-3.9.0              |       3_openblas          11 KB  conda-forge\n",
            "    libcblas-3.9.0             |       3_openblas          11 KB  conda-forge\n",
            "    libgfortran-ng-7.5.0       |      hae1eefd_17          22 KB  conda-forge\n",
            "    libgfortran4-7.5.0         |      hae1eefd_17         1.3 MB  conda-forge\n",
            "    liblapack-3.9.0            |       3_openblas          11 KB  conda-forge\n",
            "    libopenblas-0.3.12         |pthreads_hb3c22a3_1         8.2 MB  conda-forge\n",
            "    libpng-1.6.37              |       h21135ba_2         306 KB  conda-forge\n",
            "    libtiff-4.1.0              |       h4f3a223_6         618 KB  conda-forge\n",
            "    libwebp-base-1.1.0         |       h36c2ea0_3         864 KB  conda-forge\n",
            "    locket-0.2.0               |             py_2           6 KB  conda-forge\n",
            "    lz4-c-1.9.2                |       he1b5a44_3         203 KB  conda-forge\n",
            "    markupsafe-1.1.1           |   py36he6145b8_2          27 KB  conda-forge\n",
            "    msgpack-python-1.0.0       |   py36h51d7077_2          91 KB  conda-forge\n",
            "    numpy-1.19.4               |   py36h8732dcd_1         5.2 MB  conda-forge\n",
            "    olefile-0.46               |     pyh9f0ad1d_1          32 KB  conda-forge\n",
            "    openssl-1.1.1h             |       h516909a_0         2.1 MB  conda-forge\n",
            "    packaging-20.7             |     pyhd3deb0d_0          32 KB  conda-forge\n",
            "    pandas-1.0.1               |   py36hb3f55d8_0        11.1 MB  conda-forge\n",
            "    partd-1.1.0                |             py_0          17 KB  conda-forge\n",
            "    pillow-8.0.1               |   py36h10ecd5c_0         687 KB  conda-forge\n",
            "    psutil-5.7.3               |   py36he6145b8_0         338 KB  conda-forge\n",
            "    pyparsing-2.4.7            |     pyh9f0ad1d_0          60 KB  conda-forge\n",
            "    python-dateutil-2.8.1      |             py_0         220 KB  conda-forge\n",
            "    python_abi-3.6             |          1_cp36m           4 KB  conda-forge\n",
            "    pytz-2020.4                |     pyhd8ed1ab_0         229 KB  conda-forge\n",
            "    pyyaml-5.3.1               |   py36he6145b8_1         185 KB  conda-forge\n",
            "    scipy-1.5.3                |   py36h976291a_0        18.6 MB  conda-forge\n",
            "    sortedcontainers-2.3.0     |     pyhd8ed1ab_0          26 KB  conda-forge\n",
            "    tblib-1.6.0                |             py_0          14 KB  conda-forge\n",
            "    toolz-0.11.1               |             py_0          46 KB  conda-forge\n",
            "    tornado-6.1                |   py36h1d69622_0         644 KB  conda-forge\n",
            "    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge\n",
            "    zict-2.0.0                 |             py_0          10 KB  conda-forge\n",
            "    zstd-1.4.5                 |       h6597ccf_2         712 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        66.3 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  bokeh              conda-forge/linux-64::bokeh-2.2.3-py36h5fab9bb_0\n",
            "  click              conda-forge/noarch::click-7.1.2-pyh9f0ad1d_0\n",
            "  cloudpickle        conda-forge/noarch::cloudpickle-1.6.0-py_0\n",
            "  contextvars        conda-forge/noarch::contextvars-2.4-py_0\n",
            "  cytoolz            conda-forge/linux-64::cytoolz-0.11.0-py36h1d69622_1\n",
            "  dask               conda-forge/noarch::dask-2.30.0-py_0\n",
            "  dask-core          conda-forge/noarch::dask-core-2.30.0-py_0\n",
            "  distributed        conda-forge/linux-64::distributed-2.30.1-py36h5fab9bb_0\n",
            "  featuretools       conda-forge/noarch::featuretools-0.22.0-pyhd8ed1ab_0\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.4-h7ca028e_0\n",
            "  fsspec             conda-forge/noarch::fsspec-0.8.4-py_0\n",
            "  heapdict           conda-forge/noarch::heapdict-1.0.1-py_0\n",
            "  immutables         conda-forge/linux-64::immutables-0.14-py36h8c4c3a4_1\n",
            "  jinja2             conda-forge/noarch::jinja2-2.11.2-pyh9f0ad1d_0\n",
            "  jpeg               conda-forge/linux-64::jpeg-9d-h36c2ea0_0\n",
            "  lcms2              conda-forge/linux-64::lcms2-2.11-hcbb858e_1\n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-3_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-3_openblas\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-7.5.0-hae1eefd_17\n",
            "  libgfortran4       conda-forge/linux-64::libgfortran4-7.5.0-hae1eefd_17\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-3_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.12-pthreads_hb3c22a3_1\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.37-h21135ba_2\n",
            "  libtiff            conda-forge/linux-64::libtiff-4.1.0-h4f3a223_6\n",
            "  libwebp-base       conda-forge/linux-64::libwebp-base-1.1.0-h36c2ea0_3\n",
            "  locket             conda-forge/noarch::locket-0.2.0-py_2\n",
            "  lz4-c              conda-forge/linux-64::lz4-c-1.9.2-he1b5a44_3\n",
            "  markupsafe         conda-forge/linux-64::markupsafe-1.1.1-py36he6145b8_2\n",
            "  msgpack-python     conda-forge/linux-64::msgpack-python-1.0.0-py36h51d7077_2\n",
            "  numpy              conda-forge/linux-64::numpy-1.19.4-py36h8732dcd_1\n",
            "  olefile            conda-forge/noarch::olefile-0.46-pyh9f0ad1d_1\n",
            "  packaging          conda-forge/noarch::packaging-20.7-pyhd3deb0d_0\n",
            "  pandas             conda-forge/linux-64::pandas-1.0.1-py36hb3f55d8_0\n",
            "  partd              conda-forge/noarch::partd-1.1.0-py_0\n",
            "  pillow             conda-forge/linux-64::pillow-8.0.1-py36h10ecd5c_0\n",
            "  psutil             conda-forge/linux-64::psutil-5.7.3-py36he6145b8_0\n",
            "  pyparsing          conda-forge/noarch::pyparsing-2.4.7-pyh9f0ad1d_0\n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.1-py_0\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.6-1_cp36m\n",
            "  pytz               conda-forge/noarch::pytz-2020.4-pyhd8ed1ab_0\n",
            "  pyyaml             conda-forge/linux-64::pyyaml-5.3.1-py36he6145b8_1\n",
            "  scipy              conda-forge/linux-64::scipy-1.5.3-py36h976291a_0\n",
            "  sortedcontainers   conda-forge/noarch::sortedcontainers-2.3.0-pyhd8ed1ab_0\n",
            "  tblib              conda-forge/noarch::tblib-1.6.0-py_0\n",
            "  toolz              conda-forge/noarch::toolz-0.11.1-py_0\n",
            "  tornado            conda-forge/linux-64::tornado-6.1-py36h1d69622_0\n",
            "  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0\n",
            "  zict               conda-forge/noarch::zict-2.0.0-py_0\n",
            "  zstd               conda-forge/linux-64::zstd-1.4.5-h6597ccf_2\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2020.12.8-~ --> conda-forge::ca-certificates-2020.12.5-ha878542_0\n",
            "  certifi            pkgs/main::certifi-2020.12.5-py36h06a~ --> conda-forge::certifi-2020.12.5-py36h5fab9bb_0\n",
            "  conda               pkgs/main::conda-4.9.2-py36h06a4308_0 --> conda-forge::conda-4.9.2-py36h5fab9bb_0\n",
            "  openssl              pkgs/main::openssl-1.1.1i-h27cfd23_0 --> conda-forge::openssl-1.1.1h-h516909a_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "libcblas-3.9.0       | 11 KB     | : 100% 1.0/1 [00:00<00:00, 13.81it/s]\n",
            "numpy-1.19.4         | 5.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.01it/s]\n",
            "msgpack-python-1.0.0 | 91 KB     | : 100% 1.0/1 [00:00<00:00, 24.95it/s]\n",
            "libwebp-base-1.1.0   | 864 KB    | : 100% 1.0/1 [00:00<00:00,  6.47it/s]\n",
            "cytoolz-0.11.0       | 368 KB    | : 100% 1.0/1 [00:00<00:00, 10.07it/s]\n",
            "toolz-0.11.1         | 46 KB     | : 100% 1.0/1 [00:00<00:00, 27.32it/s]\n",
            "heapdict-1.0.1       | 7 KB      | : 100% 1.0/1 [00:00<00:00, 33.55it/s]\n",
            "immutables-0.14      | 68 KB     | : 100% 1.0/1 [00:00<00:00, 21.75it/s]\n",
            "python-dateutil-2.8. | 220 KB    | : 100% 1.0/1 [00:00<00:00, 19.52it/s]\n",
            "packaging-20.7       | 32 KB     | : 100% 1.0/1 [00:00<00:00, 24.66it/s]\n",
            "jinja2-2.11.2        | 93 KB     | : 100% 1.0/1 [00:00<00:00, 18.82it/s]\n",
            "ca-certificates-2020 | 137 KB    | : 100% 1.0/1 [00:00<00:00, 22.52it/s]\n",
            "fsspec-0.8.4         | 65 KB     | : 100% 1.0/1 [00:00<00:00, 26.51it/s]\n",
            "pandas-1.0.1         | 11.1 MB   | : 100% 1.0/1 [00:02<00:00,  2.07s/it]\n",
            "dask-2.30.0          | 4 KB      | : 100% 1.0/1 [00:00<00:00, 40.46it/s]\n",
            "jpeg-9d              | 264 KB    | : 100% 1.0/1 [00:00<00:00, 14.05it/s]\n",
            "libgfortran4-7.5.0   | 1.3 MB    | : 100% 1.0/1 [00:00<00:00,  4.28it/s]\n",
            "libtiff-4.1.0        | 618 KB    | : 100% 1.0/1 [00:00<00:00,  7.82it/s]\n",
            "contextvars-2.4      | 11 KB     | : 100% 1.0/1 [00:00<00:00, 37.01it/s]\n",
            "lcms2-2.11           | 434 KB    | : 100% 1.0/1 [00:00<00:00, 10.39it/s]\n",
            "freetype-2.10.4      | 912 KB    | : 100% 1.0/1 [00:00<00:00,  5.67it/s]\n",
            "tblib-1.6.0          | 14 KB     | : 100% 1.0/1 [00:00<00:00, 31.31it/s]\n",
            "zict-2.0.0           | 10 KB     | : 100% 1.0/1 [00:00<00:00, 28.96it/s]\n",
            "libpng-1.6.37        | 306 KB    | : 100% 1.0/1 [00:00<00:00, 12.04it/s]\n",
            "featuretools-0.22.0  | 256 KB    | : 100% 1.0/1 [00:00<00:00,  8.27it/s]\n",
            "certifi-2020.12.5    | 143 KB    | : 100% 1.0/1 [00:00<00:00, 20.46it/s]\n",
            "locket-0.2.0         | 6 KB      | : 100% 1.0/1 [00:00<00:00, 37.43it/s]\n",
            "python_abi-3.6       | 4 KB      | : 100% 1.0/1 [00:00<00:00, 29.76it/s]\n",
            "zstd-1.4.5           | 712 KB    | : 100% 1.0/1 [00:00<00:00,  7.40it/s]\n",
            "sortedcontainers-2.3 | 26 KB     | : 100% 1.0/1 [00:00<00:00, 29.54it/s]\n",
            "pytz-2020.4          | 229 KB    | : 100% 1.0/1 [00:00<00:00,  9.40it/s]\n",
            "markupsafe-1.1.1     | 27 KB     | : 100% 1.0/1 [00:00<00:00, 29.42it/s]\n",
            "liblapack-3.9.0      | 11 KB     | : 100% 1.0/1 [00:00<00:00, 31.57it/s]\n",
            "lz4-c-1.9.2          | 203 KB    | : 100% 1.0/1 [00:00<00:00, 15.74it/s]\n",
            "psutil-5.7.3         | 338 KB    | : 100% 1.0/1 [00:00<00:00, 10.36it/s]\n",
            "openssl-1.1.1h       | 2.1 MB    | : 100% 1.0/1 [00:00<00:00,  3.14it/s]\n",
            "dask-core-2.30.0     | 639 KB    | : 100% 1.0/1 [00:00<00:00,  5.11it/s]\n",
            "conda-4.9.2          | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.74it/s]\n",
            "distributed-2.30.1   | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  3.89it/s]\n",
            "tornado-6.1          | 644 KB    | : 100% 1.0/1 [00:00<00:00,  6.01it/s]\n",
            "typing_extensions-3. | 25 KB     | : 100% 1.0/1 [00:00<00:00, 34.71it/s]\n",
            "libblas-3.9.0        | 11 KB     | : 100% 1.0/1 [00:00<00:00, 36.14it/s]\n",
            "olefile-0.46         | 32 KB     | : 100% 1.0/1 [00:00<00:00, 32.49it/s]\n",
            "bokeh-2.2.3          | 7.0 MB    | : 100% 1.0/1 [00:01<00:00,  1.62s/it]\n",
            "libopenblas-0.3.12   | 8.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.35s/it]\n",
            "pyparsing-2.4.7      | 60 KB     | : 100% 1.0/1 [00:00<00:00, 23.92it/s]\n",
            "click-7.1.2          | 64 KB     | : 100% 1.0/1 [00:00<00:00, 24.03it/s]\n",
            "cloudpickle-1.6.0    | 22 KB     | : 100% 1.0/1 [00:00<00:00, 31.71it/s]\n",
            "libgfortran-ng-7.5.0 | 22 KB     | : 100% 1.0/1 [00:00<00:00, 34.07it/s]\n",
            "scipy-1.5.3          | 18.6 MB   | : 100% 1.0/1 [00:02<00:00,  2.90s/it]\n",
            "pyyaml-5.3.1         | 185 KB    | : 100% 1.0/1 [00:00<00:00, 15.33it/s]\n",
            "pillow-8.0.1         | 687 KB    | : 100% 1.0/1 [00:00<00:00,  7.02it/s]\n",
            "partd-1.1.0          | 17 KB     | : 100% 1.0/1 [00:00<00:00, 36.38it/s]\n",
            "Preparing transaction: / \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc0_FiWVIX7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90fd7467-0b81-4116-a44e-056028f49936"
      },
      "source": [
        "!git clone https://github.com/kexinhuang12345/DeepPurpose.git"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DeepPurpose'...\n",
            "remote: Enumerating objects: 235, done.\u001b[K\n",
            "remote: Counting objects: 100% (235/235), done.\u001b[K\n",
            "remote: Compressing objects: 100% (181/181), done.\u001b[K\n",
            "remote: Total 2696 (delta 155), reused 98 (delta 53), pack-reused 2461\u001b[K\n",
            "Receiving objects: 100% (2696/2696), 15.32 MiB | 27.71 MiB/s, done.\n",
            "Resolving deltas: 100% (1693/1693), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJfspyQfIZ1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f66dbf-b056-464c-aeab-0c66b29e5deb"
      },
      "source": [
        "%%bash\n",
        "cd /content/DeepPurpose/\n",
        "echo -y | conda env create -f environment.yml\n",
        "source activate DeepPurpose"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\rpytz-2019.3          | 237 KB    |            |   0% \rpytz-2019.3          | 237 KB    | ######     |  61% \rpytz-2019.3          | 237 KB    | ########## | 100% \n",
            "\rwcwidth-0.1.9        | 20 KB     |            |   0% \rwcwidth-0.1.9        | 20 KB     | ########## | 100% \n",
            "\rca-certificates-2020 | 125 KB    |            |   0% \rca-certificates-2020 | 125 KB    | ########## | 100% \rca-certificates-2020 | 125 KB    | ########## | 100% \n",
            "\rlibedit-3.1.20191231 | 121 KB    |            |   0% \rlibedit-3.1.20191231 | 121 KB    | ########## | 100% \n",
            "\rlibxcb-1.13          | 396 KB    |            |   0% \rlibxcb-1.13          | 396 KB    | ########## | 100% \rlibxcb-1.13          | 396 KB    | ########## | 100% \n",
            "\rnotebook-6.0.3       | 6.2 MB    |            |   0% \rnotebook-6.0.3       | 6.2 MB    | #8         |  18% \rnotebook-6.0.3       | 6.2 MB    | ########## | 100% \rnotebook-6.0.3       | 6.2 MB    | ########## | 100% \n",
            "\rpexpect-4.8.0        | 47 KB     |            |   0% \rpexpect-4.8.0        | 47 KB     | ########## | 100% \n",
            "\rimportlib_metadata-1 | 3 KB      |            |   0% \rimportlib_metadata-1 | 3 KB      | ########## | 100% \n",
            "\rexpat-2.2.9          | 191 KB    |            |   0% \rexpat-2.2.9          | 191 KB    | ########## | 100% \n",
            "\rtraitlets-4.3.3      | 133 KB    |            |   0% \rtraitlets-4.3.3      | 133 KB    | ########## | 100% \n",
            "\rlibuuid-2.32.1       | 28 KB     |            |   0% \rlibuuid-2.32.1       | 28 KB     | ########## | 100% \n",
            "\rtestpath-0.4.4       | 85 KB     |            |   0% \rtestpath-0.4.4       | 85 KB     | ########## | 100% \n",
            "\rcffi-1.14.0          | 221 KB    |            |   0% \rcffi-1.14.0          | 221 KB    | ########## | 100% \n",
            "\rmarkupsafe-1.1.1     | 27 KB     |            |   0% \rmarkupsafe-1.1.1     | 27 KB     | ########## | 100% \n",
            "\rlz4-c-1.8.3          | 187 KB    |            |   0% \rlz4-c-1.8.3          | 187 KB    | ########## | 100% \n",
            "\rprometheus_client-0. | 38 KB     |            |   0% \rprometheus_client-0. | 38 KB     | ########## | 100% \n",
            "\rdescriptastorus-2.2. | 63 KB     |            |   0% \rdescriptastorus-2.2. | 63 KB     | ########## | 100% \rdescriptastorus-2.2. | 63 KB     | ########## | 100% \n",
            "\rfontconfig-2.13.0    | 296 KB    |            |   0% \rfontconfig-2.13.0    | 296 KB    | ########## | 100% \n",
            "\rlibcblas-3.8.0       | 11 KB     |            |   0% \rlibcblas-3.8.0       | 11 KB     | ########## | 100% \n",
            "\rlibstdcxx-ng-9.3.0   | 4.0 MB    |            |   0% \rlibstdcxx-ng-9.3.0   | 4.0 MB    | ########## | 100% \rlibstdcxx-ng-9.3.0   | 4.0 MB    | ########## | 100% \n",
            "\rsix-1.14.0           | 13 KB     |            |   0% \rsix-1.14.0           | 13 KB     | ########## | 100% \n",
            "\rpixman-0.38.0        | 594 KB    |            |   0% \rpixman-0.38.0        | 594 KB    | ########## | 100% \rpixman-0.38.0        | 594 KB    | ########## | 100% \n",
            "\rpandoc-2.2.3.2       | 14.0 MB   |            |   0% \rpandoc-2.2.3.2       | 14.0 MB   | ######6    |  66% \rpandoc-2.2.3.2       | 14.0 MB   | ########## | 100% \rpandoc-2.2.3.2       | 14.0 MB   | ########## | 100% \n",
            "\rlz4-3.0.2            | 43 KB     |            |   0% \rlz4-3.0.2            | 43 KB     | ########## | 100% \n",
            "\rscipy-1.4.1          | 18.8 MB   |            |   0% \rscipy-1.4.1          | 18.8 MB   | ####6      |  47% \rscipy-1.4.1          | 18.8 MB   | ########## | 100% \rscipy-1.4.1          | 18.8 MB   | ########## | 100% \n",
            "\rmatplotlib-base-3.1. | 5.0 MB    |            |   0% \rmatplotlib-base-3.1. | 5.0 MB    | ########## | 100% \rmatplotlib-base-3.1. | 5.0 MB    | ########## | 100% \n",
            "\rlibiconv-1.15        | 2.0 MB    |            |   0% \rlibiconv-1.15        | 2.0 MB    | ########## | 100% \rlibiconv-1.15        | 2.0 MB    | ########## | 100% \n",
            "\rnumpy-base-1.18.1    | 4.2 MB    |            |   0% \rnumpy-base-1.18.1    | 4.2 MB    | ########## | 100% \rnumpy-base-1.18.1    | 4.2 MB    | ########## | 100% \n",
            "\rpip-20.0.2           | 1.0 MB    |            |   0% \rpip-20.0.2           | 1.0 MB    | ########## | 100% \rpip-20.0.2           | 1.0 MB    | ########## | 100% \n",
            "\rxorg-libxau-1.0.9    | 13 KB     |            |   0% \rxorg-libxau-1.0.9    | 13 KB     | ########## | 100% \n",
            "\rllvm-openmp-9.0.1    | 782 KB    |            |   0% \rllvm-openmp-9.0.1    | 782 KB    | ########## | 100% \rllvm-openmp-9.0.1    | 782 KB    | ########## | 100% \n",
            "\rwheel-0.34.2         | 24 KB     |            |   0% \rwheel-0.34.2         | 24 KB     | ########## | 100% \n",
            "\rliblapack-3.8.0      | 11 KB     |            |   0% \rliblapack-3.8.0      | 11 KB     | ########## | 100% \n",
            "\rtqdm-4.44.1          | 48 KB     |            |   0% \rtqdm-4.44.1          | 48 KB     | ########## | 100% \n",
            "\rfreetype-2.9.1       | 882 KB    |            |   0% \rfreetype-2.9.1       | 882 KB    | ########## | 100% \rfreetype-2.9.1       | 882 KB    | ########## | 100% \n",
            "\rqt-5.9.7             | 68.5 MB   |            |   0% \rqt-5.9.7             | 68.5 MB   | #7         |  17% \rqt-5.9.7             | 68.5 MB   | ####       |  41% \rqt-5.9.7             | 68.5 MB   | ######2    |  63% \rqt-5.9.7             | 68.5 MB   | ########   |  80% \rqt-5.9.7             | 68.5 MB   | ########## | 100% \rqt-5.9.7             | 68.5 MB   | ########## | 100% \n",
            "\rgst-plugins-base-1.1 | 6.8 MB    |            |   0% \rgst-plugins-base-1.1 | 6.8 MB    | ########## | 100% \rgst-plugins-base-1.1 | 6.8 MB    | ########## | 100% \n",
            "\rpyqt-5.9.2           | 5.7 MB    |            |   0% \rpyqt-5.9.2           | 5.7 MB    | ########## | 100% \rpyqt-5.9.2           | 5.7 MB    | ########## | 100% \n",
            "\rzipp-2.2.0           | 9 KB      |            |   0% \rzipp-2.2.0           | 9 KB      | ########## | 100% \n",
            "\ripykernel-5.1.4      | 160 KB    |            |   0% \ripykernel-5.1.4      | 160 KB    | ########## | 100% \n",
            "\rcudatoolkit-10.1.243 | 427.9 MB  |            |   0% \rcudatoolkit-10.1.243 | 427.9 MB  | 1          |   2% \rcudatoolkit-10.1.243 | 427.9 MB  | 5          |   5% \rcudatoolkit-10.1.243 | 427.9 MB  | 8          |   9% \rcudatoolkit-10.1.243 | 427.9 MB  | #          |  11% \rcudatoolkit-10.1.243 | 427.9 MB  | #3         |  14% \rcudatoolkit-10.1.243 | 427.9 MB  | #6         |  17% \rcudatoolkit-10.1.243 | 427.9 MB  | #9         |  20% \rcudatoolkit-10.1.243 | 427.9 MB  | ##3        |  23% \rcudatoolkit-10.1.243 | 427.9 MB  | ##6        |  26% \rcudatoolkit-10.1.243 | 427.9 MB  | ##9        |  30% \rcudatoolkit-10.1.243 | 427.9 MB  | ###3       |  33% \rcudatoolkit-10.1.243 | 427.9 MB  | ###6       |  36% \rcudatoolkit-10.1.243 | 427.9 MB  | ####       |  40% \rcudatoolkit-10.1.243 | 427.9 MB  | ####4      |  44% \rcudatoolkit-10.1.243 | 427.9 MB  | ####7      |  48% \rcudatoolkit-10.1.243 | 427.9 MB  | #####      |  51% \rcudatoolkit-10.1.243 | 427.9 MB  | #####4     |  54% \rcudatoolkit-10.1.243 | 427.9 MB  | #####7     |  58% \rcudatoolkit-10.1.243 | 427.9 MB  | ######     |  61% \rcudatoolkit-10.1.243 | 427.9 MB  | ######4    |  64% \rcudatoolkit-10.1.243 | 427.9 MB  | ######7    |  68% \rcudatoolkit-10.1.243 | 427.9 MB  | #######    |  71% \rcudatoolkit-10.1.243 | 427.9 MB  | #######4   |  74% \rcudatoolkit-10.1.243 | 427.9 MB  | #######7   |  77% \rcudatoolkit-10.1.243 | 427.9 MB  | ########   |  80% \rcudatoolkit-10.1.243 | 427.9 MB  | ########3  |  83% \rcudatoolkit-10.1.243 | 427.9 MB  | ########6  |  86% \rcudatoolkit-10.1.243 | 427.9 MB  | ########9  |  89% \rcudatoolkit-10.1.243 | 427.9 MB  | #########1 |  92% \rcudatoolkit-10.1.243 | 427.9 MB  | #########4 |  95% \rcudatoolkit-10.1.243 | 427.9 MB  | #########7 |  98% \rcudatoolkit-10.1.243 | 427.9 MB  | ########## | 100% \n",
            "\rbleach-3.1.0         | 110 KB    |            |   0% \rbleach-3.1.0         | 110 KB    | ########## | 100% \n",
            "\rjoblib-0.14.1        | 198 KB    |            |   0% \rjoblib-0.14.1        | 198 KB    | ########## | 100% \n",
            "\rpython-3.7.7         | 45.0 MB   |            |   0% \rpython-3.7.7         | 45.0 MB   | ##5        |  26% \rpython-3.7.7         | 45.0 MB   | #####6     |  56% \rpython-3.7.7         | 45.0 MB   | ########   |  80% \rpython-3.7.7         | 45.0 MB   | ########## | 100% \rpython-3.7.7         | 45.0 MB   | ########## | 100% \n",
            "\rtk-8.6.8             | 3.1 MB    |            |   0% \rtk-8.6.8             | 3.1 MB    | ########## | 100% \rtk-8.6.8             | 3.1 MB    | ########## | 100% \n",
            "\rptyprocess-0.6.0     | 15 KB     |            |   0% \rptyprocess-0.6.0     | 15 KB     | ########## | 100% \n",
            "\rreadline-8.0         | 281 KB    |            |   0% \rreadline-8.0         | 281 KB    | ########## | 100% \n",
            "\rmkl_random-1.1.0     | 372 KB    |            |   0% \rmkl_random-1.1.0     | 372 KB    | ########## | 100% \rmkl_random-1.1.0     | 372 KB    | ########## | 100% \n",
            "\rnbformat-5.0.4       | 98 KB     |            |   0% \rnbformat-5.0.4       | 98 KB     | ########## | 100% \n",
            "\rpygments-2.6.1       | 683 KB    |            |   0% \rpygments-2.6.1       | 683 KB    | ########## | 100% \rpygments-2.6.1       | 683 KB    | ########## | 100% \n",
            "\rlibxml2-2.9.9        | 1.3 MB    |            |   0% \rlibxml2-2.9.9        | 1.3 MB    | ########## | 100% \rlibxml2-2.9.9        | 1.3 MB    | ########## | 100% \n",
            "\rpy-boost-1.67.0      | 278 KB    |            |   0% \rpy-boost-1.67.0      | 278 KB    | ########## | 100% \rpy-boost-1.67.0      | 278 KB    | ########## | 100% \n",
            "\rpandocfilters-1.4.2  | 9 KB      |            |   0% \rpandocfilters-1.4.2  | 9 KB      | ########## | 100% \n",
            "\rstatsmodels-0.12.1   | 11.0 MB   |            |   0% \rstatsmodels-0.12.1   | 11.0 MB   | ########## | 100% \rstatsmodels-0.12.1   | 11.0 MB   | ########## | 100% \n",
            "\rsip-4.19.8           | 290 KB    |            |   0% \rsip-4.19.8           | 290 KB    | ########## | 100% \n",
            "\rsetuptools-46.1.3    | 634 KB    |            |   0% \rsetuptools-46.1.3    | 634 KB    | ########## | 100% \rsetuptools-46.1.3    | 634 KB    | ########## | 100% \n",
            "\rkiwisolver-1.1.0     | 86 KB     |            |   0% \rkiwisolver-1.1.0     | 86 KB     | ########## | 100% \n",
            "\rimportlib-metadata-1 | 42 KB     |            |   0% \rimportlib-metadata-1 | 42 KB     | ########## | 100% \n",
            "\rdefusedxml-0.6.0     | 22 KB     |            |   0% \rdefusedxml-0.6.0     | 22 KB     | ########## | 100% \n",
            "\rlibsodium-1.0.16     | 306 KB    |            |   0% \rlibsodium-1.0.16     | 306 KB    | ########## | 100% \n",
            "\rdecorator-4.4.2      | 11 KB     |            |   0% \rdecorator-4.4.2      | 11 KB     | ########## | 100% \n",
            "\rpandas-1.0.3         | 11.1 MB   |            |   0% \rpandas-1.0.3         | 11.1 MB   | ######5    |  66% \rpandas-1.0.3         | 11.1 MB   | ########## | 100% \rpandas-1.0.3         | 11.1 MB   | ########## | 100% \n",
            "\ropenssl-1.1.1f       | 2.1 MB    |            |   0% \ropenssl-1.1.1f       | 2.1 MB    | ########## | 100% \ropenssl-1.1.1f       | 2.1 MB    | ########## | 100% \n",
            "\rpatsy-0.5.1          | 187 KB    |            |   0% \rpatsy-0.5.1          | 187 KB    | ########## | 100% \n",
            "\rbzip2-1.0.8          | 484 KB    |            |   0% \rbzip2-1.0.8          | 484 KB    | ########## | 100% \rbzip2-1.0.8          | 484 KB    | ########## | 100% \n",
            "\rlibgcc-ng-9.3.0      | 7.8 MB    |            |   0% \rlibgcc-ng-9.3.0      | 7.8 MB    | ########## | 100% \rlibgcc-ng-9.3.0      | 7.8 MB    | ########## | 100% \n",
            "\rxorg-libxdmcp-1.1.3  | 18 KB     |            |   0% \rxorg-libxdmcp-1.1.3  | 18 KB     | ########## | 100% \n",
            "\rnumpy-1.18.1         | 5.2 MB    |            |   0% \rnumpy-1.18.1         | 5.2 MB    | ########## | 100% \rnumpy-1.18.1         | 5.2 MB    | ########## | 100% \n",
            "\rpython_abi-3.7       | 4 KB      |            |   0% \rpython_abi-3.7       | 4 KB      | ########## | 100% \rpython_abi-3.7       | 4 KB      | ########## | 100% \n",
            "\rmatplotlib-3.1.3     | 21 KB     |            |   0% \rmatplotlib-3.1.3     | 21 KB     | ########## | 100% \rmatplotlib-3.1.3     | 21 KB     | ########## | 100% \n",
            "\rprompt-toolkit-3.0.4 | 233 KB    |            |   0% \rprompt-toolkit-3.0.4 | 233 KB    | ########## | 100% \rprompt-toolkit-3.0.4 | 233 KB    | ########## | 100% \n",
            "\rpcre-8.43            | 257 KB    |            |   0% \rpcre-8.43            | 257 KB    | ########## | 100% \rpcre-8.43            | 257 KB    | ########## | 100% \n",
            "\rzeromq-4.3.1         | 648 KB    |            |   0% \rzeromq-4.3.1         | 648 KB    | ########## | 100% \rzeromq-4.3.1         | 648 KB    | ########## | 100% \n",
            "\rrdkit-2020.03.3.0    | 24.8 MB   |            |   0% \rrdkit-2020.03.3.0    | 24.8 MB   | ########## | 100% \rrdkit-2020.03.3.0    | 24.8 MB   | ########## | 100% \n",
            "\rseaborn-0.11.0       | 4 KB      |            |   0% \rseaborn-0.11.0       | 4 KB      | ########## | 100% \n",
            "\rpyparsing-2.4.6      | 59 KB     |            |   0% \rpyparsing-2.4.6      | 59 KB     | ########## | 100% \n",
            "\rwebencodings-0.5.1   | 12 KB     |            |   0% \rwebencodings-0.5.1   | 12 KB     | ########## | 100% \n",
            "\rlibboost-1.67.0      | 13.0 MB   |            |   0% \rlibboost-1.67.0      | 13.0 MB   | ########6  |  86% \rlibboost-1.67.0      | 13.0 MB   | ########## | 100% \n",
            "\rjedi-0.16.0          | 778 KB    |            |   0% \rjedi-0.16.0          | 778 KB    | ########## | 100% \rjedi-0.16.0          | 778 KB    | ########## | 100% \n",
            "\ripython_genutils-0.2 | 21 KB     |            |   0% \ripython_genutils-0.2 | 21 KB     | ########## | 100% \n",
            "\rxz-5.2.4             | 375 KB    |            |   0% \rxz-5.2.4             | 375 KB    | ########## | 100% \n",
            "\rseaborn-base-0.11.0  | 216 KB    |            |   0% \rseaborn-base-0.11.0  | 216 KB    | ########## | 100% \n",
            "\rsqlite-3.31.1        | 1.1 MB    |            |   0% \rsqlite-3.31.1        | 1.1 MB    | ########## | 100% \rsqlite-3.31.1        | 1.1 MB    | ########## | 100% \n",
            "\rcertifi-2019.11.28   | 149 KB    |            |   0% \rcertifi-2019.11.28   | 149 KB    | ########## | 100% \n",
            "\rcycler-0.10.0        | 9 KB      |            |   0% \rcycler-0.10.0        | 9 KB      | ########## | 100% \n",
            "\rattrs-19.3.0         | 35 KB     |            |   0% \rattrs-19.3.0         | 35 KB     | ########## | 100% \n",
            "\rblas-1.0             | 1 KB      |            |   0% \rblas-1.0             | 1 KB      | ########## | 100% \n",
            "\rpillow-7.0.0         | 598 KB    |            |   0% \rpillow-7.0.0         | 598 KB    | ########## | 100% \rpillow-7.0.0         | 598 KB    | ########## | 100% \n",
            "\rscikit-plot-0.3.7    | 25 KB     |            |   0% \rscikit-plot-0.3.7    | 25 KB     | ########## | 100% \n",
            "\rpycparser-2.20       | 94 KB     |            |   0% \rpycparser-2.20       | 94 KB     | ########## | 100% \n",
            "\ricu-58.2             | 22.6 MB   |            |   0% \ricu-58.2             | 22.6 MB   | #####4     |  54% \ricu-58.2             | 22.6 MB   | ########## | 100% \ricu-58.2             | 22.6 MB   | ########## | 100% \n",
            "\rzstd-1.3.7           | 905 KB    |            |   0% \rzstd-1.3.7           | 905 KB    | ########## | 100% \rzstd-1.3.7           | 905 KB    | ########## | 100% \n",
            "\rgettext-0.19.8.1     | 3.6 MB    |            |   0% \rgettext-0.19.8.1     | 3.6 MB    | ########## | 100% \rgettext-0.19.8.1     | 3.6 MB    | ########## | 100% \n",
            "\rentrypoints-0.3      | 8 KB      |            |   0% \rentrypoints-0.3      | 8 KB      | ########## | 100% \n",
            "\rjinja2-2.11.1        | 94 KB     |            |   0% \rjinja2-2.11.1        | 94 KB     | ########## | 100% \n",
            "\rlibblas-3.8.0        | 11 KB     |            |   0% \rlibblas-3.8.0        | 11 KB     | ########## | 100% \n",
            "\rglib-2.58.3          | 3.3 MB    |            |   0% \rglib-2.58.3          | 3.3 MB    | ########## | 100% \rglib-2.58.3          | 3.3 MB    | ########## | 100% \n",
            "\rmkl_fft-1.0.15       | 169 KB    |            |   0% \rmkl_fft-1.0.15       | 169 KB    | ########## | 100% \n",
            "\rterminado-0.8.3      | 23 KB     |            |   0% \rterminado-0.8.3      | 23 KB     | ########## | 100% \n",
            "\rnbconvert-5.6.1      | 487 KB    |            |   0% \rnbconvert-5.6.1      | 487 KB    | ########## | 100% \rnbconvert-5.6.1      | 487 KB    | ########## | 100% \n",
            "\ripython-7.13.0       | 1.1 MB    |            |   0% \ripython-7.13.0       | 1.1 MB    | ########## | 100% \ripython-7.13.0       | 1.1 MB    | ########## | 100% \n",
            "\r_libgcc_mutex-0.1    | 3 KB      |            |   0% \r_libgcc_mutex-0.1    | 3 KB      | ########## | 100% \n",
            "\rintel-openmp-2019.4  | 729 KB    |            |   0% \rintel-openmp-2019.4  | 729 KB    | ########## | 100% \rintel-openmp-2019.4  | 729 KB    | ########## | 100% \n",
            "\rlibgomp-9.3.0        | 378 KB    |            |   0% \rlibgomp-9.3.0        | 378 KB    | ########## | 100% \n",
            "\rld_impl_linux-64-2.3 | 617 KB    |            |   0% \rld_impl_linux-64-2.3 | 617 KB    | ########## | 100% \rld_impl_linux-64-2.3 | 617 KB    | ########## | 100% \n",
            "\rjupyter_core-4.6.3   | 71 KB     |            |   0% \rjupyter_core-4.6.3   | 71 KB     | ########## | 100% \n",
            "\rbackcall-0.1.0       | 13 KB     |            |   0% \rbackcall-0.1.0       | 13 KB     | ########## | 100% \n",
            "\rmkl-service-2.3.0    | 64 KB     |            |   0% \rmkl-service-2.3.0    | 64 KB     | ########## | 100% \n",
            "\rpyrsistent-0.16.0    | 89 KB     |            |   0% \rpyrsistent-0.16.0    | 89 KB     | ########## | 100% \n",
            "\rpyzmq-18.1.1         | 452 KB    |            |   0% \rpyzmq-18.1.1         | 452 KB    | ########## | 100% \rpyzmq-18.1.1         | 452 KB    | ########## | 100% \n",
            "\rmistune-0.8.4        | 54 KB     |            |   0% \rmistune-0.8.4        | 54 KB     | ########## | 100% \n",
            "\rninja-1.9.0          | 1.6 MB    |            |   0% \rninja-1.9.0          | 1.6 MB    | ########## | 100% \rninja-1.9.0          | 1.6 MB    | ########## | 100% \n",
            "\rmkl-2019.4           | 131.2 MB  |            |   0% \rmkl-2019.4           | 131.2 MB  | 8          |   8% \rmkl-2019.4           | 131.2 MB  | #4         |  15% \rmkl-2019.4           | 131.2 MB  | ##5        |  25% \rmkl-2019.4           | 131.2 MB  | ###5       |  35% \rmkl-2019.4           | 131.2 MB  | ####2      |  42% \rmkl-2019.4           | 131.2 MB  | ####9      |  49% \rmkl-2019.4           | 131.2 MB  | #####8     |  59% \rmkl-2019.4           | 131.2 MB  | ######7    |  67% \rmkl-2019.4           | 131.2 MB  | #######4   |  75% \rmkl-2019.4           | 131.2 MB  | ########4  |  85% \rmkl-2019.4           | 131.2 MB  | #########5 |  96% \rmkl-2019.4           | 131.2 MB  | ########## | 100% \n",
            "\rgstreamer-1.14.5     | 4.5 MB    |            |   0% \rgstreamer-1.14.5     | 4.5 MB    | ########## | 100% \rgstreamer-1.14.5     | 4.5 MB    | ########## | 100% \n",
            "\ropenblas-0.3.10      | 8.3 MB    |            |   0% \ropenblas-0.3.10      | 8.3 MB    | ########## | 100% \ropenblas-0.3.10      | 8.3 MB    | ########## | 100% \n",
            "\rcairo-1.14.12        | 906 KB    |            |   0% \rcairo-1.14.12        | 906 KB    | ########## | 100% \rcairo-1.14.12        | 906 KB    | ########## | 100% \n",
            "\rparso-0.6.2          | 66 KB     |            |   0% \rparso-0.6.2          | 66 KB     | ########## | 100% \n",
            "\rjupyter_client-6.1.2 | 74 KB     |            |   0% \rjupyter_client-6.1.2 | 74 KB     | ########## | 100% \n",
            "\rdbus-1.13.6          | 602 KB    |            |   0% \rdbus-1.13.6          | 602 KB    | ########## | 100% \rdbus-1.13.6          | 602 KB    | ########## | 100% \n",
            "\rpickleshare-0.7.5    | 9 KB      |            |   0% \rpickleshare-0.7.5    | 9 KB      | ########## | 100% \n",
            "\r_openmp_mutex-4.5    | 22 KB     |            |   0% \r_openmp_mutex-4.5    | 22 KB     | ########## | 100% \n",
            "\rpthread-stubs-0.4    | 5 KB      |            |   0% \rpthread-stubs-0.4    | 5 KB      | ########## | 100% \n",
            "\rjsonschema-3.2.0     | 45 KB     |            |   0% \rjsonschema-3.2.0     | 45 KB     | ########## | 100% \n",
            "\rncurses-6.2          | 985 KB    |            |   0% \rncurses-6.2          | 985 KB    | ########## | 100% \rncurses-6.2          | 985 KB    | ########## | 100% \n",
            "\rlibopenblas-0.3.10   | 7.8 MB    |            |   0% \rlibopenblas-0.3.10   | 7.8 MB    | ########## | 100% \rlibopenblas-0.3.10   | 7.8 MB    | ########## | 100% \n",
            "\rgmp-6.2.1            | 806 KB    |            |   0% \rgmp-6.2.1            | 806 KB    | ########## | 100% \rgmp-6.2.1            | 806 KB    | ########## | 100% \n",
            "\rpytorch-1.4.0        | 432.9 MB  |            |   0% \rpytorch-1.4.0        | 432.9 MB  |            |   0% \rpytorch-1.4.0        | 432.9 MB  |            |   0% \rpytorch-1.4.0        | 432.9 MB  |            |   0% \rpytorch-1.4.0        | 432.9 MB  |            |   0% \rpytorch-1.4.0        | 432.9 MB  |            |   1% \rpytorch-1.4.0        | 432.9 MB  | 2          |   2% \rpytorch-1.4.0        | 432.9 MB  | 3          |   3% \rpytorch-1.4.0        | 432.9 MB  | 4          |   4% \rpytorch-1.4.0        | 432.9 MB  | 5          |   6% \rpytorch-1.4.0        | 432.9 MB  | 6          |   7% \rpytorch-1.4.0        | 432.9 MB  | 8          |   8% \rpytorch-1.4.0        | 432.9 MB  | 9          |   9% \rpytorch-1.4.0        | 432.9 MB  | #          |  11% \rpytorch-1.4.0        | 432.9 MB  | #1         |  12% \rpytorch-1.4.0        | 432.9 MB  | #3         |  13% \rpytorch-1.4.0        | 432.9 MB  | #4         |  14% \rpytorch-1.4.0        | 432.9 MB  | #5         |  15% \rpytorch-1.4.0        | 432.9 MB  | #6         |  17% \rpytorch-1.4.0        | 432.9 MB  | #7         |  18% \rpytorch-1.4.0        | 432.9 MB  | #8         |  19% \rpytorch-1.4.0        | 432.9 MB  | ##         |  20% \rpytorch-1.4.0        | 432.9 MB  | ##1        |  21% \rpytorch-1.4.0        | 432.9 MB  | ##2        |  23% \rpytorch-1.4.0        | 432.9 MB  | ##4        |  24% \rpytorch-1.4.0        | 432.9 MB  | ##5        |  26% \rpytorch-1.4.0        | 432.9 MB  | ##6        |  27% \rpytorch-1.4.0        | 432.9 MB  | ##8        |  28% \rpytorch-1.4.0        | 432.9 MB  | ##9        |  30% \rpytorch-1.4.0        | 432.9 MB  | ###        |  31% \rpytorch-1.4.0        | 432.9 MB  | ###2       |  32% \rpytorch-1.4.0        | 432.9 MB  | ###3       |  33% \rpytorch-1.4.0        | 432.9 MB  | ###4       |  34% \rpytorch-1.4.0        | 432.9 MB  | ###4       |  35% \rpytorch-1.4.0        | 432.9 MB  | ###6       |  36% \rpytorch-1.4.0        | 432.9 MB  | ###7       |  38% \rpytorch-1.4.0        | 432.9 MB  | ###8       |  39% \rpytorch-1.4.0        | 432.9 MB  | ####       |  40% \rpytorch-1.4.0        | 432.9 MB  | ####1      |  42% \rpytorch-1.4.0        | 432.9 MB  | ####2      |  43% \rpytorch-1.4.0        | 432.9 MB  | ####3      |  44% \rpytorch-1.4.0        | 432.9 MB  | ####4      |  45% \rpytorch-1.4.0        | 432.9 MB  | ####6      |  46% \rpytorch-1.4.0        | 432.9 MB  | ####7      |  47% \rpytorch-1.4.0        | 432.9 MB  | ####8      |  48% \rpytorch-1.4.0        | 432.9 MB  | ####9      |  49% \rpytorch-1.4.0        | 432.9 MB  | #####      |  50% \rpytorch-1.4.0        | 432.9 MB  | #####1     |  51% \rpytorch-1.4.0        | 432.9 MB  | #####2     |  53% \rpytorch-1.4.0        | 432.9 MB  | #####3     |  54% \rpytorch-1.4.0        | 432.9 MB  | #####4     |  55% \rpytorch-1.4.0        | 432.9 MB  | #####5     |  56% \rpytorch-1.4.0        | 432.9 MB  | #####6     |  57% \rpytorch-1.4.0        | 432.9 MB  | #####7     |  58% \rpytorch-1.4.0        | 432.9 MB  | #####8     |  59% \rpytorch-1.4.0        | 432.9 MB  | ######     |  60% \rpytorch-1.4.0        | 432.9 MB  | ######1    |  61% \rpytorch-1.4.0        | 432.9 MB  | ######2    |  62% \rpytorch-1.4.0        | 432.9 MB  | ######3    |  64% \rpytorch-1.4.0        | 432.9 MB  | ######4    |  65% \rpytorch-1.4.0        | 432.9 MB  | ######5    |  66% \rpytorch-1.4.0        | 432.9 MB  | ######6    |  67% \rpytorch-1.4.0        | 432.9 MB  | ######7    |  68% \rpytorch-1.4.0        | 432.9 MB  | ######9    |  69% \rpytorch-1.4.0        | 432.9 MB  | #######    |  70% \rpytorch-1.4.0        | 432.9 MB  | #######1   |  71% \rpytorch-1.4.0        | 432.9 MB  | #######2   |  72% \rpytorch-1.4.0        | 432.9 MB  | #######3   |  73% \rpytorch-1.4.0        | 432.9 MB  | #######4   |  74% \rpytorch-1.4.0        | 432.9 MB  | #######5   |  75% \rpytorch-1.4.0        | 432.9 MB  | #######6   |  77% \rpytorch-1.4.0        | 432.9 MB  | #######7   |  78% \rpytorch-1.4.0        | 432.9 MB  | #######8   |  79% \rpytorch-1.4.0        | 432.9 MB  | #######9   |  80% \rpytorch-1.4.0        | 432.9 MB  | ########1  |  81% \rpytorch-1.4.0        | 432.9 MB  | ########2  |  82% \rpytorch-1.4.0        | 432.9 MB  | ########3  |  83% \rpytorch-1.4.0        | 432.9 MB  | ########4  |  84% \rpytorch-1.4.0        | 432.9 MB  | ########5  |  85% \rpytorch-1.4.0        | 432.9 MB  | ########6  |  87% \rpytorch-1.4.0        | 432.9 MB  | ########7  |  88% \rpytorch-1.4.0        | 432.9 MB  | ########8  |  89% \rpytorch-1.4.0        | 432.9 MB  | ########9  |  90% \rpytorch-1.4.0        | 432.9 MB  | #########1 |  91% \rpytorch-1.4.0        | 432.9 MB  | #########2 |  92% \rpytorch-1.4.0        | 432.9 MB  | #########3 |  93% \rpytorch-1.4.0        | 432.9 MB  | #########4 |  94% \rpytorch-1.4.0        | 432.9 MB  | #########5 |  96% \rpytorch-1.4.0        | 432.9 MB  | #########6 |  97% \rpytorch-1.4.0        | 432.9 MB  | #########7 |  98% \rpytorch-1.4.0        | 432.9 MB  | #########8 |  99% \rpytorch-1.4.0        | 432.9 MB  | ########## | 100% \n",
            "\rsend2trash-1.5.0     | 12 KB     |            |   0% \rsend2trash-1.5.0     | 12 KB     | ########## | 100% \n",
            "\rjpeg-9b              | 214 KB    |            |   0% \rjpeg-9b              | 214 KB    | ########## | 100% \n",
            "\rscikit-learn-0.22.1  | 7.1 MB    |            |   0% \rscikit-learn-0.22.1  | 7.1 MB    | #######9   |  80% \rscikit-learn-0.22.1  | 7.1 MB    | ########## | 100% \n",
            "\rtorchvision-0.5.0    | 9.1 MB    |            |   0% \rtorchvision-0.5.0    | 9.1 MB    |            |   0% \rtorchvision-0.5.0    | 9.1 MB    |            |   1% \rtorchvision-0.5.0    | 9.1 MB    | 1          |   2% \rtorchvision-0.5.0    | 9.1 MB    | 7          |   7% \rtorchvision-0.5.0    | 9.1 MB    | ##8        |  29% \rtorchvision-0.5.0    | 9.1 MB    | ########2  |  83% \rtorchvision-0.5.0    | 9.1 MB    | ########## | 100% \rtorchvision-0.5.0    | 9.1 MB    | ########## | 100% \n",
            "\rtornado-6.0.4        | 641 KB    |            |   0% \rtornado-6.0.4        | 641 KB    | ########## | 100% \rtornado-6.0.4        | 641 KB    | ########## | 100% \n",
            "\rzlib-1.2.11          | 106 KB    |            |   0% \rzlib-1.2.11          | 106 KB    | ########## | 100% \n",
            "\rlibtiff-4.1.0        | 447 KB    |            |   0% \rlibtiff-4.1.0        | 447 KB    | ########## | 100% \n",
            "\rlibffi-3.2.1         | 47 KB     |            |   0% \rlibffi-3.2.1         | 47 KB     | ########## | 100% \n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
            "\n",
            "done\n",
            "Installing pip dependencies: ...working... Ran pip subprocess with arguments:\n",
            "['/usr/local/envs/DeepPurpose/bin/python', '-m', 'pip', 'install', '-U', '-r', '/content/DeepPurpose/condaenv.xidjas2a.requirements.txt']\n",
            "Pip subprocess output:\n",
            "Collecting alabaster==0.7.12\n",
            "  Downloading alabaster-0.7.12-py2.py3-none-any.whl (14 kB)\n",
            "Collecting autograd==1.3\n",
            "  Downloading autograd-1.3.tar.gz (38 kB)\n",
            "Collecting autograd-gamma==0.4.2\n",
            "  Downloading autograd_gamma-0.4.2-py2.py3-none-any.whl (3.8 kB)\n",
            "Collecting babel==2.8.0\n",
            "  Downloading Babel-2.8.0-py2.py3-none-any.whl (8.6 MB)\n",
            "Collecting chardet==3.0.4\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "Collecting docutils==0.16\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "Collecting future==0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "Collecting idna==2.9\n",
            "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
            "Collecting imagesize==1.2.0\n",
            "  Downloading imagesize-1.2.0-py2.py3-none-any.whl (4.8 kB)\n",
            "Collecting lifelines==0.24.3\n",
            "  Downloading lifelines-0.24.3-py2.py3-none-any.whl (417 kB)\n",
            "Collecting packaging==20.3\n",
            "  Downloading packaging-20.3-py2.py3-none-any.whl (37 kB)\n",
            "Collecting prettytable==0.7.2\n",
            "  Downloading prettytable-0.7.2.tar.bz2 (21 kB)\n",
            "Collecting requests==2.23.0\n",
            "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
            "Collecting snowballstemmer==2.0.0\n",
            "  Downloading snowballstemmer-2.0.0-py2.py3-none-any.whl (97 kB)\n",
            "Collecting subword-nmt==0.3.7\n",
            "  Downloading subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\n",
            "Collecting urllib3==1.25.8\n",
            "  Downloading urllib3-1.25.8-py2.py3-none-any.whl (125 kB)\n",
            "Collecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/envs/DeepPurpose/lib/python3.7/site-packages (from autograd==1.3->-r /content/DeepPurpose/condaenv.xidjas2a.requirements.txt (line 2)) (1.18.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=1.2.0 in /usr/local/envs/DeepPurpose/lib/python3.7/site-packages (from autograd-gamma==0.4.2->-r /content/DeepPurpose/condaenv.xidjas2a.requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2015.7 in /usr/local/envs/DeepPurpose/lib/python3.7/site-packages (from babel==2.8.0->-r /content/DeepPurpose/condaenv.xidjas2a.requirements.txt (line 4)) (2019.3)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=3.0 in /usr/local/envs/DeepPurpose/lib/python3.7/site-packages (from lifelines==0.24.3->-r /content/DeepPurpose/condaenv.xidjas2a.requirements.txt (line 10)) (3.1.3)\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.23.0 in /usr/local/envs/DeepPurpose/lib/python3.7/site-packages (from lifelines==0.24.3->-r /content/DeepPurpose/condaenv.xidjas2a.requirements.txt (line 10)) (1.0.3)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/envs/DeepPurpose/lib/python3.7/site-packages (from packaging==20.3->-r /content/DeepPurpose/condaenv.xidjas2a.requirements.txt (line 11)) (1.14.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/envs/DeepPurpose/lib/python3.7/site-packages (from packaging==20.3->-r /content/DeepPurpose/condaenv.xidjas2a.requirements.txt (line 11)) (2.4.6)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/envs/DeepPurpose/lib/python3.7/site-packages (from requests==2.23.0->-r /content/DeepPurpose/condaenv.xidjas2a.requirements.txt (line 13)) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/envs/DeepPurpose/lib/python3.7/site-packages (from matplotlib>=3.0->lifelines==0.24.3->-r /content/DeepPurpose/condaenv.xidjas2a.requirements.txt (line 10)) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/envs/DeepPurpose/lib/python3.7/site-packages (from matplotlib>=3.0->lifelines==0.24.3->-r /content/DeepPurpose/condaenv.xidjas2a.requirements.txt (line 10)) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/envs/DeepPurpose/lib/python3.7/site-packages (from matplotlib>=3.0->lifelines==0.24.3->-r /content/DeepPurpose/condaenv.xidjas2a.requirements.txt (line 10)) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/envs/DeepPurpose/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=3.0->lifelines==0.24.3->-r /content/DeepPurpose/condaenv.xidjas2a.requirements.txt (line 10)) (46.1.3.post20200325)\n",
            "Building wheels for collected packages: autograd, future, prettytable, wget\n",
            "  Building wheel for autograd (setup.py): started\n",
            "  Building wheel for autograd (setup.py): finished with status 'done'\n",
            "  Created wheel for autograd: filename=autograd-1.3-py3-none-any.whl size=47990 sha256=8113f5da75f14435bb1300d425426d620cae77d6a07f0c1eebe397466363a5bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/32/31/0e87227cd0ca1d99ad51fbe4b54c6fa02afccf7e483d045e04\n",
            "  Building wheel for future (setup.py): started\n",
            "  Building wheel for future (setup.py): finished with status 'done'\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=55f65c168c36ce9eb9d1dd08ee76ea8d9734e4a04195ec24083ca0d8c7face52\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for prettytable (setup.py): started\n",
            "  Building wheel for prettytable (setup.py): finished with status 'done'\n",
            "  Created wheel for prettytable: filename=prettytable-0.7.2-py3-none-any.whl size=13698 sha256=ea0e69d2ebd5d3dd22471e288d40a132b689427540b079bcbcdedba356606492\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/76/0b/eb9eb3da7e2335e3577e3f96a0ae9f74f206e26457bd1a2bc8\n",
            "  Building wheel for wget (setup.py): started\n",
            "  Building wheel for wget (setup.py): finished with status 'done'\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9681 sha256=73003b6a6dbc1bbb232ce59e6d7cb5a34d9b130e97eb0a06c9d78193e947d976\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built autograd future prettytable wget\n",
            "Installing collected packages: alabaster, future, autograd, autograd-gamma, babel, chardet, docutils, idna, imagesize, lifelines, packaging, prettytable, urllib3, requests, snowballstemmer, subword-nmt, wget\n",
            "Successfully installed alabaster-0.7.12 autograd-1.3 autograd-gamma-0.4.2 babel-2.8.0 chardet-3.0.4 docutils-0.16 future-0.18.2 idna-2.9 imagesize-1.2.0 lifelines-0.24.3 packaging-20.3 prettytable-0.7.2 requests-2.23.0 snowballstemmer-2.0.0 subword-nmt-0.3.7 urllib3-1.25.8 wget-3.2\n",
            "\n",
            "done\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate DeepPurpose\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtjmssATIfzF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a80bafd-f74b-4c9a-f4d9-fa6078497437"
      },
      "source": [
        "!pip install git+https://github.com/bp-kelley/descriptastorus pandas-flavor==0.1.0 subword-nmt lifelines wget "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/bp-kelley/descriptastorus\n",
            "  Cloning https://github.com/bp-kelley/descriptastorus to /tmp/pip-req-build-xcd97mc_\n",
            "Collecting lifelines\n",
            "  Downloading lifelines-0.25.7-py3-none-any.whl (347 kB)\n",
            "\u001b[K     |████████████████████████████████| 347 kB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/site-packages (from lifelines) (1.19.4)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/site-packages (from lifelines) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.6/site-packages (from lifelines) (1.5.3)\n",
            "Collecting pandas-flavor==0.1.0\n",
            "  Downloading pandas_flavor-0.1.0-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/site-packages (from lifelines) (1.0.1)\n",
            "Collecting subword-nmt\n",
            "  Using cached subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\n",
            "Collecting wget\n",
            "  Using cached wget-3.2.zip (10 kB)\n",
            "Collecting autograd>=1.3\n",
            "  Using cached autograd-1.3.tar.gz (38 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/site-packages (from lifelines) (1.19.4)\n",
            "Collecting autograd-gamma>=0.3\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.6/site-packages (from lifelines) (1.5.3)\n",
            "Collecting future>=0.15.2\n",
            "  Using cached future-0.18.2.tar.gz (829 kB)\n",
            "Collecting matplotlib>=3.0\n",
            "  Downloading matplotlib-3.3.3-cp36-cp36m-manylinux1_x86_64.whl (11.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.6 MB 9.5 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/site-packages (from matplotlib>=3.0->lifelines) (8.0.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/site-packages (from lifelines) (1.19.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from matplotlib>=3.0->lifelines) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/site-packages (from matplotlib>=3.0->lifelines) (2.4.7)\n",
            "Collecting cycler>=0.10\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from cycler>=0.10->matplotlib>=3.0->lifelines) (1.15.0)\n",
            "Collecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 71.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from matplotlib>=3.0->lifelines) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas>=0.23.0->lifelines) (2020.4)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/site-packages (from lifelines) (1.19.4)\n",
            "Collecting patsy>=0.5.0\n",
            "  Downloading patsy-0.5.1-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 59.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from cycler>=0.10->matplotlib>=3.0->lifelines) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/site-packages (from lifelines) (1.19.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from cycler>=0.10->matplotlib>=3.0->lifelines) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/site-packages (from lifelines) (1.19.4)\n",
            "Building wheels for collected packages: descriptastorus, wget, autograd, autograd-gamma, future\n",
            "  Building wheel for descriptastorus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for descriptastorus: filename=descriptastorus-2.2.0.4-py3-none-any.whl size=60155 sha256=cc3983ffffb2f76424f739482c2f19f64f120cf4222078679ca4c733d119a35d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3p2gxyex/wheels/d4/c0/32/bb1ad6053d5c73df1da492e5d60860d5676890ddf38f44b5e5\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9680 sha256=69c10cb3c189353fa68eda7d41c45d561aa82f02af72345c634ee99b73885e5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/1d/93/c863ee832230df5cfc25ca497b3e88e0ee3ea9e44adc46ac62\n",
            "  Building wheel for autograd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd: filename=autograd-1.3-py3-none-any.whl size=47988 sha256=4812b270b7d69b08b32dd6d0c75b0890222bd4e8720d4c17cf4e2cad8cf05922\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/a5/3c/929b91003a3b75a175ccba7cef35200a1890cdb46903f18072\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4035 sha256=789194bb6eb81b4b3732f4814e1233a4d7d5e7c60216c58c78bd9c94b32fda6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/91/38/cc1c88a96296277582afe7217ddd50fd184f788dfa29b4cb88\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=55175cfc9c8877f2d90f440bb22e78d84396b877cf27a8a99213c05e4d0b2450\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/9c/ed/4499c9865ac1002697793e0ae05ba6be33553d098f3347fb94\n",
            "Successfully built descriptastorus wget autograd autograd-gamma future\n",
            "Installing collected packages: future, kiwisolver, cycler, autograd, patsy, matplotlib, autograd-gamma, wget, subword-nmt, pandas-flavor, lifelines, descriptastorus\n",
            "Successfully installed autograd-1.3 autograd-gamma-0.5.0 cycler-0.10.0 descriptastorus-2.2.0.4 future-0.18.2 kiwisolver-1.3.1 lifelines-0.25.7 matplotlib-3.3.3 pandas-flavor-0.1.0 patsy-0.5.1 subword-nmt-0.3.7 wget-3.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cycler",
                  "kiwisolver"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMIpsVVCOFP_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce5d0f67-b144-4f7d-a0d0-78a6ed0126b3"
      },
      "source": [
        "!conda install -c rdkit rdkit -y"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - rdkit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    bzip2-1.0.8                |       h7b6447c_0          78 KB\n",
            "    ca-certificates-2020.12.8  |       h06a4308_0         121 KB\n",
            "    certifi-2020.12.5          |   py36h06a4308_0         140 KB\n",
            "    fontconfig-2.13.0          |       h9420a91_0         227 KB\n",
            "    glib-2.66.1                |       h92f7085_0         2.9 MB\n",
            "    icu-58.2                   |       he6710b0_3        10.5 MB\n",
            "    libboost-1.73.0            |      h37e3b65_11        13.9 MB\n",
            "    libuuid-1.0.3              |       h1bed415_2          15 KB\n",
            "    libxcb-1.14                |       h7b6447c_0         505 KB\n",
            "    libxml2-2.9.10             |       hb55368b_3         1.2 MB\n",
            "    openssl-1.1.1i             |       h27cfd23_0         2.5 MB\n",
            "    pcre-8.44                  |       he6710b0_0         212 KB\n",
            "    pixman-0.40.0              |       h7b6447c_0         370 KB\n",
            "    py-boost-1.73.0            |  py36h962f231_11         204 KB\n",
            "    rdkit-2020.09.1.0          |   py36hd50e099_1        26.0 MB  rdkit\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        58.9 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h7b6447c_0\n",
            "  cairo              pkgs/main/linux-64::cairo-1.14.12-h8948797_3\n",
            "  fontconfig         pkgs/main/linux-64::fontconfig-2.13.0-h9420a91_0\n",
            "  glib               pkgs/main/linux-64::glib-2.66.1-h92f7085_0\n",
            "  icu                pkgs/main/linux-64::icu-58.2-he6710b0_3\n",
            "  libboost           pkgs/main/linux-64::libboost-1.73.0-h37e3b65_11\n",
            "  libuuid            pkgs/main/linux-64::libuuid-1.0.3-h1bed415_2\n",
            "  libxcb             pkgs/main/linux-64::libxcb-1.14-h7b6447c_0\n",
            "  libxml2            pkgs/main/linux-64::libxml2-2.9.10-hb55368b_3\n",
            "  pcre               pkgs/main/linux-64::pcre-8.44-he6710b0_0\n",
            "  pixman             pkgs/main/linux-64::pixman-0.40.0-h7b6447c_0\n",
            "  py-boost           pkgs/main/linux-64::py-boost-1.73.0-py36h962f231_11\n",
            "  rdkit              rdkit/linux-64::rdkit-2020.09.1.0-py36hd50e099_1\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2020.12.~ --> pkgs/main::ca-certificates-2020.12.8-h06a4308_0\n",
            "  openssl            conda-forge::openssl-1.1.1h-h516909a_0 --> pkgs/main::openssl-1.1.1i-h27cfd23_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            conda-forge::certifi-2020.12.5-py36h5~ --> pkgs/main::certifi-2020.12.5-py36h06a4308_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "icu-58.2             | 10.5 MB   | : 100% 1.0/1 [00:00<00:00,  2.24it/s]\n",
            "libuuid-1.0.3        | 15 KB     | : 100% 1.0/1 [00:00<00:00, 14.47it/s]\n",
            "py-boost-1.73.0      | 204 KB    | : 100% 1.0/1 [00:00<00:00, 17.73it/s]\n",
            "pcre-8.44            | 212 KB    | : 100% 1.0/1 [00:00<00:00, 18.47it/s]\n",
            "ca-certificates-2020 | 121 KB    | : 100% 1.0/1 [00:00<00:00, 19.13it/s]\n",
            "openssl-1.1.1i       | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  7.38it/s]\n",
            "pixman-0.40.0        | 370 KB    | : 100% 1.0/1 [00:00<00:00, 17.79it/s]\n",
            "libxml2-2.9.10       | 1.2 MB    | : 100% 1.0/1 [00:00<00:00, 10.79it/s]\n",
            "certifi-2020.12.5    | 140 KB    | : 100% 1.0/1 [00:00<00:00, 16.00it/s]\n",
            "libxcb-1.14          | 505 KB    | : 100% 1.0/1 [00:00<00:00, 15.83it/s]\n",
            "bzip2-1.0.8          | 78 KB     | : 100% 1.0/1 [00:00<00:00, 18.89it/s]\n",
            "rdkit-2020.09.1.0    | 26.0 MB   | : 100% 1.0/1 [00:03<00:00,  3.93s/it]\n",
            "libboost-1.73.0      | 13.9 MB   | : 100% 1.0/1 [00:01<00:00,  1.78s/it]\n",
            "glib-2.66.1          | 2.9 MB    | : 100% 1.0/1 [00:00<00:00,  5.58it/s]\n",
            "fontconfig-2.13.0    | 227 KB    | : 100% 1.0/1 [00:00<00:00, 13.47it/s]\n",
            "Preparing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMiJm8S-Ih0y"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/DeepPurpose/')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-GWZln1IlR3"
      },
      "source": [
        "**Sanity Check:** If thes two modules below load without error, you aare good to go! *(Warnings are okay)*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT_sGWHYIkSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a46a2b3-2555-4d25-b6a6-7073b231d6e5"
      },
      "source": [
        "import DeepPurpose.utils\n",
        "import DeepPurpose.dataset\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:No normalization for BCUT2D_MWHI\n",
            "WARNING:root:No normalization for BCUT2D_MWLOW\n",
            "WARNING:root:No normalization for BCUT2D_CHGHI\n",
            "WARNING:root:No normalization for BCUT2D_CHGLO\n",
            "WARNING:root:No normalization for BCUT2D_LOGPHI\n",
            "WARNING:root:No normalization for BCUT2D_LOGPLOW\n",
            "WARNING:root:No normalization for BCUT2D_MRHI\n",
            "WARNING:root:No normalization for BCUT2D_MRLOW\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VihRe7UJ_6ik"
      },
      "source": [
        "**Mount Google Drive**: Skip if running locally.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQg_5ohS_4KO",
        "outputId": "b5ed8ed2-9997-4165-8cde-f75d17630a1c"
      },
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IF_f7Y8ANZu"
      },
      "source": [
        "**Set up paths**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kpg643v_4vN"
      },
      "source": [
        "inspire_folder = \"/content/drive/MyDrive/Colab Notebooks/DTI/helpers/\" # This folder should contain helper files inspire_encoder.py, DTI_inspire.py, and utils_inspire.py\n",
        "results_folder = \"/content/drive/MyDrive/Colab Notebooks/DTI/results/\"\n",
        "data_folder = \"/content/drive/MyDrive/Colab Notebooks/DTI/data/\"\n",
        "models_folder = \"/content/drive/MyDrive/Colab Notebooks/DTI/models/\""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Oyd349-F3yn"
      },
      "source": [
        "**Add helper folder**\n",
        "NOTE - Do NOT do this if you are not using colab. Instead, copy helper scripts into a folder where they can be imported , and they can access DeepPurpose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMjT3b6-Dcbm"
      },
      "source": [
        "_ = (sys.path.append(inspire_folder))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiEJRGG6Csq_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agFRbcmPPVdO"
      },
      "source": [
        "# Implementation of: InspireDTI\n",
        "\n",
        "`TODO:` Fill out this notebook with details, explanations, and the results of training, evaluation, and refinement of your final model. This notebook does not have a prescribed structure; you are free to organize it in the way that makes the most sense to you while implementing and generating your final model. The \"That's all folks!\" marks the end of your notebook. \n",
        "\n",
        "`TODO:` You must submit this notebook in `.ipynb` format as well as a serialized version of your final model in a Pickle format `.pkl`. See the assignment instructions for all of the A3 details. Please also delete these instructions in your submitted notebook to keep it focused on your work! ;)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtaw4eGsPVdP"
      },
      "source": [
        "# [Delete me] A few imports to get you started ;)\n",
        "import os\n",
        "\n",
        "from DeepPurpose.dataset import *\n",
        "import DTI_inspire as models # It is important that this is imported AFTER DeepPurpose\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from utils_inspire import * # It is important that this is imported AFTER DeepPurpose\n",
        "\n",
        "import pickle as pickle_rick\n",
        "# [Delete me] All of your work follows from here..."
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlhoPNaDBYvf"
      },
      "source": [
        "**Pytorch version check**: Pytorch version should be 1.4.0, torchvision 0.5.0, cuda enabled"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-mQyAbFGaiP",
        "outputId": "094bf59b-1f67-4963-94cd-091833edc986"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "print(f\"torch:{torch.__version__} torchvision:{torchvision.__version__} cuda:{torch.cuda.is_available()}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch:1.4.0 torchvision:0.5.0 cuda:True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8aCskX3HNCD"
      },
      "source": [
        "# Load Data\n",
        "We are loading the BindingDB and DAVIS datasets and concatenating them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJm6vNoeHjrn",
        "outputId": "94f47e41-4686-4916-8650-0469ca9d8743"
      },
      "source": [
        "X_drug, X_target, y  = load_process_DAVIS(path = data_folder, binary = False, convert_to_log = True, threshold = 30)\n",
        "\n",
        "if not os.path.isfile(data_folder + 'BindingDB_All.tsv'):\n",
        "    bdb_path = download_BindingDB(path = data_folder)\n",
        "else:\n",
        "    bdb_path = data_folder + 'BindingDB_All.tsv'\n",
        "X_drug2, X_target2, y2  = process_BindingDB(path = bdb_path, df = None, y = 'Kd', binary = False, convert_to_log = True, threshold = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning Processing...\n",
            "Beginning to extract zip file...\n",
            "Default set to logspace (nM -> p) for easier regression\n",
            "Done!\n",
            "Loading Dataset from path...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 772572: expected 193 fields, saw 205\\nSkipping line 772598: expected 193 fields, saw 205\\n'\n",
            "b'Skipping line 805291: expected 193 fields, saw 205\\n'\n",
            "b'Skipping line 827961: expected 193 fields, saw 265\\n'\n",
            "b'Skipping line 1231688: expected 193 fields, saw 241\\n'\n",
            "b'Skipping line 1345591: expected 193 fields, saw 241\\nSkipping line 1345592: expected 193 fields, saw 241\\nSkipping line 1345593: expected 193 fields, saw 241\\nSkipping line 1345594: expected 193 fields, saw 241\\nSkipping line 1345595: expected 193 fields, saw 241\\nSkipping line 1345596: expected 193 fields, saw 241\\nSkipping line 1345597: expected 193 fields, saw 241\\nSkipping line 1345598: expected 193 fields, saw 241\\nSkipping line 1345599: expected 193 fields, saw 241\\n'\n",
            "b'Skipping line 1358864: expected 193 fields, saw 205\\n'\n",
            "b'Skipping line 1378087: expected 193 fields, saw 241\\nSkipping line 1378088: expected 193 fields, saw 241\\nSkipping line 1378089: expected 193 fields, saw 241\\nSkipping line 1378090: expected 193 fields, saw 241\\nSkipping line 1378091: expected 193 fields, saw 241\\nSkipping line 1378092: expected 193 fields, saw 241\\nSkipping line 1378093: expected 193 fields, saw 241\\nSkipping line 1378094: expected 193 fields, saw 241\\nSkipping line 1378095: expected 193 fields, saw 241\\n'\n",
            "b'Skipping line 1417264: expected 193 fields, saw 205\\n'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Beginning Processing...\n",
            "There are 66444 drug target pairs.\n",
            "Default set to logspace (nM -> p) for easier regression\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFmcRYdKHkEr"
      },
      "source": [
        "X_drug = np.concatenate((X_drug, X_drug2), axis=0)\n",
        "X_target = np.concatenate((X_target, X_target2), axis=0)\n",
        "y = np.concatenate((y, y2), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeWLhC-TI8CD"
      },
      "source": [
        "As duplicates may occur between the two sets, we need to drop them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NwnKbnyLFyB"
      },
      "source": [
        "upper = np.vectorize(lambda x: x.upper())\n",
        "X_drug = upper(X_drug)\n",
        "X_target = upper(X_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "UgjwDK3jHkPE",
        "outputId": "fb335cc2-331d-47fd-c555-7762ee48bd86"
      },
      "source": [
        "all_data = pd.DataFrame(data=np.transpose(np.stack([X_drug, X_target, y])))\n",
        "all_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
              "      <td>MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQV...</td>\n",
              "      <td>7.3655227298392685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
              "      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n",
              "      <td>4.999995657076895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
              "      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n",
              "      <td>4.999995657076895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
              "      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n",
              "      <td>4.999995657076895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
              "      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n",
              "      <td>4.999995657076895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96495</th>\n",
              "      <td>CC(C)(C)C[C@@H]1N[C@H]([C@H](C2CCCC(CL)C2F)[C@...</td>\n",
              "      <td>MCNTNMSVPTDGAVTTSQIPASEQETLVRPKPLLLKLLKSVGAQKD...</td>\n",
              "      <td>8.55284196865778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96496</th>\n",
              "      <td>CC[C@@H]([C@H](C)O)N1[C@@H]([C@H](C[C@](C)(CC(...</td>\n",
              "      <td>MCNTNMSVPTDGAVTTSQIPASEQETLVRPKPLLLKLLKSVGAQKD...</td>\n",
              "      <td>9.301029995663981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96497</th>\n",
              "      <td>CC(C)[C@@H](CS(=O)(=O)C(C)C)N1[C@@H]([C@H](C[C...</td>\n",
              "      <td>MCNTNMSVPTDGAVTTSQIPASEQETLVRPKPLLLKLLKSVGAQKD...</td>\n",
              "      <td>9.838631997765026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96498</th>\n",
              "      <td>CCCCCCCOC1OC(CO)C(O)C(O)C1O</td>\n",
              "      <td>MKRVITLFAVLLMGWSVNAWSFACKTANGTAIPIGGGSANVYVNLA...</td>\n",
              "      <td>7.767003889607846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96499</th>\n",
              "      <td>CCCCCCCOC1OC(CO)C(O)C(O)C1O</td>\n",
              "      <td>MKRVITLFAVLLMGWSVNAWSFACKTANGTAIPIGGGSANVYVNLA...</td>\n",
              "      <td>7.718966632752273</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>96500 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                       0  ...                   2\n",
              "0      CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...  ...  7.3655227298392685\n",
              "1      CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...  ...   4.999995657076895\n",
              "2      CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...  ...   4.999995657076895\n",
              "3      CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...  ...   4.999995657076895\n",
              "4      CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...  ...   4.999995657076895\n",
              "...                                                  ...  ...                 ...\n",
              "96495  CC(C)(C)C[C@@H]1N[C@H]([C@H](C2CCCC(CL)C2F)[C@...  ...    8.55284196865778\n",
              "96496  CC[C@@H]([C@H](C)O)N1[C@@H]([C@H](C[C@](C)(CC(...  ...   9.301029995663981\n",
              "96497  CC(C)[C@@H](CS(=O)(=O)C(C)C)N1[C@@H]([C@H](C[C...  ...   9.838631997765026\n",
              "96498                        CCCCCCCOC1OC(CO)C(O)C(O)C1O  ...   7.767003889607846\n",
              "96499                        CCCCCCCOC1OC(CO)C(O)C(O)C1O  ...   7.718966632752273\n",
              "\n",
              "[96500 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "vlb8_RLuJ1yO",
        "outputId": "add2f6c3-6e69-43d8-b035-556fa59df91d"
      },
      "source": [
        "# Keep DAVIS version if duplicates exist\n",
        "all_data = all_data.drop_duplicates(subset=[0, 1], keep='first')\n",
        "all_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
              "      <td>MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQV...</td>\n",
              "      <td>7.3655227298392685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
              "      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n",
              "      <td>4.999995657076895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
              "      <td>MVLGTVLLPPNSYGRDQDTSLCCLCTEASESALPDLTDHFASCVED...</td>\n",
              "      <td>4.999995657076895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
              "      <td>MVDGVMILPVLIMIALPSPSMEDEKPKVNPKLYMCVCEGLSCGNED...</td>\n",
              "      <td>4.999995657076895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
              "      <td>MAESAGASSFFPLVVLLLAGSGGSGPRGVQALLCACTSCLQANYTC...</td>\n",
              "      <td>4.999995657076895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96492</th>\n",
              "      <td>O=C(NCCCCS(=O)(=O)C1CCCCC1)C1CCC2NCCN2C1</td>\n",
              "      <td>MNPAAEAEFNILLATDSYKVTHYKQYPPNTSKVYSYFECREKKTEN...</td>\n",
              "      <td>6.065451052333853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96493</th>\n",
              "      <td>CCOC1CC(CCC1C1=N[C@@](C)(C2CCC(CL)CC2)[C@](C)(...</td>\n",
              "      <td>MCNTNMSVPTDGAVTTSQIPASEQETLVRPKPLLLKLLKSVGAQKD...</td>\n",
              "      <td>8.522878745280337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96494</th>\n",
              "      <td>COC1CC(CCC1NC(=O)[C@@H]1N[C@@H](CC(C)(C)C)[C@@...</td>\n",
              "      <td>MCNTNMSVPTDGAVTTSQIPASEQETLVRPKPLLLKLLKSVGAQKD...</td>\n",
              "      <td>9.602059991327963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96495</th>\n",
              "      <td>CC(C)(C)C[C@@H]1N[C@H]([C@H](C2CCCC(CL)C2F)[C@...</td>\n",
              "      <td>MCNTNMSVPTDGAVTTSQIPASEQETLVRPKPLLLKLLKSVGAQKD...</td>\n",
              "      <td>8.55284196865778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96497</th>\n",
              "      <td>CC(C)[C@@H](CS(=O)(=O)C(C)C)N1[C@@H]([C@H](C[C...</td>\n",
              "      <td>MCNTNMSVPTDGAVTTSQIPASEQETLVRPKPLLLKLLKSVGAQKD...</td>\n",
              "      <td>9.838631997765026</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>71845 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                       0  ...                   2\n",
              "0      CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...  ...  7.3655227298392685\n",
              "1      CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...  ...   4.999995657076895\n",
              "16     CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...  ...   4.999995657076895\n",
              "17     CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...  ...   4.999995657076895\n",
              "18     CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...  ...   4.999995657076895\n",
              "...                                                  ...  ...                 ...\n",
              "96492           O=C(NCCCCS(=O)(=O)C1CCCCC1)C1CCC2NCCN2C1  ...   6.065451052333853\n",
              "96493  CCOC1CC(CCC1C1=N[C@@](C)(C2CCC(CL)CC2)[C@](C)(...  ...   8.522878745280337\n",
              "96494  COC1CC(CCC1NC(=O)[C@@H]1N[C@@H](CC(C)(C)C)[C@@...  ...   9.602059991327963\n",
              "96495  CC(C)(C)C[C@@H]1N[C@H]([C@H](C2CCCC(CL)C2F)[C@...  ...    8.55284196865778\n",
              "96497  CC(C)[C@@H](CS(=O)(=O)C(C)C)N1[C@@H]([C@H](C[C...  ...   9.838631997765026\n",
              "\n",
              "[71845 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEoXm7HJL2HA"
      },
      "source": [
        "## Split Data\n",
        "We are using the transformer encoding for drugs, and a custom encoding based on DeepConvDTI's encoding for the target.\n",
        "\n",
        "\"Cold Target\" is chosen to allow generalizing between proteins in our dataset, and query SARS-Cov-2 proteins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RGmRQx9MKCY",
        "outputId": "c8fb0ffa-c9d5-448c-aa11-44d57ed5cc31"
      },
      "source": [
        "drug_encoding, target_encoding = 'Transformer', 'CNN_inspire'\n",
        "#ensure none of test smiles appears in set\n",
        "train, val, test  = data_process(all_data[0], all_data[1], y, \n",
        "                      drug_encoding, target_encoding, \n",
        "                      split_method='cold_protein',frac=[0.8,0.1,0.1],\n",
        "                      random_seed = 42)\n",
        "# train.head()\n",
        "print(f\"First drug representation: \\n{train.drug_encoding.iloc[0]}\")\n",
        "print(f\"First target representation: \\n{train.target_encoding.iloc[0]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drug Target Interaction Prediction Mode...\n",
            "in total: 71845 drug-target pairs\n",
            "encoding drug...\n",
            "unique drugs: 10685\n",
            "encoding protein...\n",
            "unique target sequence: 1449\n",
            "splitting dataset...\n",
            "Done.\n",
            "First drug representation: \n",
            "(array([ 800,  122,  248,  282,  623,  272, 1256, 2210,   91,   85,  109,\n",
            "        119,   80,    8,  282,  861,  209,   19,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0]))\n",
            "First target representation: \n",
            "[11 18 18 ...  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhBbXFXhNkt-"
      },
      "source": [
        "## Save Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_URh1GsRNiNi"
      },
      "source": [
        "train.to_pickle(data_folder + 'G35_data_train.pkl')\n",
        "val.to_pickle(data_folder + 'G35_data_vali.pkl')\n",
        "test.to_pickle(data_folder + 'G35_data_test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y11IYKeCORV_"
      },
      "source": [
        "# Benchmark DeepDTA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6PeCSkfOW5W",
        "outputId": "43587630-46a3-4789-bf19-af1e9dcc5684"
      },
      "source": [
        "drug_encoding, target_encoding = 'CNN', 'CNN'\n",
        "train, val, test  = data_process(all_data[0], all_data[1], y, \n",
        "                      drug_encoding, target_encoding, \n",
        "                      split_method='cold_protein',frac=[0.8,0.1,0.1],\n",
        "                      random_seed = 42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drug Target Interaction Prediction Mode...\n",
            "in total: 71845 drug-target pairs\n",
            "encoding drug...\n",
            "unique drugs: 10685\n",
            "encoding protein...\n",
            "unique target sequence: 1449\n",
            "splitting dataset...\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "our1oFL_OkGa",
        "outputId": "d18a514f-c368-4226-a915-bc0f3c556d36"
      },
      "source": [
        "config = generate_config(drug_encoding = drug_encoding, \n",
        "                         target_encoding = target_encoding,\n",
        "                         result_folder = results_folder + 'DeepDTA', \n",
        "                         cls_hidden_dims = [1024,1024,512], \n",
        "                         train_epoch = 100, \n",
        "                         LR = 0.001, \n",
        "                         batch_size = 256,\n",
        "                         cnn_drug_filters = [32,64,96],\n",
        "                         cnn_target_filters = [32,64,96],\n",
        "                         cnn_drug_kernels = [4,6,8],\n",
        "                         cnn_target_kernels = [4,8,12]\n",
        "                        )\n",
        "\n",
        "model = models.model_initialize(**config)\n",
        "model.train(train, val, test)\n",
        "model.save_model(models_folder + 'model_DeepDTA')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's use 1 GPU!\n",
            "--- Data Preparation ---\n",
            "--- Go for Training ---\n",
            "Training at Epoch 1 iteration 0 with loss 34.8850. Total time 0.00027 hours\n",
            "Training at Epoch 1 iteration 100 with loss 1.94207. Total time 0.01722 hours\n",
            "Training at Epoch 1 iteration 200 with loss 1.38965. Total time 0.03444 hours\n",
            "Validation at Epoch 1 , MSE: 1.41203 , Pearson Correlation: 0.25100 with p-value: 4.70733 , Concordance Index: 0.60293\n",
            "Training at Epoch 2 iteration 0 with loss 1.36667. Total time 0.04333 hours\n",
            "Training at Epoch 2 iteration 100 with loss 1.21046. Total time 0.06027 hours\n",
            "Training at Epoch 2 iteration 200 with loss 1.19423. Total time 0.0775 hours\n",
            "Validation at Epoch 2 , MSE: 1.55709 , Pearson Correlation: 0.29269 with p-value: 3.23787 , Concordance Index: 0.62362\n",
            "Training at Epoch 3 iteration 0 with loss 1.57818. Total time 0.08583 hours\n",
            "Training at Epoch 3 iteration 100 with loss 1.44674. Total time 0.10277 hours\n",
            "Training at Epoch 3 iteration 200 with loss 1.21424. Total time 0.12 hours\n",
            "Validation at Epoch 3 , MSE: 1.31890 , Pearson Correlation: 0.29093 with p-value: 2.78747 , Concordance Index: 0.62551\n",
            "Training at Epoch 4 iteration 0 with loss 1.27430. Total time 0.12833 hours\n",
            "Training at Epoch 4 iteration 100 with loss 1.21261. Total time 0.14527 hours\n",
            "Training at Epoch 4 iteration 200 with loss 1.38038. Total time 0.16222 hours\n",
            "Validation at Epoch 4 , MSE: 1.32467 , Pearson Correlation: 0.28786 with p-value: 6.07523 , Concordance Index: 0.62331\n",
            "Training at Epoch 5 iteration 0 with loss 1.32030. Total time 0.17055 hours\n",
            "Training at Epoch 5 iteration 100 with loss 1.40931. Total time 0.1875 hours\n",
            "Training at Epoch 5 iteration 200 with loss 1.34319. Total time 0.20416 hours\n",
            "Validation at Epoch 5 , MSE: 1.42795 , Pearson Correlation: 0.29361 with p-value: 3.14161 , Concordance Index: 0.62421\n",
            "Training at Epoch 6 iteration 0 with loss 1.23533. Total time 0.21277 hours\n",
            "Training at Epoch 6 iteration 100 with loss 1.49818. Total time 0.22944 hours\n",
            "Training at Epoch 6 iteration 200 with loss 1.25680. Total time 0.24611 hours\n",
            "Validation at Epoch 6 , MSE: 1.32530 , Pearson Correlation: 0.29721 with p-value: 3.02726 , Concordance Index: 0.62640\n",
            "Training at Epoch 7 iteration 0 with loss 1.40575. Total time 0.25444 hours\n",
            "Training at Epoch 7 iteration 100 with loss 1.57302. Total time 0.27138 hours\n",
            "Training at Epoch 7 iteration 200 with loss 1.09168. Total time 0.28805 hours\n",
            "Validation at Epoch 7 , MSE: 1.32369 , Pearson Correlation: 0.30037 with p-value: 8.14430 , Concordance Index: 0.62769\n",
            "Training at Epoch 8 iteration 0 with loss 1.28606. Total time 0.29638 hours\n",
            "Training at Epoch 8 iteration 100 with loss 1.29275. Total time 0.31305 hours\n",
            "Training at Epoch 8 iteration 200 with loss 1.27299. Total time 0.33027 hours\n",
            "Validation at Epoch 8 , MSE: 1.32676 , Pearson Correlation: 0.29480 with p-value: 1.50505 , Concordance Index: 0.62695\n",
            "Training at Epoch 9 iteration 0 with loss 1.35040. Total time 0.33888 hours\n",
            "Training at Epoch 9 iteration 100 with loss 1.13163. Total time 0.35583 hours\n",
            "Training at Epoch 9 iteration 200 with loss 1.41997. Total time 0.37277 hours\n",
            "Validation at Epoch 9 , MSE: 1.59572 , Pearson Correlation: 0.29046 with p-value: 9.02880 , Concordance Index: 0.62773\n",
            "Training at Epoch 10 iteration 0 with loss 1.45893. Total time 0.38138 hours\n",
            "Training at Epoch 10 iteration 100 with loss 1.25841. Total time 0.39805 hours\n",
            "Training at Epoch 10 iteration 200 with loss 0.92942. Total time 0.415 hours\n",
            "Validation at Epoch 10 , MSE: 1.40371 , Pearson Correlation: 0.29504 with p-value: 8.18753 , Concordance Index: 0.62631\n",
            "Training at Epoch 11 iteration 0 with loss 1.36809. Total time 0.42333 hours\n",
            "Training at Epoch 11 iteration 100 with loss 1.25008. Total time 0.44027 hours\n",
            "Training at Epoch 11 iteration 200 with loss 1.00566. Total time 0.45722 hours\n",
            "Validation at Epoch 11 , MSE: 1.32528 , Pearson Correlation: 0.29709 with p-value: 4.11677 , Concordance Index: 0.62949\n",
            "Training at Epoch 12 iteration 0 with loss 1.05099. Total time 0.46583 hours\n",
            "Training at Epoch 12 iteration 100 with loss 1.63867. Total time 0.48305 hours\n",
            "Training at Epoch 12 iteration 200 with loss 1.34866. Total time 0.5 hours\n",
            "Validation at Epoch 12 , MSE: 1.36918 , Pearson Correlation: 0.29288 with p-value: 2.00795 , Concordance Index: 0.62594\n",
            "Training at Epoch 13 iteration 0 with loss 0.97556. Total time 0.50861 hours\n",
            "Training at Epoch 13 iteration 100 with loss 1.08455. Total time 0.52583 hours\n",
            "Training at Epoch 13 iteration 200 with loss 1.43732. Total time 0.54305 hours\n",
            "Validation at Epoch 13 , MSE: 1.33376 , Pearson Correlation: 0.29707 with p-value: 4.41338 , Concordance Index: 0.62725\n",
            "Training at Epoch 14 iteration 0 with loss 1.13002. Total time 0.55166 hours\n",
            "Training at Epoch 14 iteration 100 with loss 1.28148. Total time 0.56888 hours\n",
            "Training at Epoch 14 iteration 200 with loss 1.30481. Total time 0.58611 hours\n",
            "Validation at Epoch 14 , MSE: 1.33523 , Pearson Correlation: 0.28979 with p-value: 4.91318 , Concordance Index: 0.62563\n",
            "Training at Epoch 15 iteration 0 with loss 1.02521. Total time 0.59472 hours\n",
            "Training at Epoch 15 iteration 100 with loss 1.20605. Total time 0.61166 hours\n",
            "Training at Epoch 15 iteration 200 with loss 1.49073. Total time 0.62861 hours\n",
            "Validation at Epoch 15 , MSE: 1.32172 , Pearson Correlation: 0.29685 with p-value: 7.65785 , Concordance Index: 0.62742\n",
            "Training at Epoch 16 iteration 0 with loss 1.23810. Total time 0.63722 hours\n",
            "Training at Epoch 16 iteration 100 with loss 0.98914. Total time 0.65444 hours\n",
            "Training at Epoch 16 iteration 200 with loss 1.39389. Total time 0.67166 hours\n",
            "Validation at Epoch 16 , MSE: 1.31696 , Pearson Correlation: 0.29839 with p-value: 1.44736 , Concordance Index: 0.63001\n",
            "Training at Epoch 17 iteration 0 with loss 1.07032. Total time 0.68027 hours\n",
            "Training at Epoch 17 iteration 100 with loss 1.28303. Total time 0.6975 hours\n",
            "Training at Epoch 17 iteration 200 with loss 0.93102. Total time 0.71472 hours\n",
            "Validation at Epoch 17 , MSE: 1.34185 , Pearson Correlation: 0.29200 with p-value: 1.87553 , Concordance Index: 0.62495\n",
            "Training at Epoch 18 iteration 0 with loss 1.26111. Total time 0.72361 hours\n",
            "Training at Epoch 18 iteration 100 with loss 1.40321. Total time 0.74083 hours\n",
            "Training at Epoch 18 iteration 200 with loss 1.02695. Total time 0.75805 hours\n",
            "Validation at Epoch 18 , MSE: 1.39157 , Pearson Correlation: 0.28597 with p-value: 6.53460 , Concordance Index: 0.62315\n",
            "Training at Epoch 19 iteration 0 with loss 1.05789. Total time 0.76666 hours\n",
            "Training at Epoch 19 iteration 100 with loss 1.31436. Total time 0.78388 hours\n",
            "Training at Epoch 19 iteration 200 with loss 1.17961. Total time 0.80138 hours\n",
            "Validation at Epoch 19 , MSE: 1.31942 , Pearson Correlation: 0.29989 with p-value: 2.90748 , Concordance Index: 0.63051\n",
            "Training at Epoch 20 iteration 0 with loss 1.05076. Total time 0.81 hours\n",
            "Training at Epoch 20 iteration 100 with loss 1.09932. Total time 0.8275 hours\n",
            "Training at Epoch 20 iteration 200 with loss 1.20313. Total time 0.84472 hours\n",
            "Validation at Epoch 20 , MSE: 1.33110 , Pearson Correlation: 0.29326 with p-value: 7.64220 , Concordance Index: 0.62540\n",
            "Training at Epoch 21 iteration 0 with loss 1.34609. Total time 0.85333 hours\n",
            "Training at Epoch 21 iteration 100 with loss 1.12390. Total time 0.87055 hours\n",
            "Training at Epoch 21 iteration 200 with loss 1.07094. Total time 0.88777 hours\n",
            "Validation at Epoch 21 , MSE: 1.34471 , Pearson Correlation: 0.25898 with p-value: 1.49872 , Concordance Index: 0.61522\n",
            "Training at Epoch 22 iteration 0 with loss 1.11355. Total time 0.89638 hours\n",
            "Training at Epoch 22 iteration 100 with loss 1.24212. Total time 0.91361 hours\n",
            "Training at Epoch 22 iteration 200 with loss 1.13376. Total time 0.93083 hours\n",
            "Validation at Epoch 22 , MSE: 1.31206 , Pearson Correlation: 0.30590 with p-value: 3.64125 , Concordance Index: 0.63123\n",
            "Training at Epoch 23 iteration 0 with loss 1.27521. Total time 0.93944 hours\n",
            "Training at Epoch 23 iteration 100 with loss 1.03130. Total time 0.95694 hours\n",
            "Training at Epoch 23 iteration 200 with loss 1.45556. Total time 0.97388 hours\n",
            "Validation at Epoch 23 , MSE: 1.31724 , Pearson Correlation: 0.29556 with p-value: 2.15912 , Concordance Index: 0.63080\n",
            "Training at Epoch 24 iteration 0 with loss 1.19495. Total time 0.98277 hours\n",
            "Training at Epoch 24 iteration 100 with loss 1.08108. Total time 1.0 hours\n",
            "Training at Epoch 24 iteration 200 with loss 0.95928. Total time 1.01722 hours\n",
            "Validation at Epoch 24 , MSE: 1.32106 , Pearson Correlation: 0.30562 with p-value: 7.64679 , Concordance Index: 0.63442\n",
            "Training at Epoch 25 iteration 0 with loss 1.25433. Total time 1.02583 hours\n",
            "Training at Epoch 25 iteration 100 with loss 1.17248. Total time 1.04305 hours\n",
            "Training at Epoch 25 iteration 200 with loss 0.92654. Total time 1.06027 hours\n",
            "Validation at Epoch 25 , MSE: 1.33751 , Pearson Correlation: 0.29829 with p-value: 1.84912 , Concordance Index: 0.63010\n",
            "Training at Epoch 26 iteration 0 with loss 0.93523. Total time 1.06888 hours\n",
            "Training at Epoch 26 iteration 100 with loss 1.31478. Total time 1.08611 hours\n",
            "Training at Epoch 26 iteration 200 with loss 1.22842. Total time 1.10333 hours\n",
            "Validation at Epoch 26 , MSE: 1.33676 , Pearson Correlation: 0.29106 with p-value: 2.02422 , Concordance Index: 0.62831\n",
            "Training at Epoch 27 iteration 0 with loss 1.04393. Total time 1.11194 hours\n",
            "Training at Epoch 27 iteration 100 with loss 1.45118. Total time 1.12916 hours\n",
            "Training at Epoch 27 iteration 200 with loss 1.42975. Total time 1.14638 hours\n",
            "Validation at Epoch 27 , MSE: 1.35262 , Pearson Correlation: 0.30263 with p-value: 2.18473 , Concordance Index: 0.63198\n",
            "Training at Epoch 28 iteration 0 with loss 1.28531. Total time 1.155 hours\n",
            "Training at Epoch 28 iteration 100 with loss 1.21165. Total time 1.17222 hours\n",
            "Training at Epoch 28 iteration 200 with loss 1.50164. Total time 1.18944 hours\n",
            "Validation at Epoch 28 , MSE: 1.33752 , Pearson Correlation: 0.29531 with p-value: 4.08999 , Concordance Index: 0.62896\n",
            "Training at Epoch 29 iteration 0 with loss 0.97034. Total time 1.19805 hours\n",
            "Training at Epoch 29 iteration 100 with loss 1.03364. Total time 1.21527 hours\n",
            "Training at Epoch 29 iteration 200 with loss 1.43687. Total time 1.23277 hours\n",
            "Validation at Epoch 29 , MSE: 1.34466 , Pearson Correlation: 0.29725 with p-value: 2.72098 , Concordance Index: 0.63113\n",
            "Training at Epoch 30 iteration 0 with loss 0.95709. Total time 1.24138 hours\n",
            "Training at Epoch 30 iteration 100 with loss 1.15222. Total time 1.25861 hours\n",
            "Training at Epoch 30 iteration 200 with loss 1.43043. Total time 1.27583 hours\n",
            "Validation at Epoch 30 , MSE: 1.34029 , Pearson Correlation: 0.28083 with p-value: 1.87883 , Concordance Index: 0.62694\n",
            "Training at Epoch 31 iteration 0 with loss 1.22199. Total time 1.28444 hours\n",
            "Training at Epoch 31 iteration 100 with loss 1.00264. Total time 1.30166 hours\n",
            "Training at Epoch 31 iteration 200 with loss 1.32648. Total time 1.31888 hours\n",
            "Validation at Epoch 31 , MSE: 1.33778 , Pearson Correlation: 0.28615 with p-value: 4.25376 , Concordance Index: 0.62818\n",
            "Training at Epoch 32 iteration 0 with loss 1.06953. Total time 1.32777 hours\n",
            "Training at Epoch 32 iteration 100 with loss 1.05260. Total time 1.345 hours\n",
            "Training at Epoch 32 iteration 200 with loss 1.16996. Total time 1.36222 hours\n",
            "Validation at Epoch 32 , MSE: 1.43068 , Pearson Correlation: 0.26755 with p-value: 6.75416 , Concordance Index: 0.62194\n",
            "Training at Epoch 33 iteration 0 with loss 1.17044. Total time 1.37083 hours\n",
            "Training at Epoch 33 iteration 100 with loss 1.19903. Total time 1.38777 hours\n",
            "Training at Epoch 33 iteration 200 with loss 1.04211. Total time 1.405 hours\n",
            "Validation at Epoch 33 , MSE: 1.37048 , Pearson Correlation: 0.27510 with p-value: 1.63810 , Concordance Index: 0.62415\n",
            "Training at Epoch 34 iteration 0 with loss 1.12004. Total time 1.41361 hours\n",
            "Training at Epoch 34 iteration 100 with loss 0.85553. Total time 1.43083 hours\n",
            "Training at Epoch 34 iteration 200 with loss 0.90706. Total time 1.44805 hours\n",
            "Validation at Epoch 34 , MSE: 1.35575 , Pearson Correlation: 0.26054 with p-value: 4.86955 , Concordance Index: 0.61936\n",
            "Training at Epoch 35 iteration 0 with loss 0.99786. Total time 1.45666 hours\n",
            "Training at Epoch 35 iteration 100 with loss 0.92532. Total time 1.47388 hours\n",
            "Training at Epoch 35 iteration 200 with loss 1.19723. Total time 1.49083 hours\n",
            "Validation at Epoch 35 , MSE: 1.35727 , Pearson Correlation: 0.27681 with p-value: 2.86053 , Concordance Index: 0.62522\n",
            "Training at Epoch 36 iteration 0 with loss 1.17298. Total time 1.49972 hours\n",
            "Training at Epoch 36 iteration 100 with loss 1.33467. Total time 1.51666 hours\n",
            "Training at Epoch 36 iteration 200 with loss 0.98799. Total time 1.53388 hours\n",
            "Validation at Epoch 36 , MSE: 1.37857 , Pearson Correlation: 0.26674 with p-value: 4.25343 , Concordance Index: 0.62344\n",
            "Training at Epoch 37 iteration 0 with loss 0.86800. Total time 1.5425 hours\n",
            "Training at Epoch 37 iteration 100 with loss 1.02183. Total time 1.55972 hours\n",
            "Training at Epoch 37 iteration 200 with loss 1.11763. Total time 1.57694 hours\n",
            "Validation at Epoch 37 , MSE: 1.39079 , Pearson Correlation: 0.27314 with p-value: 1.65755 , Concordance Index: 0.62549\n",
            "Training at Epoch 38 iteration 0 with loss 0.89053. Total time 1.58555 hours\n",
            "Training at Epoch 38 iteration 100 with loss 0.98146. Total time 1.60277 hours\n",
            "Training at Epoch 38 iteration 200 with loss 1.03521. Total time 1.62 hours\n",
            "Validation at Epoch 38 , MSE: 1.42657 , Pearson Correlation: 0.23562 with p-value: 2.41887 , Concordance Index: 0.61174\n",
            "Training at Epoch 39 iteration 0 with loss 1.20533. Total time 1.62861 hours\n",
            "Training at Epoch 39 iteration 100 with loss 0.80680. Total time 1.64555 hours\n",
            "Training at Epoch 39 iteration 200 with loss 0.97844. Total time 1.66277 hours\n",
            "Validation at Epoch 39 , MSE: 1.40810 , Pearson Correlation: 0.27057 with p-value: 6.54394 , Concordance Index: 0.62487\n",
            "Training at Epoch 40 iteration 0 with loss 1.05295. Total time 1.67138 hours\n",
            "Training at Epoch 40 iteration 100 with loss 0.94204. Total time 1.68861 hours\n",
            "Training at Epoch 40 iteration 200 with loss 0.84819. Total time 1.70583 hours\n",
            "Validation at Epoch 40 , MSE: 1.40191 , Pearson Correlation: 0.26753 with p-value: 7.00628 , Concordance Index: 0.62884\n",
            "Training at Epoch 41 iteration 0 with loss 1.23193. Total time 1.71444 hours\n",
            "Training at Epoch 41 iteration 100 with loss 1.27051. Total time 1.73166 hours\n",
            "Training at Epoch 41 iteration 200 with loss 0.90975. Total time 1.74888 hours\n",
            "Validation at Epoch 41 , MSE: 1.42075 , Pearson Correlation: 0.26325 with p-value: 1.14119 , Concordance Index: 0.62757\n",
            "Training at Epoch 42 iteration 0 with loss 0.75092. Total time 1.7575 hours\n",
            "Training at Epoch 42 iteration 100 with loss 1.20157. Total time 1.77472 hours\n",
            "Training at Epoch 42 iteration 200 with loss 0.89459. Total time 1.79222 hours\n",
            "Validation at Epoch 42 , MSE: 1.41715 , Pearson Correlation: 0.25088 with p-value: 6.07684 , Concordance Index: 0.62123\n",
            "Training at Epoch 43 iteration 0 with loss 1.01228. Total time 1.80083 hours\n",
            "Training at Epoch 43 iteration 100 with loss 0.90999. Total time 1.81777 hours\n",
            "Training at Epoch 43 iteration 200 with loss 0.92960. Total time 1.835 hours\n",
            "Validation at Epoch 43 , MSE: 1.48800 , Pearson Correlation: 0.21625 with p-value: 1.89501 , Concordance Index: 0.61325\n",
            "Training at Epoch 44 iteration 0 with loss 0.72699. Total time 1.84361 hours\n",
            "Training at Epoch 44 iteration 100 with loss 1.21999. Total time 1.86083 hours\n",
            "Training at Epoch 44 iteration 200 with loss 0.95894. Total time 1.87777 hours\n",
            "Validation at Epoch 44 , MSE: 1.43332 , Pearson Correlation: 0.25786 with p-value: 1.76022 , Concordance Index: 0.62730\n",
            "Training at Epoch 45 iteration 0 with loss 1.03576. Total time 1.88638 hours\n",
            "Training at Epoch 45 iteration 100 with loss 0.96345. Total time 1.90388 hours\n",
            "Training at Epoch 45 iteration 200 with loss 0.75988. Total time 1.92083 hours\n",
            "Validation at Epoch 45 , MSE: 1.48993 , Pearson Correlation: 0.23449 with p-value: 2.22558 , Concordance Index: 0.61746\n",
            "Training at Epoch 46 iteration 0 with loss 1.08371. Total time 1.92972 hours\n",
            "Training at Epoch 46 iteration 100 with loss 0.72614. Total time 1.94666 hours\n",
            "Training at Epoch 46 iteration 200 with loss 0.85649. Total time 1.96388 hours\n",
            "Validation at Epoch 46 , MSE: 1.55315 , Pearson Correlation: 0.23844 with p-value: 8.66684 , Concordance Index: 0.61380\n",
            "Training at Epoch 47 iteration 0 with loss 0.96297. Total time 1.9725 hours\n",
            "Training at Epoch 47 iteration 100 with loss 1.16714. Total time 1.98972 hours\n",
            "Training at Epoch 47 iteration 200 with loss 0.85076. Total time 2.00694 hours\n",
            "Validation at Epoch 47 , MSE: 1.47335 , Pearson Correlation: 0.23672 with p-value: 2.70473 , Concordance Index: 0.62172\n",
            "Training at Epoch 48 iteration 0 with loss 0.62776. Total time 2.01583 hours\n",
            "Training at Epoch 48 iteration 100 with loss 0.83226. Total time 2.03305 hours\n",
            "Training at Epoch 48 iteration 200 with loss 0.81366. Total time 2.05027 hours\n",
            "Validation at Epoch 48 , MSE: 1.48650 , Pearson Correlation: 0.23376 with p-value: 9.47912 , Concordance Index: 0.61641\n",
            "Training at Epoch 49 iteration 0 with loss 0.88198. Total time 2.05888 hours\n",
            "Training at Epoch 49 iteration 100 with loss 0.90242. Total time 2.07611 hours\n",
            "Training at Epoch 49 iteration 200 with loss 0.99377. Total time 2.09333 hours\n",
            "Validation at Epoch 49 , MSE: 1.46207 , Pearson Correlation: 0.23766 with p-value: 4.18018 , Concordance Index: 0.61954\n",
            "Training at Epoch 50 iteration 0 with loss 0.88767. Total time 2.10194 hours\n",
            "Training at Epoch 50 iteration 100 with loss 0.77549. Total time 2.11916 hours\n",
            "Training at Epoch 50 iteration 200 with loss 0.96692. Total time 2.13638 hours\n",
            "Validation at Epoch 50 , MSE: 1.44867 , Pearson Correlation: 0.22719 with p-value: 3.06736 , Concordance Index: 0.61263\n",
            "Training at Epoch 51 iteration 0 with loss 1.09301. Total time 2.145 hours\n",
            "Training at Epoch 51 iteration 100 with loss 1.13230. Total time 2.16222 hours\n",
            "Training at Epoch 51 iteration 200 with loss 0.86602. Total time 2.17916 hours\n",
            "Validation at Epoch 51 , MSE: 1.48405 , Pearson Correlation: 0.22738 with p-value: 2.13481 , Concordance Index: 0.62157\n",
            "Training at Epoch 52 iteration 0 with loss 0.95045. Total time 2.18777 hours\n",
            "Training at Epoch 52 iteration 100 with loss 0.77575. Total time 2.205 hours\n",
            "Training at Epoch 52 iteration 200 with loss 0.83354. Total time 2.22222 hours\n",
            "Validation at Epoch 52 , MSE: 1.44321 , Pearson Correlation: 0.25266 with p-value: 1.37588 , Concordance Index: 0.62316\n",
            "Training at Epoch 53 iteration 0 with loss 0.79373. Total time 2.23083 hours\n",
            "Training at Epoch 53 iteration 100 with loss 0.84879. Total time 2.24805 hours\n",
            "Training at Epoch 53 iteration 200 with loss 0.78709. Total time 2.26527 hours\n",
            "Validation at Epoch 53 , MSE: 1.48839 , Pearson Correlation: 0.22985 with p-value: 1.88218 , Concordance Index: 0.61709\n",
            "Training at Epoch 54 iteration 0 with loss 0.70789. Total time 2.27388 hours\n",
            "Training at Epoch 54 iteration 100 with loss 0.71538. Total time 2.29111 hours\n",
            "Training at Epoch 54 iteration 200 with loss 0.90420. Total time 2.30805 hours\n",
            "Validation at Epoch 54 , MSE: 1.53389 , Pearson Correlation: 0.20930 with p-value: 4.16368 , Concordance Index: 0.60851\n",
            "Training at Epoch 55 iteration 0 with loss 0.71308. Total time 2.31666 hours\n",
            "Training at Epoch 55 iteration 100 with loss 0.96927. Total time 2.33388 hours\n",
            "Training at Epoch 55 iteration 200 with loss 0.87331. Total time 2.35083 hours\n",
            "Validation at Epoch 55 , MSE: 1.49871 , Pearson Correlation: 0.22928 with p-value: 5.63741 , Concordance Index: 0.61781\n",
            "Training at Epoch 56 iteration 0 with loss 0.76195. Total time 2.35944 hours\n",
            "Training at Epoch 56 iteration 100 with loss 0.80976. Total time 2.37666 hours\n",
            "Training at Epoch 56 iteration 200 with loss 0.97585. Total time 2.39361 hours\n",
            "Validation at Epoch 56 , MSE: 1.61221 , Pearson Correlation: 0.20815 with p-value: 3.02107 , Concordance Index: 0.60586\n",
            "Training at Epoch 57 iteration 0 with loss 0.69899. Total time 2.40222 hours\n",
            "Training at Epoch 57 iteration 100 with loss 0.85501. Total time 2.41944 hours\n",
            "Training at Epoch 57 iteration 200 with loss 0.93589. Total time 2.43666 hours\n",
            "Validation at Epoch 57 , MSE: 1.57375 , Pearson Correlation: 0.19330 with p-value: 1.57233 , Concordance Index: 0.60698\n",
            "Training at Epoch 58 iteration 0 with loss 0.77840. Total time 2.44527 hours\n",
            "Training at Epoch 58 iteration 100 with loss 0.96974. Total time 2.4625 hours\n",
            "Training at Epoch 58 iteration 200 with loss 0.80861. Total time 2.47972 hours\n",
            "Validation at Epoch 58 , MSE: 1.61267 , Pearson Correlation: 0.20630 with p-value: 7.32815 , Concordance Index: 0.60550\n",
            "Training at Epoch 59 iteration 0 with loss 0.81326. Total time 2.48833 hours\n",
            "Training at Epoch 59 iteration 100 with loss 0.77037. Total time 2.50555 hours\n",
            "Training at Epoch 59 iteration 200 with loss 0.85034. Total time 2.5225 hours\n",
            "Validation at Epoch 59 , MSE: 1.53913 , Pearson Correlation: 0.22319 with p-value: 5.71999 , Concordance Index: 0.61672\n",
            "Training at Epoch 60 iteration 0 with loss 0.84921. Total time 2.53111 hours\n",
            "Training at Epoch 60 iteration 100 with loss 0.88410. Total time 2.54833 hours\n",
            "Training at Epoch 60 iteration 200 with loss 0.61036. Total time 2.56555 hours\n",
            "Validation at Epoch 60 , MSE: 1.51235 , Pearson Correlation: 0.21053 with p-value: 4.88983 , Concordance Index: 0.60403\n",
            "Training at Epoch 61 iteration 0 with loss 0.66053. Total time 2.57416 hours\n",
            "Training at Epoch 61 iteration 100 with loss 0.64008. Total time 2.59138 hours\n",
            "Training at Epoch 61 iteration 200 with loss 0.79164. Total time 2.60861 hours\n",
            "Validation at Epoch 61 , MSE: 1.52064 , Pearson Correlation: 0.20214 with p-value: 8.38925 , Concordance Index: 0.60556\n",
            "Training at Epoch 62 iteration 0 with loss 0.64460. Total time 2.61722 hours\n",
            "Training at Epoch 62 iteration 100 with loss 0.93873. Total time 2.63416 hours\n",
            "Training at Epoch 62 iteration 200 with loss 0.83429. Total time 2.65138 hours\n",
            "Validation at Epoch 62 , MSE: 1.52660 , Pearson Correlation: 0.20018 with p-value: 2.16642 , Concordance Index: 0.60364\n",
            "Training at Epoch 63 iteration 0 with loss 0.83663. Total time 2.66 hours\n",
            "Training at Epoch 63 iteration 100 with loss 0.56308. Total time 2.67722 hours\n",
            "Training at Epoch 63 iteration 200 with loss 0.76813. Total time 2.69444 hours\n",
            "Validation at Epoch 63 , MSE: 1.53195 , Pearson Correlation: 0.18374 with p-value: 4.39501 , Concordance Index: 0.59977\n",
            "Training at Epoch 64 iteration 0 with loss 0.76145. Total time 2.70305 hours\n",
            "Training at Epoch 64 iteration 100 with loss 0.70008. Total time 2.72027 hours\n",
            "Training at Epoch 64 iteration 200 with loss 0.90955. Total time 2.7375 hours\n",
            "Validation at Epoch 64 , MSE: 1.58324 , Pearson Correlation: 0.20243 with p-value: 5.09508 , Concordance Index: 0.60545\n",
            "Training at Epoch 65 iteration 0 with loss 0.68071. Total time 2.74611 hours\n",
            "Training at Epoch 65 iteration 100 with loss 0.70826. Total time 2.76333 hours\n",
            "Training at Epoch 65 iteration 200 with loss 0.73206. Total time 2.78055 hours\n",
            "Validation at Epoch 65 , MSE: 1.56709 , Pearson Correlation: 0.18900 with p-value: 1.37322 , Concordance Index: 0.59472\n",
            "Training at Epoch 66 iteration 0 with loss 0.77692. Total time 2.78916 hours\n",
            "Training at Epoch 66 iteration 100 with loss 0.73590. Total time 2.80638 hours\n",
            "Training at Epoch 66 iteration 200 with loss 0.81161. Total time 2.82361 hours\n",
            "Validation at Epoch 66 , MSE: 1.56798 , Pearson Correlation: 0.19373 with p-value: 7.93177 , Concordance Index: 0.60062\n",
            "Training at Epoch 67 iteration 0 with loss 0.80724. Total time 2.83222 hours\n",
            "Training at Epoch 67 iteration 100 with loss 0.78260. Total time 2.84944 hours\n",
            "Training at Epoch 67 iteration 200 with loss 0.74929. Total time 2.86666 hours\n",
            "Validation at Epoch 67 , MSE: 1.56802 , Pearson Correlation: 0.20424 with p-value: 2.42691 , Concordance Index: 0.59549\n",
            "Training at Epoch 68 iteration 0 with loss 0.58938. Total time 2.87527 hours\n",
            "Training at Epoch 68 iteration 100 with loss 0.73658. Total time 2.89222 hours\n",
            "Training at Epoch 68 iteration 200 with loss 0.75321. Total time 2.90944 hours\n",
            "Validation at Epoch 68 , MSE: 1.53964 , Pearson Correlation: 0.21339 with p-value: 3.16783 , Concordance Index: 0.60802\n",
            "Training at Epoch 69 iteration 0 with loss 0.72712. Total time 2.91805 hours\n",
            "Training at Epoch 69 iteration 100 with loss 0.97896. Total time 2.93527 hours\n",
            "Training at Epoch 69 iteration 200 with loss 0.83643. Total time 2.95222 hours\n",
            "Validation at Epoch 69 , MSE: 1.59098 , Pearson Correlation: 0.19075 with p-value: 8.89451 , Concordance Index: 0.60121\n",
            "Training at Epoch 70 iteration 0 with loss 0.61041. Total time 2.96083 hours\n",
            "Training at Epoch 70 iteration 100 with loss 0.64881. Total time 2.97805 hours\n",
            "Training at Epoch 70 iteration 200 with loss 0.84894. Total time 2.99527 hours\n",
            "Validation at Epoch 70 , MSE: 1.56726 , Pearson Correlation: 0.18826 with p-value: 4.30977 , Concordance Index: 0.59702\n",
            "Training at Epoch 71 iteration 0 with loss 0.56213. Total time 3.00388 hours\n",
            "Training at Epoch 71 iteration 100 with loss 0.79680. Total time 3.02111 hours\n",
            "Training at Epoch 71 iteration 200 with loss 0.74125. Total time 3.03805 hours\n",
            "Validation at Epoch 71 , MSE: 1.56787 , Pearson Correlation: 0.20263 with p-value: 3.66872 , Concordance Index: 0.60189\n",
            "Training at Epoch 72 iteration 0 with loss 0.56805. Total time 3.04694 hours\n",
            "Training at Epoch 72 iteration 100 with loss 0.85374. Total time 3.06416 hours\n",
            "Training at Epoch 72 iteration 200 with loss 0.64791. Total time 3.08138 hours\n",
            "Validation at Epoch 72 , MSE: 1.59546 , Pearson Correlation: 0.20591 with p-value: 1.43330 , Concordance Index: 0.60080\n",
            "Training at Epoch 73 iteration 0 with loss 0.65091. Total time 3.09 hours\n",
            "Training at Epoch 73 iteration 100 with loss 0.91414. Total time 3.10722 hours\n",
            "Training at Epoch 73 iteration 200 with loss 0.64721. Total time 3.12444 hours\n",
            "Validation at Epoch 73 , MSE: 1.64392 , Pearson Correlation: 0.19549 with p-value: 4.65344 , Concordance Index: 0.60252\n",
            "Training at Epoch 74 iteration 0 with loss 0.58457. Total time 3.13305 hours\n",
            "Training at Epoch 74 iteration 100 with loss 0.58273. Total time 3.15027 hours\n",
            "Training at Epoch 74 iteration 200 with loss 0.99174. Total time 3.16722 hours\n",
            "Validation at Epoch 74 , MSE: 1.59295 , Pearson Correlation: 0.17880 with p-value: 6.85798 , Concordance Index: 0.58911\n",
            "Training at Epoch 75 iteration 0 with loss 0.55673. Total time 3.17583 hours\n",
            "Training at Epoch 75 iteration 100 with loss 0.78287. Total time 3.19305 hours\n",
            "Training at Epoch 75 iteration 200 with loss 0.63260. Total time 3.21027 hours\n",
            "Validation at Epoch 75 , MSE: 1.58179 , Pearson Correlation: 0.19656 with p-value: 8.20240 , Concordance Index: 0.60028\n",
            "Training at Epoch 76 iteration 0 with loss 0.54000. Total time 3.21888 hours\n",
            "Training at Epoch 76 iteration 100 with loss 0.76027. Total time 3.23583 hours\n",
            "Training at Epoch 76 iteration 200 with loss 0.60753. Total time 3.25305 hours\n",
            "Validation at Epoch 76 , MSE: 1.60422 , Pearson Correlation: 0.19918 with p-value: 1.13386 , Concordance Index: 0.59949\n",
            "Training at Epoch 77 iteration 0 with loss 0.60880. Total time 3.26166 hours\n",
            "Training at Epoch 77 iteration 100 with loss 0.68566. Total time 3.27888 hours\n",
            "Training at Epoch 77 iteration 200 with loss 0.60135. Total time 3.29611 hours\n",
            "Validation at Epoch 77 , MSE: 1.57934 , Pearson Correlation: 0.19602 with p-value: 1.99278 , Concordance Index: 0.59745\n",
            "Training at Epoch 78 iteration 0 with loss 0.67929. Total time 3.30472 hours\n",
            "Training at Epoch 78 iteration 100 with loss 0.86431. Total time 3.32166 hours\n",
            "Training at Epoch 78 iteration 200 with loss 0.59361. Total time 3.33888 hours\n",
            "Validation at Epoch 78 , MSE: 1.54442 , Pearson Correlation: 0.21060 with p-value: 4.33030 , Concordance Index: 0.60564\n",
            "Training at Epoch 79 iteration 0 with loss 0.91767. Total time 3.3475 hours\n",
            "Training at Epoch 79 iteration 100 with loss 0.66685. Total time 3.36472 hours\n",
            "Training at Epoch 79 iteration 200 with loss 0.82493. Total time 3.38194 hours\n",
            "Validation at Epoch 79 , MSE: 1.62805 , Pearson Correlation: 0.19850 with p-value: 3.48914 , Concordance Index: 0.60066\n",
            "Training at Epoch 80 iteration 0 with loss 0.60442. Total time 3.39055 hours\n",
            "Training at Epoch 80 iteration 100 with loss 0.67915. Total time 3.40777 hours\n",
            "Training at Epoch 80 iteration 200 with loss 0.77898. Total time 3.425 hours\n",
            "Validation at Epoch 80 , MSE: 1.64930 , Pearson Correlation: 0.18970 with p-value: 4.58100 , Concordance Index: 0.59448\n",
            "Training at Epoch 81 iteration 0 with loss 0.62487. Total time 3.43361 hours\n",
            "Training at Epoch 81 iteration 100 with loss 0.52840. Total time 3.45055 hours\n",
            "Training at Epoch 81 iteration 200 with loss 0.53477. Total time 3.46777 hours\n",
            "Validation at Epoch 81 , MSE: 1.67050 , Pearson Correlation: 0.17698 with p-value: 9.77062 , Concordance Index: 0.59384\n",
            "Training at Epoch 82 iteration 0 with loss 0.66016. Total time 3.47638 hours\n",
            "Training at Epoch 82 iteration 100 with loss 0.77362. Total time 3.49361 hours\n",
            "Training at Epoch 82 iteration 200 with loss 0.74091. Total time 3.51083 hours\n",
            "Validation at Epoch 82 , MSE: 1.66805 , Pearson Correlation: 0.18715 with p-value: 2.42170 , Concordance Index: 0.59468\n",
            "Training at Epoch 83 iteration 0 with loss 0.53208. Total time 3.51944 hours\n",
            "Training at Epoch 83 iteration 100 with loss 0.70020. Total time 3.53666 hours\n",
            "Training at Epoch 83 iteration 200 with loss 0.48581. Total time 3.55388 hours\n",
            "Validation at Epoch 83 , MSE: 1.64661 , Pearson Correlation: 0.19146 with p-value: 2.89207 , Concordance Index: 0.59076\n",
            "Training at Epoch 84 iteration 0 with loss 0.39985. Total time 3.5625 hours\n",
            "Training at Epoch 84 iteration 100 with loss 0.48113. Total time 3.57972 hours\n",
            "Training at Epoch 84 iteration 200 with loss 0.69549. Total time 3.59666 hours\n",
            "Validation at Epoch 84 , MSE: 1.58026 , Pearson Correlation: 0.17468 with p-value: 2.72148 , Concordance Index: 0.58695\n",
            "Training at Epoch 85 iteration 0 with loss 0.47154. Total time 3.60527 hours\n",
            "Training at Epoch 85 iteration 100 with loss 0.61385. Total time 3.6225 hours\n",
            "Training at Epoch 85 iteration 200 with loss 0.61671. Total time 3.63972 hours\n",
            "Validation at Epoch 85 , MSE: 1.60915 , Pearson Correlation: 0.17331 with p-value: 1.91277 , Concordance Index: 0.59026\n",
            "Training at Epoch 86 iteration 0 with loss 0.71849. Total time 3.64833 hours\n",
            "Training at Epoch 86 iteration 100 with loss 0.74650. Total time 3.66527 hours\n",
            "Training at Epoch 86 iteration 200 with loss 0.61249. Total time 3.6825 hours\n",
            "Validation at Epoch 86 , MSE: 1.68525 , Pearson Correlation: 0.18509 with p-value: 5.62160 , Concordance Index: 0.58709\n",
            "Training at Epoch 87 iteration 0 with loss 0.61569. Total time 3.69111 hours\n",
            "Training at Epoch 87 iteration 100 with loss 0.62983. Total time 3.70805 hours\n",
            "Training at Epoch 87 iteration 200 with loss 0.56410. Total time 3.72527 hours\n",
            "Validation at Epoch 87 , MSE: 1.68525 , Pearson Correlation: 0.18530 with p-value: 4.11839 , Concordance Index: 0.59028\n",
            "Training at Epoch 88 iteration 0 with loss 0.50451. Total time 3.73388 hours\n",
            "Training at Epoch 88 iteration 100 with loss 0.57541. Total time 3.75083 hours\n",
            "Training at Epoch 88 iteration 200 with loss 0.66001. Total time 3.76805 hours\n",
            "Validation at Epoch 88 , MSE: 1.62463 , Pearson Correlation: 0.19903 with p-value: 1.44736 , Concordance Index: 0.59451\n",
            "Training at Epoch 89 iteration 0 with loss 0.45800. Total time 3.77666 hours\n",
            "Training at Epoch 89 iteration 100 with loss 0.51849. Total time 3.79388 hours\n",
            "Training at Epoch 89 iteration 200 with loss 0.73282. Total time 3.81083 hours\n",
            "Validation at Epoch 89 , MSE: 1.61281 , Pearson Correlation: 0.18680 with p-value: 4.14796 , Concordance Index: 0.58996\n",
            "Training at Epoch 90 iteration 0 with loss 0.47728. Total time 3.81944 hours\n",
            "Training at Epoch 90 iteration 100 with loss 0.48720. Total time 3.83666 hours\n",
            "Training at Epoch 90 iteration 200 with loss 0.64775. Total time 3.85361 hours\n",
            "Validation at Epoch 90 , MSE: 1.63205 , Pearson Correlation: 0.19480 with p-value: 1.41969 , Concordance Index: 0.59597\n",
            "Training at Epoch 91 iteration 0 with loss 0.38002. Total time 3.86222 hours\n",
            "Training at Epoch 91 iteration 100 with loss 0.47585. Total time 3.87944 hours\n",
            "Training at Epoch 91 iteration 200 with loss 0.66036. Total time 3.89638 hours\n",
            "Validation at Epoch 91 , MSE: 1.64642 , Pearson Correlation: 0.19105 with p-value: 5.57746 , Concordance Index: 0.59060\n",
            "Training at Epoch 92 iteration 0 with loss 0.42245. Total time 3.905 hours\n",
            "Training at Epoch 92 iteration 100 with loss 0.58634. Total time 3.92194 hours\n",
            "Training at Epoch 92 iteration 200 with loss 0.63008. Total time 3.93916 hours\n",
            "Validation at Epoch 92 , MSE: 1.65179 , Pearson Correlation: 0.19964 with p-value: 5.31500 , Concordance Index: 0.59735\n",
            "Training at Epoch 93 iteration 0 with loss 0.35253. Total time 3.94777 hours\n",
            "Training at Epoch 93 iteration 100 with loss 0.46836. Total time 3.96472 hours\n",
            "Training at Epoch 93 iteration 200 with loss 0.53237. Total time 3.98194 hours\n",
            "Validation at Epoch 93 , MSE: 1.68399 , Pearson Correlation: 0.16929 with p-value: 5.34993 , Concordance Index: 0.58417\n",
            "Training at Epoch 94 iteration 0 with loss 0.55635. Total time 3.99055 hours\n",
            "Training at Epoch 94 iteration 100 with loss 0.44840. Total time 4.0075 hours\n",
            "Training at Epoch 94 iteration 200 with loss 0.59555. Total time 4.02472 hours\n",
            "Validation at Epoch 94 , MSE: 1.65009 , Pearson Correlation: 0.17932 with p-value: 3.17820 , Concordance Index: 0.58692\n",
            "Training at Epoch 95 iteration 0 with loss 0.50068. Total time 4.03305 hours\n",
            "Training at Epoch 95 iteration 100 with loss 0.53672. Total time 4.05027 hours\n",
            "Training at Epoch 95 iteration 200 with loss 0.47361. Total time 4.06722 hours\n",
            "Validation at Epoch 95 , MSE: 1.67794 , Pearson Correlation: 0.17274 with p-value: 4.27619 , Concordance Index: 0.58311\n",
            "Training at Epoch 96 iteration 0 with loss 0.57784. Total time 4.07583 hours\n",
            "Training at Epoch 96 iteration 100 with loss 0.45167. Total time 4.09305 hours\n",
            "Training at Epoch 96 iteration 200 with loss 0.64311. Total time 4.11 hours\n",
            "Validation at Epoch 96 , MSE: 1.65016 , Pearson Correlation: 0.16540 with p-value: 1.10261 , Concordance Index: 0.58631\n",
            "Training at Epoch 97 iteration 0 with loss 0.49665. Total time 4.11861 hours\n",
            "Training at Epoch 97 iteration 100 with loss 0.55070. Total time 4.13583 hours\n",
            "Training at Epoch 97 iteration 200 with loss 0.43748. Total time 4.15277 hours\n",
            "Validation at Epoch 97 , MSE: 1.66139 , Pearson Correlation: 0.18482 with p-value: 8.54470 , Concordance Index: 0.58877\n",
            "Training at Epoch 98 iteration 0 with loss 0.59147. Total time 4.16138 hours\n",
            "Training at Epoch 98 iteration 100 with loss 0.42832. Total time 4.17861 hours\n",
            "Training at Epoch 98 iteration 200 with loss 0.51914. Total time 4.19555 hours\n",
            "Validation at Epoch 98 , MSE: 1.67241 , Pearson Correlation: 0.17521 with p-value: 1.26378 , Concordance Index: 0.58459\n",
            "Training at Epoch 99 iteration 0 with loss 0.55375. Total time 4.20416 hours\n",
            "Training at Epoch 99 iteration 100 with loss 0.50390. Total time 4.22111 hours\n",
            "Training at Epoch 99 iteration 200 with loss 0.61989. Total time 4.23833 hours\n",
            "Validation at Epoch 99 , MSE: 1.67220 , Pearson Correlation: 0.17671 with p-value: 1.44600 , Concordance Index: 0.57945\n",
            "Training at Epoch 100 iteration 0 with loss 0.49616. Total time 4.24694 hours\n",
            "Training at Epoch 100 iteration 100 with loss 0.53439. Total time 4.26388 hours\n",
            "Training at Epoch 100 iteration 200 with loss 0.39119. Total time 4.28111 hours\n",
            "Validation at Epoch 100 , MSE: 1.76015 , Pearson Correlation: 0.16262 with p-value: 4.59080 , Concordance Index: 0.57995\n",
            "--- Go for Testing ---\n",
            "Testing MSE: 1.3492910755590664 , Pearson Correlation: 0.29319999725802137 with p-value: 3.8866759121479926e-153 , Concordance Index: 0.624096063721686\n",
            "--- Training Finished ---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQPV7PykA4M7"
      },
      "source": [
        "pickle_rick.dump(model, open(models_folder + \"G35_model_DeepDTA.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDuYWZIA2BiI"
      },
      "source": [
        "The MSE of this model was 1.3492910755590664 on the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVa7PTAl2Rob"
      },
      "source": [
        "# First model training\n",
        "Time to train our model. We will load back our data we saved earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuTReBGJ2cjS"
      },
      "source": [
        "train = pd.read_pickle(data_folder + 'G35_data_train.pkl')\n",
        "val = pd.read_pickle(data_folder + 'G35_data_vali.pkl')\n",
        "test = pd.read_pickle(data_folder + 'G35_data_test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FijrGDkE2ofd"
      },
      "source": [
        "Our configuration for our first run will follow the MT-DTI transformer structure (bert-base) for the the transformer model, and the DeepConvDTI hyperparameters froms that paper for the protein encoder, with the DeepDTA decoder.\n",
        "\n",
        "The additional code used to implement the DeepConvDTI encoder can be found in the inspire_encoder.py file included."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uWj9W0M3vhy"
      },
      "source": [
        "drug_encoding, target_encoding = 'Transformer', 'CNN_inspire'\n",
        "\n",
        "config = generate_config(drug_encoding = drug_encoding, \n",
        "                      target_encoding = target_encoding, \n",
        "                      result_folder = results_folder + 'first_colab',\n",
        "                      cls_hidden_dims = [1024,1024,512],  \n",
        "                      train_epoch = 100, \n",
        "                      LR = 0.0001, \n",
        "                      batch_size = 64,\n",
        "                      inspire_activation = 'elu',\n",
        "                      CNN_inspire_filters = 128,\n",
        "                      protein_strides = [10, 15, 20, 25, 30],\n",
        "                      inspire_dropout =  0,\n",
        "                      protein_layers =  [128],\n",
        "                      transformer_emb_size_drug = 128,\n",
        "                      transformer_intermediate_size_drug = 512,\n",
        "                      transformer_num_attention_heads_drug = 8,\n",
        "                      transformer_n_layer_drug = 8,\n",
        "                      transformer_dropout_rate = 0.1,\n",
        "                      transformer_attention_probs_dropout = 0.1,\n",
        "                      transformer_hidden_dropout_rate = 0.1,\n",
        "                      num_workers = 0,\n",
        "                      decay=0.0001\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9AOF6rb5KjZ"
      },
      "source": [
        "We will use the DeepPurpose model / training. The DeepPurpose trainer at every epoch employs early stopping. If the validation loss goes up, it will retrieve the previous model with the lowest validation loss.\n",
        "\n",
        "The optimizer used by DeepPurpose is Adam, and the loss function is MSE.\n",
        "\n",
        "We choose to drop the last batch, as for all power of 2 batch sizes under 256, the last batch will have a size of 1 in our dataset. This will cause the batch normalization layer to fail (this is a quirk in PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oIQsdHFI6e_Q",
        "outputId": "6b484652-0eb2-448c-9dbb-45ff4ce118ee"
      },
      "source": [
        "model = models.model_initialize(**config)\n",
        "model.train(train, val, test, drop_last = True, save_path = models_folder + 'model_first_colab')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's use 1 GPU!\n",
            "--- Data Preparation ---\n",
            "--- Go for Training ---\n",
            "Training at Epoch 1 iteration 0 with loss 32.2019. Total time 0.0 hours\n",
            "Training at Epoch 1 iteration 100 with loss 1.29257. Total time 0.00694 hours\n",
            "Training at Epoch 1 iteration 200 with loss 1.12035. Total time 0.01361 hours\n",
            "Training at Epoch 1 iteration 300 with loss 1.82808. Total time 0.02055 hours\n",
            "Training at Epoch 1 iteration 400 with loss 1.70025. Total time 0.02777 hours\n",
            "Training at Epoch 1 iteration 500 with loss 1.76557. Total time 0.03472 hours\n",
            "Training at Epoch 1 iteration 600 with loss 1.68387. Total time 0.04166 hours\n",
            "Training at Epoch 1 iteration 700 with loss 1.28168. Total time 0.04888 hours\n",
            "Training at Epoch 1 iteration 800 with loss 1.72632. Total time 0.05611 hours\n",
            "Validation at Epoch 1 , MSE: 1.41617 , Pearson Correlation: 0.22611 with p-value: 8.12353 , Concordance Index: 0.59654\n",
            "Training at Epoch 2 iteration 0 with loss 0.77184. Total time 0.06444 hours\n",
            "Training at Epoch 2 iteration 100 with loss 1.39129. Total time 0.07166 hours\n",
            "Training at Epoch 2 iteration 200 with loss 1.08888. Total time 0.07888 hours\n",
            "Training at Epoch 2 iteration 300 with loss 0.90090. Total time 0.08611 hours\n",
            "Training at Epoch 2 iteration 400 with loss 1.88315. Total time 0.09361 hours\n",
            "Training at Epoch 2 iteration 500 with loss 1.71361. Total time 0.10083 hours\n",
            "Training at Epoch 2 iteration 600 with loss 1.50917. Total time 0.10833 hours\n",
            "Training at Epoch 2 iteration 700 with loss 1.08013. Total time 0.11555 hours\n",
            "Training at Epoch 2 iteration 800 with loss 1.91128. Total time 0.12305 hours\n",
            "Validation at Epoch 2 , MSE: 1.37881 , Pearson Correlation: 0.23303 with p-value: 1.47012 , Concordance Index: 0.60244\n",
            "Training at Epoch 3 iteration 0 with loss 1.88066. Total time 0.13138 hours\n",
            "Training at Epoch 3 iteration 100 with loss 1.47505. Total time 0.13888 hours\n",
            "Training at Epoch 3 iteration 200 with loss 1.20648. Total time 0.14611 hours\n",
            "Training at Epoch 3 iteration 300 with loss 1.44921. Total time 0.15333 hours\n",
            "Training at Epoch 3 iteration 400 with loss 1.17376. Total time 0.16083 hours\n",
            "Training at Epoch 3 iteration 500 with loss 2.56142. Total time 0.16805 hours\n",
            "Training at Epoch 3 iteration 600 with loss 1.22621. Total time 0.17555 hours\n",
            "Training at Epoch 3 iteration 700 with loss 0.88592. Total time 0.18277 hours\n",
            "Training at Epoch 3 iteration 800 with loss 0.60171. Total time 0.19027 hours\n",
            "Validation at Epoch 3 , MSE: 1.37134 , Pearson Correlation: 0.26006 with p-value: 7.24902 , Concordance Index: 0.60803\n",
            "Training at Epoch 4 iteration 0 with loss 1.07760. Total time 0.19888 hours\n",
            "Training at Epoch 4 iteration 100 with loss 1.08614. Total time 0.20611 hours\n",
            "Training at Epoch 4 iteration 200 with loss 1.87865. Total time 0.21361 hours\n",
            "Training at Epoch 4 iteration 300 with loss 1.39858. Total time 0.22083 hours\n",
            "Training at Epoch 4 iteration 400 with loss 1.38096. Total time 0.22833 hours\n",
            "Training at Epoch 4 iteration 500 with loss 1.45662. Total time 0.23555 hours\n",
            "Training at Epoch 4 iteration 600 with loss 1.32688. Total time 0.24305 hours\n",
            "Training at Epoch 4 iteration 700 with loss 1.41026. Total time 0.25027 hours\n",
            "Training at Epoch 4 iteration 800 with loss 1.61012. Total time 0.25777 hours\n",
            "Validation at Epoch 4 , MSE: 1.40003 , Pearson Correlation: 0.25268 with p-value: 6.21763 , Concordance Index: 0.60320\n",
            "Training at Epoch 5 iteration 0 with loss 1.41581. Total time 0.26611 hours\n",
            "Training at Epoch 5 iteration 100 with loss 1.08205. Total time 0.27361 hours\n",
            "Training at Epoch 5 iteration 200 with loss 1.28604. Total time 0.28111 hours\n",
            "Training at Epoch 5 iteration 300 with loss 1.66973. Total time 0.28833 hours\n",
            "Training at Epoch 5 iteration 400 with loss 1.09949. Total time 0.29555 hours\n",
            "Training at Epoch 5 iteration 500 with loss 1.10380. Total time 0.30305 hours\n",
            "Training at Epoch 5 iteration 600 with loss 1.10389. Total time 0.31055 hours\n",
            "Training at Epoch 5 iteration 700 with loss 1.07870. Total time 0.31777 hours\n",
            "Training at Epoch 5 iteration 800 with loss 1.39799. Total time 0.32527 hours\n",
            "Validation at Epoch 5 , MSE: 1.43374 , Pearson Correlation: 0.27584 with p-value: 1.85308 , Concordance Index: 0.61752\n",
            "Training at Epoch 6 iteration 0 with loss 1.52789. Total time 0.33361 hours\n",
            "Training at Epoch 6 iteration 100 with loss 1.75360. Total time 0.34111 hours\n",
            "Training at Epoch 6 iteration 200 with loss 1.17554. Total time 0.34833 hours\n",
            "Training at Epoch 6 iteration 300 with loss 1.11767. Total time 0.35583 hours\n",
            "Training at Epoch 6 iteration 400 with loss 1.38532. Total time 0.36305 hours\n",
            "Training at Epoch 6 iteration 500 with loss 1.36198. Total time 0.37055 hours\n",
            "Training at Epoch 6 iteration 600 with loss 1.03124. Total time 0.37777 hours\n",
            "Training at Epoch 6 iteration 700 with loss 1.76155. Total time 0.38527 hours\n",
            "Training at Epoch 6 iteration 800 with loss 1.76601. Total time 0.3925 hours\n",
            "Validation at Epoch 6 , MSE: 1.36365 , Pearson Correlation: 0.25919 with p-value: 4.85031 , Concordance Index: 0.60864\n",
            "Training at Epoch 7 iteration 0 with loss 0.85315. Total time 0.40111 hours\n",
            "Training at Epoch 7 iteration 100 with loss 1.28102. Total time 0.40861 hours\n",
            "Training at Epoch 7 iteration 200 with loss 1.38990. Total time 0.41583 hours\n",
            "Training at Epoch 7 iteration 300 with loss 1.24877. Total time 0.42333 hours\n",
            "Training at Epoch 7 iteration 400 with loss 1.20118. Total time 0.43055 hours\n",
            "Training at Epoch 7 iteration 500 with loss 1.32790. Total time 0.43805 hours\n",
            "Training at Epoch 7 iteration 600 with loss 1.58467. Total time 0.44527 hours\n",
            "Training at Epoch 7 iteration 700 with loss 1.23972. Total time 0.45277 hours\n",
            "Training at Epoch 7 iteration 800 with loss 0.83145. Total time 0.46 hours\n",
            "Validation at Epoch 7 , MSE: 1.40938 , Pearson Correlation: 0.26621 with p-value: 8.01336 , Concordance Index: 0.60907\n",
            "Training at Epoch 8 iteration 0 with loss 0.93552. Total time 0.46861 hours\n",
            "Training at Epoch 8 iteration 100 with loss 1.64366. Total time 0.47583 hours\n",
            "Training at Epoch 8 iteration 200 with loss 1.68307. Total time 0.48333 hours\n",
            "Training at Epoch 8 iteration 300 with loss 1.35139. Total time 0.49083 hours\n",
            "Training at Epoch 8 iteration 400 with loss 0.99523. Total time 0.49805 hours\n",
            "Training at Epoch 8 iteration 500 with loss 1.10573. Total time 0.50555 hours\n",
            "Training at Epoch 8 iteration 600 with loss 0.90663. Total time 0.51277 hours\n",
            "Training at Epoch 8 iteration 700 with loss 1.59508. Total time 0.52027 hours\n",
            "Training at Epoch 8 iteration 800 with loss 1.20527. Total time 0.5275 hours\n",
            "Validation at Epoch 8 , MSE: 1.47805 , Pearson Correlation: 0.25930 with p-value: 3.84110 , Concordance Index: 0.61611\n",
            "Training at Epoch 9 iteration 0 with loss 1.31564. Total time 0.53611 hours\n",
            "Training at Epoch 9 iteration 100 with loss 1.48970. Total time 0.54333 hours\n",
            "Training at Epoch 9 iteration 200 with loss 1.72752. Total time 0.55083 hours\n",
            "Training at Epoch 9 iteration 300 with loss 1.06800. Total time 0.55833 hours\n",
            "Training at Epoch 9 iteration 400 with loss 1.22902. Total time 0.56555 hours\n",
            "Training at Epoch 9 iteration 500 with loss 0.81548. Total time 0.57305 hours\n",
            "Training at Epoch 9 iteration 600 with loss 0.75638. Total time 0.58027 hours\n",
            "Training at Epoch 9 iteration 700 with loss 1.42444. Total time 0.58777 hours\n",
            "Training at Epoch 9 iteration 800 with loss 1.18185. Total time 0.595 hours\n",
            "Validation at Epoch 9 , MSE: 1.41037 , Pearson Correlation: 0.24791 with p-value: 1.42502 , Concordance Index: 0.60938\n",
            "Training at Epoch 10 iteration 0 with loss 1.32881. Total time 0.60361 hours\n",
            "Training at Epoch 10 iteration 100 with loss 1.26512. Total time 0.61083 hours\n",
            "Training at Epoch 10 iteration 200 with loss 1.23323. Total time 0.61833 hours\n",
            "Training at Epoch 10 iteration 300 with loss 1.03603. Total time 0.62555 hours\n",
            "Training at Epoch 10 iteration 400 with loss 0.89555. Total time 0.63305 hours\n",
            "Training at Epoch 10 iteration 500 with loss 1.16964. Total time 0.64027 hours\n",
            "Training at Epoch 10 iteration 600 with loss 0.82357. Total time 0.64777 hours\n",
            "Training at Epoch 10 iteration 700 with loss 1.02830. Total time 0.655 hours\n",
            "Training at Epoch 10 iteration 800 with loss 1.29236. Total time 0.6625 hours\n",
            "Validation at Epoch 10 , MSE: 1.37887 , Pearson Correlation: 0.27846 with p-value: 3.75639 , Concordance Index: 0.61910\n",
            "Training at Epoch 11 iteration 0 with loss 1.59541. Total time 0.67083 hours\n",
            "Training at Epoch 11 iteration 100 with loss 1.36934. Total time 0.67833 hours\n",
            "Training at Epoch 11 iteration 200 with loss 0.81967. Total time 0.68555 hours\n",
            "Training at Epoch 11 iteration 300 with loss 1.10960. Total time 0.69305 hours\n",
            "Training at Epoch 11 iteration 400 with loss 1.16034. Total time 0.70027 hours\n",
            "Training at Epoch 11 iteration 500 with loss 1.01611. Total time 0.70777 hours\n",
            "Training at Epoch 11 iteration 600 with loss 0.95865. Total time 0.71527 hours\n",
            "Training at Epoch 11 iteration 700 with loss 0.92458. Total time 0.7225 hours\n",
            "Training at Epoch 11 iteration 800 with loss 1.21293. Total time 0.73 hours\n",
            "Validation at Epoch 11 , MSE: 1.44272 , Pearson Correlation: 0.25363 with p-value: 8.23302 , Concordance Index: 0.61183\n",
            "Training at Epoch 12 iteration 0 with loss 1.28862. Total time 0.73833 hours\n",
            "Training at Epoch 12 iteration 100 with loss 1.56018. Total time 0.74583 hours\n",
            "Training at Epoch 12 iteration 200 with loss 1.09760. Total time 0.75305 hours\n",
            "Training at Epoch 12 iteration 300 with loss 1.16433. Total time 0.76055 hours\n",
            "Training at Epoch 12 iteration 400 with loss 1.13178. Total time 0.76805 hours\n",
            "Training at Epoch 12 iteration 500 with loss 1.45574. Total time 0.77527 hours\n",
            "Training at Epoch 12 iteration 600 with loss 1.14798. Total time 0.78277 hours\n",
            "Training at Epoch 12 iteration 700 with loss 1.00927. Total time 0.79 hours\n",
            "Training at Epoch 12 iteration 800 with loss 1.34683. Total time 0.7975 hours\n",
            "Validation at Epoch 12 , MSE: 1.39781 , Pearson Correlation: 0.26611 with p-value: 1.01980 , Concordance Index: 0.61685\n",
            "Training at Epoch 13 iteration 0 with loss 1.15773. Total time 0.80583 hours\n",
            "Training at Epoch 13 iteration 100 with loss 0.82419. Total time 0.81333 hours\n",
            "Training at Epoch 13 iteration 200 with loss 0.79910. Total time 0.82055 hours\n",
            "Training at Epoch 13 iteration 300 with loss 1.63826. Total time 0.82805 hours\n",
            "Training at Epoch 13 iteration 400 with loss 0.98930. Total time 0.83527 hours\n",
            "Training at Epoch 13 iteration 500 with loss 1.28067. Total time 0.84277 hours\n",
            "Training at Epoch 13 iteration 600 with loss 1.00615. Total time 0.85027 hours\n",
            "Training at Epoch 13 iteration 700 with loss 1.14136. Total time 0.8575 hours\n",
            "Training at Epoch 13 iteration 800 with loss 1.47368. Total time 0.865 hours\n",
            "Validation at Epoch 13 , MSE: 1.37401 , Pearson Correlation: 0.26816 with p-value: 9.75589 , Concordance Index: 0.61639\n",
            "Training at Epoch 14 iteration 0 with loss 1.65051. Total time 0.87333 hours\n",
            "Training at Epoch 14 iteration 100 with loss 1.33604. Total time 0.88083 hours\n",
            "Training at Epoch 14 iteration 200 with loss 1.42735. Total time 0.88805 hours\n",
            "Training at Epoch 14 iteration 300 with loss 1.66643. Total time 0.89555 hours\n",
            "Training at Epoch 14 iteration 400 with loss 1.35388. Total time 0.90277 hours\n",
            "Training at Epoch 14 iteration 500 with loss 1.32791. Total time 0.91027 hours\n",
            "Training at Epoch 14 iteration 600 with loss 1.13159. Total time 0.9175 hours\n",
            "Training at Epoch 14 iteration 700 with loss 1.22417. Total time 0.925 hours\n",
            "Training at Epoch 14 iteration 800 with loss 1.16020. Total time 0.93222 hours\n",
            "Validation at Epoch 14 , MSE: 1.45332 , Pearson Correlation: 0.24250 with p-value: 9.77177 , Concordance Index: 0.60512\n",
            "Training at Epoch 15 iteration 0 with loss 1.36128. Total time 0.94083 hours\n",
            "Training at Epoch 15 iteration 100 with loss 0.94521. Total time 0.94805 hours\n",
            "Training at Epoch 15 iteration 200 with loss 0.89392. Total time 0.95555 hours\n",
            "Training at Epoch 15 iteration 300 with loss 1.04221. Total time 0.96305 hours\n",
            "Training at Epoch 15 iteration 400 with loss 1.55822. Total time 0.97027 hours\n",
            "Training at Epoch 15 iteration 500 with loss 1.37433. Total time 0.9775 hours\n",
            "Training at Epoch 15 iteration 600 with loss 1.28501. Total time 0.985 hours\n",
            "Training at Epoch 15 iteration 700 with loss 1.61649. Total time 0.9925 hours\n",
            "Training at Epoch 15 iteration 800 with loss 1.01906. Total time 0.99972 hours\n",
            "Validation at Epoch 15 , MSE: 1.47113 , Pearson Correlation: 0.23474 with p-value: 5.23402 , Concordance Index: 0.60271\n",
            "Training at Epoch 16 iteration 0 with loss 0.62541. Total time 1.00833 hours\n",
            "Training at Epoch 16 iteration 100 with loss 1.02801. Total time 1.01555 hours\n",
            "Training at Epoch 16 iteration 200 with loss 1.38136. Total time 1.02305 hours\n",
            "Training at Epoch 16 iteration 300 with loss 1.26592. Total time 1.03027 hours\n",
            "Training at Epoch 16 iteration 400 with loss 1.26653. Total time 1.03777 hours\n",
            "Training at Epoch 16 iteration 500 with loss 0.84544. Total time 1.045 hours\n",
            "Training at Epoch 16 iteration 600 with loss 1.54139. Total time 1.0525 hours\n",
            "Training at Epoch 16 iteration 700 with loss 0.96611. Total time 1.05972 hours\n",
            "Training at Epoch 16 iteration 800 with loss 1.23896. Total time 1.06722 hours\n",
            "Validation at Epoch 16 , MSE: 1.52048 , Pearson Correlation: 0.24648 with p-value: 2.82019 , Concordance Index: 0.61074\n",
            "Training at Epoch 17 iteration 0 with loss 1.27034. Total time 1.07555 hours\n",
            "Training at Epoch 17 iteration 100 with loss 1.34359. Total time 1.08305 hours\n",
            "Training at Epoch 17 iteration 200 with loss 1.20749. Total time 1.09027 hours\n",
            "Training at Epoch 17 iteration 300 with loss 0.80232. Total time 1.09777 hours\n",
            "Training at Epoch 17 iteration 400 with loss 0.98724. Total time 1.105 hours\n",
            "Training at Epoch 17 iteration 500 with loss 1.23523. Total time 1.1125 hours\n",
            "Training at Epoch 17 iteration 600 with loss 1.30126. Total time 1.11972 hours\n",
            "Training at Epoch 17 iteration 700 with loss 1.24660. Total time 1.12722 hours\n",
            "Training at Epoch 17 iteration 800 with loss 1.03277. Total time 1.13444 hours\n",
            "Validation at Epoch 17 , MSE: 1.52907 , Pearson Correlation: 0.27395 with p-value: 1.53261 , Concordance Index: 0.61897\n",
            "Training at Epoch 18 iteration 0 with loss 0.79659. Total time 1.14305 hours\n",
            "Training at Epoch 18 iteration 100 with loss 1.48022. Total time 1.15027 hours\n",
            "Training at Epoch 18 iteration 200 with loss 1.12956. Total time 1.15777 hours\n",
            "Training at Epoch 18 iteration 300 with loss 0.71751. Total time 1.165 hours\n",
            "Training at Epoch 18 iteration 400 with loss 1.31376. Total time 1.1725 hours\n",
            "Training at Epoch 18 iteration 500 with loss 1.77347. Total time 1.17972 hours\n",
            "Training at Epoch 18 iteration 600 with loss 0.84642. Total time 1.18722 hours\n",
            "Training at Epoch 18 iteration 700 with loss 0.96404. Total time 1.19444 hours\n",
            "Training at Epoch 18 iteration 800 with loss 1.10195. Total time 1.20194 hours\n",
            "Validation at Epoch 18 , MSE: 1.41641 , Pearson Correlation: 0.23834 with p-value: 4.22745 , Concordance Index: 0.60830\n",
            "Training at Epoch 19 iteration 0 with loss 0.89749. Total time 1.21027 hours\n",
            "Training at Epoch 19 iteration 100 with loss 1.04876. Total time 1.21777 hours\n",
            "Training at Epoch 19 iteration 200 with loss 1.17805. Total time 1.225 hours\n",
            "Training at Epoch 19 iteration 300 with loss 1.07014. Total time 1.23222 hours\n",
            "Training at Epoch 19 iteration 400 with loss 1.35106. Total time 1.23972 hours\n",
            "Training at Epoch 19 iteration 500 with loss 1.02119. Total time 1.24694 hours\n",
            "Training at Epoch 19 iteration 600 with loss 1.43161. Total time 1.25444 hours\n",
            "Training at Epoch 19 iteration 700 with loss 1.16117. Total time 1.26194 hours\n",
            "Training at Epoch 19 iteration 800 with loss 1.25239. Total time 1.26916 hours\n",
            "Validation at Epoch 19 , MSE: 1.50715 , Pearson Correlation: 0.26288 with p-value: 1.43023 , Concordance Index: 0.61473\n",
            "Training at Epoch 20 iteration 0 with loss 1.11267. Total time 1.27777 hours\n",
            "Training at Epoch 20 iteration 100 with loss 1.43604. Total time 1.285 hours\n",
            "Training at Epoch 20 iteration 200 with loss 1.00255. Total time 1.2925 hours\n",
            "Training at Epoch 20 iteration 300 with loss 1.09674. Total time 1.29972 hours\n",
            "Training at Epoch 20 iteration 400 with loss 0.73737. Total time 1.30722 hours\n",
            "Training at Epoch 20 iteration 500 with loss 1.10082. Total time 1.31444 hours\n",
            "Training at Epoch 20 iteration 600 with loss 1.28314. Total time 1.32194 hours\n",
            "Training at Epoch 20 iteration 700 with loss 1.20645. Total time 1.32916 hours\n",
            "Training at Epoch 20 iteration 800 with loss 0.91114. Total time 1.33666 hours\n",
            "Validation at Epoch 20 , MSE: 1.40634 , Pearson Correlation: 0.23798 with p-value: 8.63672 , Concordance Index: 0.60450\n",
            "Training at Epoch 21 iteration 0 with loss 0.57132. Total time 1.345 hours\n",
            "Training at Epoch 21 iteration 100 with loss 1.10783. Total time 1.3525 hours\n",
            "Training at Epoch 21 iteration 200 with loss 1.08158. Total time 1.35972 hours\n",
            "Training at Epoch 21 iteration 300 with loss 0.92299. Total time 1.36722 hours\n",
            "Training at Epoch 21 iteration 400 with loss 0.60331. Total time 1.37444 hours\n",
            "Training at Epoch 21 iteration 500 with loss 0.88174. Total time 1.38194 hours\n",
            "Training at Epoch 21 iteration 600 with loss 1.24965. Total time 1.38916 hours\n",
            "Training at Epoch 21 iteration 700 with loss 0.91897. Total time 1.39666 hours\n",
            "Training at Epoch 21 iteration 800 with loss 1.28514. Total time 1.40388 hours\n",
            "Validation at Epoch 21 , MSE: 1.47699 , Pearson Correlation: 0.25865 with p-value: 1.58117 , Concordance Index: 0.61536\n",
            "Training at Epoch 22 iteration 0 with loss 0.85795. Total time 1.4125 hours\n",
            "Training at Epoch 22 iteration 100 with loss 0.83662. Total time 1.41972 hours\n",
            "Training at Epoch 22 iteration 200 with loss 1.56511. Total time 1.42722 hours\n",
            "Training at Epoch 22 iteration 300 with loss 1.08706. Total time 1.43444 hours\n",
            "Training at Epoch 22 iteration 400 with loss 1.61662. Total time 1.44194 hours\n",
            "Training at Epoch 22 iteration 500 with loss 1.19730. Total time 1.44916 hours\n",
            "Training at Epoch 22 iteration 600 with loss 1.33801. Total time 1.45666 hours\n",
            "Training at Epoch 22 iteration 700 with loss 0.63557. Total time 1.46388 hours\n",
            "Training at Epoch 22 iteration 800 with loss 1.00906. Total time 1.47138 hours\n",
            "Validation at Epoch 22 , MSE: 1.45403 , Pearson Correlation: 0.26107 with p-value: 7.85046 , Concordance Index: 0.61449\n",
            "Training at Epoch 23 iteration 0 with loss 1.43425. Total time 1.47972 hours\n",
            "Training at Epoch 23 iteration 100 with loss 1.62468. Total time 1.48722 hours\n",
            "Training at Epoch 23 iteration 200 with loss 1.43327. Total time 1.49444 hours\n",
            "Training at Epoch 23 iteration 300 with loss 0.98383. Total time 1.50194 hours\n",
            "Training at Epoch 23 iteration 400 with loss 1.01501. Total time 1.50916 hours\n",
            "Training at Epoch 23 iteration 500 with loss 0.89936. Total time 1.51666 hours\n",
            "Training at Epoch 23 iteration 600 with loss 1.06936. Total time 1.52388 hours\n",
            "Training at Epoch 23 iteration 700 with loss 1.37446. Total time 1.53138 hours\n",
            "Training at Epoch 23 iteration 800 with loss 0.99002. Total time 1.53861 hours\n",
            "Validation at Epoch 23 , MSE: 1.49310 , Pearson Correlation: 0.23159 with p-value: 2.39745 , Concordance Index: 0.60879\n",
            "Training at Epoch 24 iteration 0 with loss 1.09293. Total time 1.54722 hours\n",
            "Training at Epoch 24 iteration 100 with loss 0.78792. Total time 1.55444 hours\n",
            "Training at Epoch 24 iteration 200 with loss 1.50381. Total time 1.56194 hours\n",
            "Training at Epoch 24 iteration 300 with loss 0.98658. Total time 1.56916 hours\n",
            "Training at Epoch 24 iteration 400 with loss 0.70630. Total time 1.57666 hours\n",
            "Training at Epoch 24 iteration 500 with loss 1.08827. Total time 1.58388 hours\n",
            "Training at Epoch 24 iteration 600 with loss 0.92244. Total time 1.59138 hours\n",
            "Training at Epoch 24 iteration 700 with loss 0.72408. Total time 1.59861 hours\n",
            "Training at Epoch 24 iteration 800 with loss 0.94993. Total time 1.60611 hours\n",
            "Validation at Epoch 24 , MSE: 1.43406 , Pearson Correlation: 0.24180 with p-value: 4.08939 , Concordance Index: 0.60948\n",
            "Training at Epoch 25 iteration 0 with loss 0.70537. Total time 1.61472 hours\n",
            "Training at Epoch 25 iteration 100 with loss 1.04293. Total time 1.62194 hours\n",
            "Training at Epoch 25 iteration 200 with loss 0.76618. Total time 1.62916 hours\n",
            "Training at Epoch 25 iteration 300 with loss 0.97669. Total time 1.63666 hours\n",
            "Training at Epoch 25 iteration 400 with loss 1.27171. Total time 1.64388 hours\n",
            "Training at Epoch 25 iteration 500 with loss 1.38822. Total time 1.65138 hours\n",
            "Training at Epoch 25 iteration 600 with loss 1.09563. Total time 1.65861 hours\n",
            "Training at Epoch 25 iteration 700 with loss 1.01132. Total time 1.66611 hours\n",
            "Training at Epoch 25 iteration 800 with loss 1.72564. Total time 1.67333 hours\n",
            "Validation at Epoch 25 , MSE: 1.43418 , Pearson Correlation: 0.25552 with p-value: 1.43374 , Concordance Index: 0.61282\n",
            "Training at Epoch 26 iteration 0 with loss 1.33853. Total time 1.68194 hours\n",
            "Training at Epoch 26 iteration 100 with loss 0.59548. Total time 1.68916 hours\n",
            "Training at Epoch 26 iteration 200 with loss 0.76678. Total time 1.69638 hours\n",
            "Training at Epoch 26 iteration 300 with loss 0.94711. Total time 1.70388 hours\n",
            "Training at Epoch 26 iteration 400 with loss 1.01366. Total time 1.71111 hours\n",
            "Training at Epoch 26 iteration 500 with loss 0.77556. Total time 1.71861 hours\n",
            "Training at Epoch 26 iteration 600 with loss 1.12225. Total time 1.72583 hours\n",
            "Training at Epoch 26 iteration 700 with loss 0.99705. Total time 1.73333 hours\n",
            "Training at Epoch 26 iteration 800 with loss 1.09130. Total time 1.74055 hours\n",
            "Validation at Epoch 26 , MSE: 1.45736 , Pearson Correlation: 0.24874 with p-value: 2.53542 , Concordance Index: 0.61327\n",
            "Training at Epoch 27 iteration 0 with loss 1.10179. Total time 1.74916 hours\n",
            "Training at Epoch 27 iteration 100 with loss 1.01630. Total time 1.75638 hours\n",
            "Training at Epoch 27 iteration 200 with loss 0.95960. Total time 1.76388 hours\n",
            "Training at Epoch 27 iteration 300 with loss 0.88073. Total time 1.77111 hours\n",
            "Training at Epoch 27 iteration 400 with loss 0.87290. Total time 1.77861 hours\n",
            "Training at Epoch 27 iteration 500 with loss 1.49912. Total time 1.78583 hours\n",
            "Training at Epoch 27 iteration 600 with loss 0.84379. Total time 1.79305 hours\n",
            "Training at Epoch 27 iteration 700 with loss 0.62630. Total time 1.80055 hours\n",
            "Training at Epoch 27 iteration 800 with loss 0.74768. Total time 1.80777 hours\n",
            "Validation at Epoch 27 , MSE: 1.56305 , Pearson Correlation: 0.23343 with p-value: 6.70818 , Concordance Index: 0.60735\n",
            "Training at Epoch 28 iteration 0 with loss 0.93528. Total time 1.81638 hours\n",
            "Training at Epoch 28 iteration 100 with loss 1.12708. Total time 1.82361 hours\n",
            "Training at Epoch 28 iteration 200 with loss 1.05370. Total time 1.83083 hours\n",
            "Training at Epoch 28 iteration 300 with loss 0.85666. Total time 1.83833 hours\n",
            "Training at Epoch 28 iteration 400 with loss 0.51967. Total time 1.84555 hours\n",
            "Training at Epoch 28 iteration 500 with loss 0.99437. Total time 1.85305 hours\n",
            "Training at Epoch 28 iteration 600 with loss 1.05424. Total time 1.86027 hours\n",
            "Training at Epoch 28 iteration 700 with loss 0.84567. Total time 1.8675 hours\n",
            "Training at Epoch 28 iteration 800 with loss 1.10732. Total time 1.875 hours\n",
            "Validation at Epoch 28 , MSE: 1.47683 , Pearson Correlation: 0.26061 with p-value: 2.15919 , Concordance Index: 0.61518\n",
            "Training at Epoch 29 iteration 0 with loss 0.92770. Total time 1.88333 hours\n",
            "Training at Epoch 29 iteration 100 with loss 0.90015. Total time 1.89083 hours\n",
            "Training at Epoch 29 iteration 200 with loss 0.54899. Total time 1.89805 hours\n",
            "Training at Epoch 29 iteration 300 with loss 1.15281. Total time 1.90555 hours\n",
            "Training at Epoch 29 iteration 400 with loss 0.91215. Total time 1.91277 hours\n",
            "Training at Epoch 29 iteration 500 with loss 0.92784. Total time 1.92 hours\n",
            "Training at Epoch 29 iteration 600 with loss 0.52459. Total time 1.9275 hours\n",
            "Training at Epoch 29 iteration 700 with loss 0.95717. Total time 1.93472 hours\n",
            "Training at Epoch 29 iteration 800 with loss 1.04549. Total time 1.94222 hours\n",
            "Validation at Epoch 29 , MSE: 1.48281 , Pearson Correlation: 0.25135 with p-value: 1.05087 , Concordance Index: 0.61446\n",
            "Training at Epoch 30 iteration 0 with loss 0.77219. Total time 1.95055 hours\n",
            "Training at Epoch 30 iteration 100 with loss 0.71260. Total time 1.95805 hours\n",
            "Training at Epoch 30 iteration 200 with loss 1.04485. Total time 1.96527 hours\n",
            "Training at Epoch 30 iteration 300 with loss 0.82594. Total time 1.9725 hours\n",
            "Training at Epoch 30 iteration 400 with loss 1.02060. Total time 1.98 hours\n",
            "Training at Epoch 30 iteration 500 with loss 0.81784. Total time 1.98722 hours\n",
            "Training at Epoch 30 iteration 600 with loss 0.70613. Total time 1.99472 hours\n",
            "Training at Epoch 30 iteration 700 with loss 1.01145. Total time 2.00194 hours\n",
            "Training at Epoch 30 iteration 800 with loss 1.27358. Total time 2.00944 hours\n",
            "Validation at Epoch 30 , MSE: 1.49653 , Pearson Correlation: 0.24896 with p-value: 1.59749 , Concordance Index: 0.61240\n",
            "Training at Epoch 31 iteration 0 with loss 0.77117. Total time 2.01777 hours\n",
            "Training at Epoch 31 iteration 100 with loss 0.93927. Total time 2.02527 hours\n",
            "Training at Epoch 31 iteration 200 with loss 0.69032. Total time 2.0325 hours\n",
            "Training at Epoch 31 iteration 300 with loss 0.40118. Total time 2.04 hours\n",
            "Training at Epoch 31 iteration 400 with loss 0.86891. Total time 2.04722 hours\n",
            "Training at Epoch 31 iteration 500 with loss 0.72728. Total time 2.05444 hours\n",
            "Training at Epoch 31 iteration 600 with loss 1.28589. Total time 2.06194 hours\n",
            "Training at Epoch 31 iteration 700 with loss 0.99350. Total time 2.06916 hours\n",
            "Training at Epoch 31 iteration 800 with loss 0.94510. Total time 2.07666 hours\n",
            "Validation at Epoch 31 , MSE: 1.48283 , Pearson Correlation: 0.23080 with p-value: 1.09177 , Concordance Index: 0.60990\n",
            "Training at Epoch 32 iteration 0 with loss 0.66588. Total time 2.085 hours\n",
            "Training at Epoch 32 iteration 100 with loss 1.03157. Total time 2.0925 hours\n",
            "Training at Epoch 32 iteration 200 with loss 0.81248. Total time 2.09972 hours\n",
            "Training at Epoch 32 iteration 300 with loss 0.93417. Total time 2.10694 hours\n",
            "Training at Epoch 32 iteration 400 with loss 0.99251. Total time 2.11444 hours\n",
            "Training at Epoch 32 iteration 500 with loss 0.98656. Total time 2.12166 hours\n",
            "Training at Epoch 32 iteration 600 with loss 0.47942. Total time 2.12916 hours\n",
            "Training at Epoch 32 iteration 700 with loss 1.01488. Total time 2.13638 hours\n",
            "Training at Epoch 32 iteration 800 with loss 0.78746. Total time 2.14388 hours\n",
            "Validation at Epoch 32 , MSE: 1.49107 , Pearson Correlation: 0.24726 with p-value: 5.51834 , Concordance Index: 0.61298\n",
            "Training at Epoch 33 iteration 0 with loss 0.48913. Total time 2.15222 hours\n",
            "Training at Epoch 33 iteration 100 with loss 1.11572. Total time 2.15944 hours\n",
            "Training at Epoch 33 iteration 200 with loss 0.72576. Total time 2.16694 hours\n",
            "Training at Epoch 33 iteration 300 with loss 0.77240. Total time 2.17416 hours\n",
            "Training at Epoch 33 iteration 400 with loss 1.29776. Total time 2.18166 hours\n",
            "Training at Epoch 33 iteration 500 with loss 1.36983. Total time 2.18888 hours\n",
            "Training at Epoch 33 iteration 600 with loss 1.43699. Total time 2.19611 hours\n",
            "Training at Epoch 33 iteration 700 with loss 1.03629. Total time 2.20361 hours\n",
            "Training at Epoch 33 iteration 800 with loss 0.94173. Total time 2.21083 hours\n",
            "Validation at Epoch 33 , MSE: 1.55158 , Pearson Correlation: 0.22822 with p-value: 1.50014 , Concordance Index: 0.60772\n",
            "Training at Epoch 34 iteration 0 with loss 1.21385. Total time 2.21944 hours\n",
            "Training at Epoch 34 iteration 100 with loss 1.19034. Total time 2.22666 hours\n",
            "Training at Epoch 34 iteration 200 with loss 0.80850. Total time 2.23388 hours\n",
            "Training at Epoch 34 iteration 300 with loss 0.96093. Total time 2.24138 hours\n",
            "Training at Epoch 34 iteration 400 with loss 0.71345. Total time 2.24861 hours\n",
            "Training at Epoch 34 iteration 500 with loss 0.81175. Total time 2.25611 hours\n",
            "Training at Epoch 34 iteration 600 with loss 0.84993. Total time 2.26333 hours\n",
            "Training at Epoch 34 iteration 700 with loss 0.95666. Total time 2.27083 hours\n",
            "Training at Epoch 34 iteration 800 with loss 1.11829. Total time 2.27805 hours\n",
            "Validation at Epoch 34 , MSE: 1.49890 , Pearson Correlation: 0.24451 with p-value: 1.61013 , Concordance Index: 0.61103\n",
            "Training at Epoch 35 iteration 0 with loss 0.80800. Total time 2.28666 hours\n",
            "Training at Epoch 35 iteration 100 with loss 0.71658. Total time 2.29388 hours\n",
            "Training at Epoch 35 iteration 200 with loss 0.57660. Total time 2.30138 hours\n",
            "Training at Epoch 35 iteration 300 with loss 0.59326. Total time 2.30861 hours\n",
            "Training at Epoch 35 iteration 400 with loss 1.05208. Total time 2.31583 hours\n",
            "Training at Epoch 35 iteration 500 with loss 0.83102. Total time 2.32333 hours\n",
            "Training at Epoch 35 iteration 600 with loss 0.73537. Total time 2.33055 hours\n",
            "Training at Epoch 35 iteration 700 with loss 1.23490. Total time 2.33805 hours\n",
            "Training at Epoch 35 iteration 800 with loss 0.51767. Total time 2.34527 hours\n",
            "Validation at Epoch 35 , MSE: 1.65438 , Pearson Correlation: 0.21925 with p-value: 2.60392 , Concordance Index: 0.60273\n",
            "Training at Epoch 36 iteration 0 with loss 0.79687. Total time 2.35388 hours\n",
            "Training at Epoch 36 iteration 100 with loss 0.82048. Total time 2.36111 hours\n",
            "Training at Epoch 36 iteration 200 with loss 0.56827. Total time 2.36861 hours\n",
            "Training at Epoch 36 iteration 300 with loss 0.81513. Total time 2.37583 hours\n",
            "Training at Epoch 36 iteration 400 with loss 0.77449. Total time 2.38305 hours\n",
            "Training at Epoch 36 iteration 500 with loss 0.80955. Total time 2.39055 hours\n",
            "Training at Epoch 36 iteration 600 with loss 0.78620. Total time 2.39777 hours\n",
            "Training at Epoch 36 iteration 700 with loss 0.92074. Total time 2.40527 hours\n",
            "Training at Epoch 36 iteration 800 with loss 1.07452. Total time 2.4125 hours\n",
            "Validation at Epoch 36 , MSE: 1.52303 , Pearson Correlation: 0.24632 with p-value: 3.88461 , Concordance Index: 0.61590\n",
            "Training at Epoch 37 iteration 0 with loss 0.74386. Total time 2.42111 hours\n",
            "Training at Epoch 37 iteration 100 with loss 0.67337. Total time 2.42833 hours\n",
            "Training at Epoch 37 iteration 200 with loss 0.81480. Total time 2.43583 hours\n",
            "Training at Epoch 37 iteration 300 with loss 0.68975. Total time 2.44305 hours\n",
            "Training at Epoch 37 iteration 400 with loss 0.73902. Total time 2.45055 hours\n",
            "Training at Epoch 37 iteration 500 with loss 0.86113. Total time 2.45777 hours\n",
            "Training at Epoch 37 iteration 600 with loss 0.80810. Total time 2.465 hours\n",
            "Training at Epoch 37 iteration 700 with loss 0.40763. Total time 2.4725 hours\n",
            "Training at Epoch 37 iteration 800 with loss 1.27238. Total time 2.47972 hours\n",
            "Validation at Epoch 37 , MSE: 1.54264 , Pearson Correlation: 0.20740 with p-value: 3.09457 , Concordance Index: 0.60072\n",
            "Training at Epoch 38 iteration 0 with loss 0.65818. Total time 2.48833 hours\n",
            "Training at Epoch 38 iteration 100 with loss 0.79540. Total time 2.49555 hours\n",
            "Training at Epoch 38 iteration 200 with loss 0.75639. Total time 2.50305 hours\n",
            "Training at Epoch 38 iteration 300 with loss 1.21303. Total time 2.51027 hours\n",
            "Training at Epoch 38 iteration 400 with loss 0.88777. Total time 2.51777 hours\n",
            "Training at Epoch 38 iteration 500 with loss 1.13096. Total time 2.525 hours\n",
            "Training at Epoch 38 iteration 600 with loss 1.12534. Total time 2.5325 hours\n",
            "Training at Epoch 38 iteration 700 with loss 0.80971. Total time 2.53972 hours\n",
            "Training at Epoch 38 iteration 800 with loss 1.16602. Total time 2.54722 hours\n",
            "Validation at Epoch 38 , MSE: 1.50531 , Pearson Correlation: 0.25156 with p-value: 6.72997 , Concordance Index: 0.61545\n",
            "Training at Epoch 39 iteration 0 with loss 0.66344. Total time 2.55555 hours\n",
            "Training at Epoch 39 iteration 100 with loss 0.57692. Total time 2.56305 hours\n",
            "Training at Epoch 39 iteration 200 with loss 0.75101. Total time 2.57027 hours\n",
            "Training at Epoch 39 iteration 300 with loss 1.72198. Total time 2.57777 hours\n",
            "Training at Epoch 39 iteration 400 with loss 0.79350. Total time 2.585 hours\n",
            "Training at Epoch 39 iteration 500 with loss 0.77805. Total time 2.59222 hours\n",
            "Training at Epoch 39 iteration 600 with loss 0.70976. Total time 2.59972 hours\n",
            "Training at Epoch 39 iteration 700 with loss 0.84929. Total time 2.60694 hours\n",
            "Training at Epoch 39 iteration 800 with loss 1.14104. Total time 2.61444 hours\n",
            "Validation at Epoch 39 , MSE: 1.52264 , Pearson Correlation: 0.23910 with p-value: 9.30744 , Concordance Index: 0.61118\n",
            "Training at Epoch 40 iteration 0 with loss 0.69185. Total time 2.62277 hours\n",
            "Training at Epoch 40 iteration 100 with loss 0.92702. Total time 2.63027 hours\n",
            "Training at Epoch 40 iteration 200 with loss 1.00120. Total time 2.6375 hours\n",
            "Training at Epoch 40 iteration 300 with loss 0.80615. Total time 2.64472 hours\n",
            "Training at Epoch 40 iteration 400 with loss 1.00577. Total time 2.65222 hours\n",
            "Training at Epoch 40 iteration 500 with loss 0.76211. Total time 2.65972 hours\n",
            "Training at Epoch 40 iteration 600 with loss 0.86007. Total time 2.66694 hours\n",
            "Training at Epoch 40 iteration 700 with loss 0.85142. Total time 2.67444 hours\n",
            "Training at Epoch 40 iteration 800 with loss 0.99666. Total time 2.68166 hours\n",
            "Validation at Epoch 40 , MSE: 1.58806 , Pearson Correlation: 0.21793 with p-value: 2.85339 , Concordance Index: 0.60296\n",
            "Training at Epoch 41 iteration 0 with loss 0.89095. Total time 2.69027 hours\n",
            "Training at Epoch 41 iteration 100 with loss 0.66503. Total time 2.69777 hours\n",
            "Training at Epoch 41 iteration 200 with loss 0.71711. Total time 2.705 hours\n",
            "Training at Epoch 41 iteration 300 with loss 1.04008. Total time 2.7125 hours\n",
            "Training at Epoch 41 iteration 400 with loss 0.61496. Total time 2.72 hours\n",
            "Training at Epoch 41 iteration 500 with loss 0.85480. Total time 2.72722 hours\n",
            "Training at Epoch 41 iteration 600 with loss 0.87476. Total time 2.73472 hours\n",
            "Training at Epoch 41 iteration 700 with loss 0.92256. Total time 2.74222 hours\n",
            "Training at Epoch 41 iteration 800 with loss 1.02725. Total time 2.74944 hours\n",
            "Validation at Epoch 41 , MSE: 1.55306 , Pearson Correlation: 0.23301 with p-value: 1.51330 , Concordance Index: 0.60714\n",
            "Training at Epoch 42 iteration 0 with loss 1.21841. Total time 2.75805 hours\n",
            "Training at Epoch 42 iteration 100 with loss 0.55393. Total time 2.76555 hours\n",
            "Training at Epoch 42 iteration 200 with loss 0.76392. Total time 2.77277 hours\n",
            "Training at Epoch 42 iteration 300 with loss 1.02361. Total time 2.78027 hours\n",
            "Training at Epoch 42 iteration 400 with loss 1.20064. Total time 2.78777 hours\n",
            "Training at Epoch 42 iteration 500 with loss 0.99497. Total time 2.79527 hours\n",
            "Training at Epoch 42 iteration 600 with loss 0.71425. Total time 2.8025 hours\n",
            "Training at Epoch 42 iteration 700 with loss 0.86740. Total time 2.81 hours\n",
            "Training at Epoch 42 iteration 800 with loss 1.23893. Total time 2.81722 hours\n",
            "Validation at Epoch 42 , MSE: 1.56229 , Pearson Correlation: 0.22299 with p-value: 2.76233 , Concordance Index: 0.60700\n",
            "Training at Epoch 43 iteration 0 with loss 0.80728. Total time 2.82583 hours\n",
            "Training at Epoch 43 iteration 100 with loss 0.66786. Total time 2.83333 hours\n",
            "Training at Epoch 43 iteration 200 with loss 0.56750. Total time 2.84055 hours\n",
            "Training at Epoch 43 iteration 300 with loss 0.98602. Total time 2.84805 hours\n",
            "Training at Epoch 43 iteration 400 with loss 0.81116. Total time 2.85555 hours\n",
            "Training at Epoch 43 iteration 500 with loss 1.03188. Total time 2.86277 hours\n",
            "Training at Epoch 43 iteration 600 with loss 0.76765. Total time 2.87027 hours\n",
            "Training at Epoch 43 iteration 700 with loss 0.89623. Total time 2.87777 hours\n",
            "Training at Epoch 43 iteration 800 with loss 0.80552. Total time 2.885 hours\n",
            "Validation at Epoch 43 , MSE: 1.59701 , Pearson Correlation: 0.23921 with p-value: 7.39309 , Concordance Index: 0.60730\n",
            "Training at Epoch 44 iteration 0 with loss 0.74856. Total time 2.89361 hours\n",
            "Training at Epoch 44 iteration 100 with loss 0.58756. Total time 2.90111 hours\n",
            "Training at Epoch 44 iteration 200 with loss 0.87881. Total time 2.90833 hours\n",
            "Training at Epoch 44 iteration 300 with loss 0.58505. Total time 2.91583 hours\n",
            "Training at Epoch 44 iteration 400 with loss 0.83923. Total time 2.92333 hours\n",
            "Training at Epoch 44 iteration 500 with loss 0.72289. Total time 2.93083 hours\n",
            "Training at Epoch 44 iteration 600 with loss 0.78560. Total time 2.93805 hours\n",
            "Training at Epoch 44 iteration 700 with loss 1.13624. Total time 2.94555 hours\n",
            "Training at Epoch 44 iteration 800 with loss 1.29441. Total time 2.95305 hours\n",
            "Validation at Epoch 44 , MSE: 1.52210 , Pearson Correlation: 0.23764 with p-value: 1.70396 , Concordance Index: 0.60965\n",
            "Training at Epoch 45 iteration 0 with loss 0.73325. Total time 2.96166 hours\n",
            "Training at Epoch 45 iteration 100 with loss 0.95751. Total time 2.96888 hours\n",
            "Training at Epoch 45 iteration 200 with loss 0.75647. Total time 2.97638 hours\n",
            "Training at Epoch 45 iteration 300 with loss 0.78502. Total time 2.98388 hours\n",
            "Training at Epoch 45 iteration 400 with loss 0.93469. Total time 2.99138 hours\n",
            "Training at Epoch 45 iteration 500 with loss 0.90411. Total time 2.99888 hours\n",
            "Training at Epoch 45 iteration 600 with loss 0.68700. Total time 3.00611 hours\n",
            "Training at Epoch 45 iteration 700 with loss 0.66183. Total time 3.01361 hours\n",
            "Training at Epoch 45 iteration 800 with loss 0.83888. Total time 3.02111 hours\n",
            "Validation at Epoch 45 , MSE: 1.56650 , Pearson Correlation: 0.23510 with p-value: 2.56286 , Concordance Index: 0.60583\n",
            "Training at Epoch 46 iteration 0 with loss 0.57791. Total time 3.02972 hours\n",
            "Training at Epoch 46 iteration 100 with loss 0.72897. Total time 3.03722 hours\n",
            "Training at Epoch 46 iteration 200 with loss 0.54906. Total time 3.04472 hours\n",
            "Training at Epoch 46 iteration 300 with loss 0.75360. Total time 3.05222 hours\n",
            "Training at Epoch 46 iteration 400 with loss 1.25594. Total time 3.05944 hours\n",
            "Training at Epoch 46 iteration 500 with loss 0.81223. Total time 3.06694 hours\n",
            "Training at Epoch 46 iteration 600 with loss 0.56658. Total time 3.07444 hours\n",
            "Training at Epoch 46 iteration 700 with loss 1.12332. Total time 3.08194 hours\n",
            "Training at Epoch 46 iteration 800 with loss 0.68545. Total time 3.08944 hours\n",
            "Validation at Epoch 46 , MSE: 1.52464 , Pearson Correlation: 0.21526 with p-value: 3.43407 , Concordance Index: 0.59988\n",
            "Training at Epoch 47 iteration 0 with loss 0.87270. Total time 3.09805 hours\n",
            "Training at Epoch 47 iteration 100 with loss 0.90978. Total time 3.10555 hours\n",
            "Training at Epoch 47 iteration 200 with loss 0.63666. Total time 3.11305 hours\n",
            "Training at Epoch 47 iteration 300 with loss 0.67756. Total time 3.12055 hours\n",
            "Training at Epoch 47 iteration 400 with loss 0.77201. Total time 3.12777 hours\n",
            "Training at Epoch 47 iteration 500 with loss 1.02733. Total time 3.13527 hours\n",
            "Training at Epoch 47 iteration 600 with loss 0.72703. Total time 3.14277 hours\n",
            "Training at Epoch 47 iteration 700 with loss 0.73363. Total time 3.15027 hours\n",
            "Training at Epoch 47 iteration 800 with loss 1.17400. Total time 3.15777 hours\n",
            "Validation at Epoch 47 , MSE: 1.60028 , Pearson Correlation: 0.22614 with p-value: 7.70996 , Concordance Index: 0.60578\n",
            "Training at Epoch 48 iteration 0 with loss 0.77058. Total time 3.16638 hours\n",
            "Training at Epoch 48 iteration 100 with loss 0.38768. Total time 3.17388 hours\n",
            "Training at Epoch 48 iteration 200 with loss 1.11191. Total time 3.18138 hours\n",
            "Training at Epoch 48 iteration 300 with loss 0.91794. Total time 3.18888 hours\n",
            "Training at Epoch 48 iteration 400 with loss 0.71605. Total time 3.19611 hours\n",
            "Training at Epoch 48 iteration 500 with loss 0.85764. Total time 3.20361 hours\n",
            "Training at Epoch 48 iteration 600 with loss 0.79489. Total time 3.21111 hours\n",
            "Training at Epoch 48 iteration 700 with loss 0.76525. Total time 3.21861 hours\n",
            "Training at Epoch 48 iteration 800 with loss 0.41663. Total time 3.22611 hours\n",
            "Validation at Epoch 48 , MSE: 1.56396 , Pearson Correlation: 0.20328 with p-value: 3.33532 , Concordance Index: 0.59300\n",
            "Training at Epoch 49 iteration 0 with loss 0.73565. Total time 3.23472 hours\n",
            "Training at Epoch 49 iteration 100 with loss 0.92599. Total time 3.24222 hours\n",
            "Training at Epoch 49 iteration 200 with loss 0.71758. Total time 3.24972 hours\n",
            "Training at Epoch 49 iteration 300 with loss 0.74626. Total time 3.25722 hours\n",
            "Training at Epoch 49 iteration 400 with loss 0.78969. Total time 3.26472 hours\n",
            "Training at Epoch 49 iteration 500 with loss 1.00690. Total time 3.27222 hours\n",
            "Training at Epoch 49 iteration 600 with loss 0.71887. Total time 3.27972 hours\n",
            "Training at Epoch 49 iteration 700 with loss 0.85274. Total time 3.28722 hours\n",
            "Training at Epoch 49 iteration 800 with loss 0.91190. Total time 3.29444 hours\n",
            "Validation at Epoch 49 , MSE: 1.55022 , Pearson Correlation: 0.23741 with p-value: 2.70320 , Concordance Index: 0.61052\n",
            "Training at Epoch 50 iteration 0 with loss 0.67002. Total time 3.30333 hours\n",
            "Training at Epoch 50 iteration 100 with loss 0.57774. Total time 3.31083 hours\n",
            "Training at Epoch 50 iteration 200 with loss 0.72682. Total time 3.31805 hours\n",
            "Training at Epoch 50 iteration 300 with loss 0.79328. Total time 3.32555 hours\n",
            "Training at Epoch 50 iteration 400 with loss 0.66488. Total time 3.33305 hours\n",
            "Training at Epoch 50 iteration 500 with loss 0.58206. Total time 3.34055 hours\n",
            "Training at Epoch 50 iteration 600 with loss 0.41666. Total time 3.34805 hours\n",
            "Training at Epoch 50 iteration 700 with loss 0.66243. Total time 3.35555 hours\n",
            "Training at Epoch 50 iteration 800 with loss 0.91969. Total time 3.36305 hours\n",
            "Validation at Epoch 50 , MSE: 1.58497 , Pearson Correlation: 0.22713 with p-value: 1.18797 , Concordance Index: 0.61030\n",
            "Training at Epoch 51 iteration 0 with loss 0.57807. Total time 3.37166 hours\n",
            "Training at Epoch 51 iteration 100 with loss 0.45924. Total time 3.37916 hours\n",
            "Training at Epoch 51 iteration 200 with loss 1.05220. Total time 3.38666 hours\n",
            "Training at Epoch 51 iteration 300 with loss 0.46376. Total time 3.39416 hours\n",
            "Training at Epoch 51 iteration 400 with loss 0.98789. Total time 3.40138 hours\n",
            "Training at Epoch 51 iteration 500 with loss 0.60629. Total time 3.40888 hours\n",
            "Training at Epoch 51 iteration 600 with loss 0.79772. Total time 3.41638 hours\n",
            "Training at Epoch 51 iteration 700 with loss 0.73506. Total time 3.42388 hours\n",
            "Training at Epoch 51 iteration 800 with loss 0.83622. Total time 3.43138 hours\n",
            "Validation at Epoch 51 , MSE: 1.56859 , Pearson Correlation: 0.23366 with p-value: 4.27296 , Concordance Index: 0.61086\n",
            "Training at Epoch 52 iteration 0 with loss 0.69054. Total time 3.44 hours\n",
            "Training at Epoch 52 iteration 100 with loss 0.60117. Total time 3.4475 hours\n",
            "Training at Epoch 52 iteration 200 with loss 0.49420. Total time 3.455 hours\n",
            "Training at Epoch 52 iteration 300 with loss 0.51283. Total time 3.4625 hours\n",
            "Training at Epoch 52 iteration 400 with loss 0.77010. Total time 3.47 hours\n",
            "Training at Epoch 52 iteration 500 with loss 0.89572. Total time 3.4775 hours\n",
            "Training at Epoch 52 iteration 600 with loss 0.81003. Total time 3.485 hours\n",
            "Training at Epoch 52 iteration 700 with loss 0.91200. Total time 3.4925 hours\n",
            "Training at Epoch 52 iteration 800 with loss 0.74100. Total time 3.5 hours\n",
            "Validation at Epoch 52 , MSE: 1.55366 , Pearson Correlation: 0.23179 with p-value: 1.61982 , Concordance Index: 0.61110\n",
            "Training at Epoch 53 iteration 0 with loss 0.34573. Total time 3.50861 hours\n",
            "Training at Epoch 53 iteration 100 with loss 0.54551. Total time 3.51611 hours\n",
            "Training at Epoch 53 iteration 200 with loss 0.80985. Total time 3.52361 hours\n",
            "Training at Epoch 53 iteration 300 with loss 0.50114. Total time 3.53111 hours\n",
            "Training at Epoch 53 iteration 400 with loss 0.82612. Total time 3.53833 hours\n",
            "Training at Epoch 53 iteration 500 with loss 0.58425. Total time 3.54583 hours\n",
            "Training at Epoch 53 iteration 600 with loss 0.47497. Total time 3.55333 hours\n",
            "Training at Epoch 53 iteration 700 with loss 0.59605. Total time 3.56083 hours\n",
            "Training at Epoch 53 iteration 800 with loss 0.54216. Total time 3.56833 hours\n",
            "Validation at Epoch 53 , MSE: 1.59722 , Pearson Correlation: 0.23112 with p-value: 5.87780 , Concordance Index: 0.60943\n",
            "Training at Epoch 54 iteration 0 with loss 0.77114. Total time 3.57722 hours\n",
            "Training at Epoch 54 iteration 100 with loss 0.54706. Total time 3.58444 hours\n",
            "Training at Epoch 54 iteration 200 with loss 0.56664. Total time 3.59194 hours\n",
            "Training at Epoch 54 iteration 300 with loss 0.81231. Total time 3.59944 hours\n",
            "Training at Epoch 54 iteration 400 with loss 0.86592. Total time 3.60694 hours\n",
            "Training at Epoch 54 iteration 500 with loss 0.54377. Total time 3.61444 hours\n",
            "Training at Epoch 54 iteration 600 with loss 0.86210. Total time 3.62194 hours\n",
            "Training at Epoch 54 iteration 700 with loss 0.67549. Total time 3.62944 hours\n",
            "Training at Epoch 54 iteration 800 with loss 0.78352. Total time 3.63694 hours\n",
            "Validation at Epoch 54 , MSE: 1.59654 , Pearson Correlation: 0.21594 with p-value: 1.02441 , Concordance Index: 0.60430\n",
            "Training at Epoch 55 iteration 0 with loss 0.39943. Total time 3.64583 hours\n",
            "Training at Epoch 55 iteration 100 with loss 1.01606. Total time 3.65333 hours\n",
            "Training at Epoch 55 iteration 200 with loss 1.00469. Total time 3.66083 hours\n",
            "Training at Epoch 55 iteration 300 with loss 0.85242. Total time 3.66833 hours\n",
            "Training at Epoch 55 iteration 400 with loss 0.54382. Total time 3.67583 hours\n",
            "Training at Epoch 55 iteration 500 with loss 0.33665. Total time 3.68333 hours\n",
            "Training at Epoch 55 iteration 600 with loss 0.50195. Total time 3.69055 hours\n",
            "Training at Epoch 55 iteration 700 with loss 0.70396. Total time 3.69833 hours\n",
            "Training at Epoch 55 iteration 800 with loss 0.82329. Total time 3.70583 hours\n",
            "Validation at Epoch 55 , MSE: 1.55975 , Pearson Correlation: 0.23876 with p-value: 1.81605 , Concordance Index: 0.61215\n",
            "Training at Epoch 56 iteration 0 with loss 0.74844. Total time 3.71444 hours\n",
            "Training at Epoch 56 iteration 100 with loss 0.72229. Total time 3.72194 hours\n",
            "Training at Epoch 56 iteration 200 with loss 0.65535. Total time 3.72944 hours\n",
            "Training at Epoch 56 iteration 300 with loss 0.71251. Total time 3.73666 hours\n",
            "Training at Epoch 56 iteration 400 with loss 0.64921. Total time 3.74416 hours\n",
            "Training at Epoch 56 iteration 500 with loss 0.55320. Total time 3.75166 hours\n",
            "Training at Epoch 56 iteration 600 with loss 0.37695. Total time 3.75916 hours\n",
            "Training at Epoch 56 iteration 700 with loss 0.63221. Total time 3.76666 hours\n",
            "Training at Epoch 56 iteration 800 with loss 0.61639. Total time 3.77416 hours\n",
            "Validation at Epoch 56 , MSE: 1.64471 , Pearson Correlation: 0.21427 with p-value: 1.98270 , Concordance Index: 0.60741\n",
            "Training at Epoch 57 iteration 0 with loss 0.74645. Total time 3.78305 hours\n",
            "Training at Epoch 57 iteration 100 with loss 0.50894. Total time 3.79055 hours\n",
            "Training at Epoch 57 iteration 200 with loss 0.43166. Total time 3.79805 hours\n",
            "Training at Epoch 57 iteration 300 with loss 0.75100. Total time 3.80527 hours\n",
            "Training at Epoch 57 iteration 400 with loss 0.94980. Total time 3.81277 hours\n",
            "Training at Epoch 57 iteration 500 with loss 0.45842. Total time 3.82027 hours\n",
            "Training at Epoch 57 iteration 600 with loss 0.97022. Total time 3.82777 hours\n",
            "Training at Epoch 57 iteration 700 with loss 0.97022. Total time 3.83527 hours\n",
            "Training at Epoch 57 iteration 800 with loss 0.48830. Total time 3.84277 hours\n",
            "Validation at Epoch 57 , MSE: 1.60644 , Pearson Correlation: 0.22077 with p-value: 1.63646 , Concordance Index: 0.60802\n",
            "Training at Epoch 58 iteration 0 with loss 1.00238. Total time 3.85138 hours\n",
            "Training at Epoch 58 iteration 100 with loss 0.49988. Total time 3.85888 hours\n",
            "Training at Epoch 58 iteration 200 with loss 0.67438. Total time 3.86638 hours\n",
            "Training at Epoch 58 iteration 300 with loss 0.70505. Total time 3.87388 hours\n",
            "Training at Epoch 58 iteration 400 with loss 0.67204. Total time 3.88138 hours\n",
            "Training at Epoch 58 iteration 500 with loss 0.68080. Total time 3.88861 hours\n",
            "Training at Epoch 58 iteration 600 with loss 0.60646. Total time 3.89611 hours\n",
            "Training at Epoch 58 iteration 700 with loss 0.52906. Total time 3.90361 hours\n",
            "Training at Epoch 58 iteration 800 with loss 0.52149. Total time 3.91111 hours\n",
            "Validation at Epoch 58 , MSE: 1.59914 , Pearson Correlation: 0.22691 with p-value: 1.80682 , Concordance Index: 0.60747\n",
            "Training at Epoch 59 iteration 0 with loss 0.56959. Total time 3.91972 hours\n",
            "Training at Epoch 59 iteration 100 with loss 0.53163. Total time 3.92722 hours\n",
            "Training at Epoch 59 iteration 200 with loss 0.53257. Total time 3.93444 hours\n",
            "Training at Epoch 59 iteration 300 with loss 0.87560. Total time 3.94194 hours\n",
            "Training at Epoch 59 iteration 400 with loss 0.49841. Total time 3.94944 hours\n",
            "Training at Epoch 59 iteration 500 with loss 0.77067. Total time 3.95694 hours\n",
            "Training at Epoch 59 iteration 600 with loss 0.81386. Total time 3.96444 hours\n",
            "Training at Epoch 59 iteration 700 with loss 0.61919. Total time 3.97166 hours\n",
            "Training at Epoch 59 iteration 800 with loss 0.50687. Total time 3.97916 hours\n",
            "Validation at Epoch 59 , MSE: 1.60679 , Pearson Correlation: 0.20285 with p-value: 6.80323 , Concordance Index: 0.59813\n",
            "Training at Epoch 60 iteration 0 with loss 0.48792. Total time 3.98805 hours\n",
            "Training at Epoch 60 iteration 100 with loss 0.61028. Total time 3.99555 hours\n",
            "Training at Epoch 60 iteration 200 with loss 0.53699. Total time 4.00277 hours\n",
            "Training at Epoch 60 iteration 300 with loss 0.55459. Total time 4.01027 hours\n",
            "Training at Epoch 60 iteration 400 with loss 0.45328. Total time 4.01777 hours\n",
            "Training at Epoch 60 iteration 500 with loss 0.46506. Total time 4.02527 hours\n",
            "Training at Epoch 60 iteration 600 with loss 0.67314. Total time 4.03277 hours\n",
            "Training at Epoch 60 iteration 700 with loss 0.72196. Total time 4.04027 hours\n",
            "Training at Epoch 60 iteration 800 with loss 0.62815. Total time 4.04777 hours\n",
            "Validation at Epoch 60 , MSE: 1.58314 , Pearson Correlation: 0.22449 with p-value: 1.69882 , Concordance Index: 0.60798\n",
            "Training at Epoch 61 iteration 0 with loss 1.12574. Total time 4.05638 hours\n",
            "Training at Epoch 61 iteration 100 with loss 0.56236. Total time 4.06388 hours\n",
            "Training at Epoch 61 iteration 200 with loss 0.62204. Total time 4.07138 hours\n",
            "Training at Epoch 61 iteration 300 with loss 0.94485. Total time 4.07888 hours\n",
            "Training at Epoch 61 iteration 400 with loss 0.46890. Total time 4.08638 hours\n",
            "Training at Epoch 61 iteration 500 with loss 0.40531. Total time 4.09388 hours\n",
            "Training at Epoch 61 iteration 600 with loss 0.63207. Total time 4.10111 hours\n",
            "Training at Epoch 61 iteration 700 with loss 0.73457. Total time 4.10861 hours\n",
            "Training at Epoch 61 iteration 800 with loss 0.79665. Total time 4.11611 hours\n",
            "Validation at Epoch 61 , MSE: 1.70404 , Pearson Correlation: 0.23007 with p-value: 4.44695 , Concordance Index: 0.61020\n",
            "Training at Epoch 62 iteration 0 with loss 0.50378. Total time 4.12472 hours\n",
            "Training at Epoch 62 iteration 100 with loss 0.43351. Total time 4.13222 hours\n",
            "Training at Epoch 62 iteration 200 with loss 0.58923. Total time 4.13972 hours\n",
            "Training at Epoch 62 iteration 300 with loss 0.66283. Total time 4.14722 hours\n",
            "Training at Epoch 62 iteration 400 with loss 0.45796. Total time 4.15472 hours\n",
            "Training at Epoch 62 iteration 500 with loss 0.65557. Total time 4.16222 hours\n",
            "Training at Epoch 62 iteration 600 with loss 0.60654. Total time 4.16972 hours\n",
            "Training at Epoch 62 iteration 700 with loss 0.91883. Total time 4.17722 hours\n",
            "Training at Epoch 62 iteration 800 with loss 0.86593. Total time 4.18472 hours\n",
            "Validation at Epoch 62 , MSE: 1.62537 , Pearson Correlation: 0.21317 with p-value: 1.39380 , Concordance Index: 0.60243\n",
            "Training at Epoch 63 iteration 0 with loss 0.55293. Total time 4.19361 hours\n",
            "Training at Epoch 63 iteration 100 with loss 0.50431. Total time 4.20111 hours\n",
            "Training at Epoch 63 iteration 200 with loss 0.66534. Total time 4.20861 hours\n",
            "Training at Epoch 63 iteration 300 with loss 0.46995. Total time 4.21611 hours\n",
            "Training at Epoch 63 iteration 400 with loss 0.54852. Total time 4.22361 hours\n",
            "Training at Epoch 63 iteration 500 with loss 0.68365. Total time 4.23111 hours\n",
            "Training at Epoch 63 iteration 600 with loss 0.89989. Total time 4.23833 hours\n",
            "Training at Epoch 63 iteration 700 with loss 0.44566. Total time 4.24583 hours\n",
            "Training at Epoch 63 iteration 800 with loss 0.49825. Total time 4.25333 hours\n",
            "Validation at Epoch 63 , MSE: 1.84680 , Pearson Correlation: 0.21564 with p-value: 1.72813 , Concordance Index: 0.60709\n",
            "Training at Epoch 64 iteration 0 with loss 0.67279. Total time 4.26222 hours\n",
            "Training at Epoch 64 iteration 100 with loss 0.83716. Total time 4.26944 hours\n",
            "Training at Epoch 64 iteration 200 with loss 0.83461. Total time 4.27694 hours\n",
            "Training at Epoch 64 iteration 300 with loss 0.63628. Total time 4.28444 hours\n",
            "Training at Epoch 64 iteration 400 with loss 0.54122. Total time 4.29194 hours\n",
            "Training at Epoch 64 iteration 500 with loss 0.44645. Total time 4.29944 hours\n",
            "Training at Epoch 64 iteration 600 with loss 0.51467. Total time 4.30694 hours\n",
            "Training at Epoch 64 iteration 700 with loss 0.80594. Total time 4.31444 hours\n",
            "Training at Epoch 64 iteration 800 with loss 0.47856. Total time 4.32194 hours\n",
            "Validation at Epoch 64 , MSE: 1.68957 , Pearson Correlation: 0.23143 with p-value: 3.25050 , Concordance Index: 0.61221\n",
            "Training at Epoch 65 iteration 0 with loss 0.39362. Total time 4.33055 hours\n",
            "Training at Epoch 65 iteration 100 with loss 0.53797. Total time 4.33805 hours\n",
            "Training at Epoch 65 iteration 200 with loss 0.50812. Total time 4.34555 hours\n",
            "Training at Epoch 65 iteration 300 with loss 0.70749. Total time 4.35305 hours\n",
            "Training at Epoch 65 iteration 400 with loss 0.51532. Total time 4.36055 hours\n",
            "Training at Epoch 65 iteration 500 with loss 0.59772. Total time 4.36805 hours\n",
            "Training at Epoch 65 iteration 600 with loss 0.67611. Total time 4.37555 hours\n",
            "Training at Epoch 65 iteration 700 with loss 0.54583. Total time 4.38305 hours\n",
            "Training at Epoch 65 iteration 800 with loss 0.36020. Total time 4.39055 hours\n",
            "Validation at Epoch 65 , MSE: 1.62164 , Pearson Correlation: 0.22430 with p-value: 2.38659 , Concordance Index: 0.60645\n",
            "Training at Epoch 66 iteration 0 with loss 0.51530. Total time 4.39916 hours\n",
            "Training at Epoch 66 iteration 100 with loss 0.58838. Total time 4.40666 hours\n",
            "Training at Epoch 66 iteration 200 with loss 0.32089. Total time 4.41416 hours\n",
            "Training at Epoch 66 iteration 300 with loss 0.58841. Total time 4.42166 hours\n",
            "Training at Epoch 66 iteration 400 with loss 0.68190. Total time 4.42916 hours\n",
            "Training at Epoch 66 iteration 500 with loss 0.35824. Total time 4.43666 hours\n",
            "Training at Epoch 66 iteration 600 with loss 0.44025. Total time 4.44416 hours\n",
            "Training at Epoch 66 iteration 700 with loss 0.39289. Total time 4.45166 hours\n",
            "Training at Epoch 66 iteration 800 with loss 0.61614. Total time 4.45916 hours\n",
            "Validation at Epoch 66 , MSE: 1.68130 , Pearson Correlation: 0.22372 with p-value: 7.09241 , Concordance Index: 0.60968\n",
            "Training at Epoch 67 iteration 0 with loss 0.62872. Total time 4.46805 hours\n",
            "Training at Epoch 67 iteration 100 with loss 0.51096. Total time 4.47555 hours\n",
            "Training at Epoch 67 iteration 200 with loss 0.50094. Total time 4.48305 hours\n",
            "Training at Epoch 67 iteration 300 with loss 0.64659. Total time 4.49055 hours\n",
            "Training at Epoch 67 iteration 400 with loss 0.82672. Total time 4.49805 hours\n",
            "Training at Epoch 67 iteration 500 with loss 0.46692. Total time 4.50527 hours\n",
            "Training at Epoch 67 iteration 600 with loss 0.42311. Total time 4.51277 hours\n",
            "Training at Epoch 67 iteration 700 with loss 0.41468. Total time 4.52027 hours\n",
            "Training at Epoch 67 iteration 800 with loss 0.68145. Total time 4.52777 hours\n",
            "Validation at Epoch 67 , MSE: 1.66538 , Pearson Correlation: 0.22062 with p-value: 2.15808 , Concordance Index: 0.60552\n",
            "Training at Epoch 68 iteration 0 with loss 0.70204. Total time 4.53666 hours\n",
            "Training at Epoch 68 iteration 100 with loss 0.88824. Total time 4.54388 hours\n",
            "Training at Epoch 68 iteration 200 with loss 0.84810. Total time 4.55138 hours\n",
            "Training at Epoch 68 iteration 300 with loss 0.46143. Total time 4.55916 hours\n",
            "Training at Epoch 68 iteration 400 with loss 0.53974. Total time 4.56666 hours\n",
            "Training at Epoch 68 iteration 500 with loss 0.74550. Total time 4.57416 hours\n",
            "Training at Epoch 68 iteration 600 with loss 0.43725. Total time 4.58166 hours\n",
            "Training at Epoch 68 iteration 700 with loss 0.60987. Total time 4.58916 hours\n",
            "Training at Epoch 68 iteration 800 with loss 0.62222. Total time 4.59666 hours\n",
            "Validation at Epoch 68 , MSE: 1.64430 , Pearson Correlation: 0.21305 with p-value: 1.71607 , Concordance Index: 0.60427\n",
            "Training at Epoch 69 iteration 0 with loss 0.66382. Total time 4.60527 hours\n",
            "Training at Epoch 69 iteration 100 with loss 0.55092. Total time 4.61277 hours\n",
            "Training at Epoch 69 iteration 200 with loss 0.36573. Total time 4.62027 hours\n",
            "Training at Epoch 69 iteration 300 with loss 0.75497. Total time 4.6275 hours\n",
            "Training at Epoch 69 iteration 400 with loss 0.73939. Total time 4.635 hours\n",
            "Training at Epoch 69 iteration 500 with loss 0.37958. Total time 4.6425 hours\n",
            "Training at Epoch 69 iteration 600 with loss 0.47277. Total time 4.65 hours\n",
            "Training at Epoch 69 iteration 700 with loss 0.47458. Total time 4.65722 hours\n",
            "Training at Epoch 69 iteration 800 with loss 0.87148. Total time 4.66472 hours\n",
            "Validation at Epoch 69 , MSE: 1.66378 , Pearson Correlation: 0.22116 with p-value: 7.96492 , Concordance Index: 0.60560\n",
            "Training at Epoch 70 iteration 0 with loss 0.55000. Total time 4.67333 hours\n",
            "Training at Epoch 70 iteration 100 with loss 0.59318. Total time 4.68055 hours\n",
            "Training at Epoch 70 iteration 200 with loss 0.57589. Total time 4.68805 hours\n",
            "Training at Epoch 70 iteration 300 with loss 0.76787. Total time 4.69555 hours\n",
            "Training at Epoch 70 iteration 400 with loss 0.43407. Total time 4.70277 hours\n",
            "Training at Epoch 70 iteration 500 with loss 0.28555. Total time 4.71027 hours\n",
            "Training at Epoch 70 iteration 600 with loss 0.79759. Total time 4.7175 hours\n",
            "Training at Epoch 70 iteration 700 with loss 0.64702. Total time 4.725 hours\n",
            "Training at Epoch 70 iteration 800 with loss 0.52952. Total time 4.7325 hours\n",
            "Validation at Epoch 70 , MSE: 1.61025 , Pearson Correlation: 0.21937 with p-value: 2.10421 , Concordance Index: 0.60083\n",
            "Training at Epoch 71 iteration 0 with loss 0.45343. Total time 4.74111 hours\n",
            "Training at Epoch 71 iteration 100 with loss 0.54927. Total time 4.74833 hours\n",
            "Training at Epoch 71 iteration 200 with loss 0.44147. Total time 4.75583 hours\n",
            "Training at Epoch 71 iteration 300 with loss 0.36620. Total time 4.76333 hours\n",
            "Training at Epoch 71 iteration 400 with loss 0.36550. Total time 4.77055 hours\n",
            "Training at Epoch 71 iteration 500 with loss 0.38063. Total time 4.77805 hours\n",
            "Training at Epoch 71 iteration 600 with loss 0.47780. Total time 4.78555 hours\n",
            "Training at Epoch 71 iteration 700 with loss 0.77491. Total time 4.79277 hours\n",
            "Training at Epoch 71 iteration 800 with loss 0.55878. Total time 4.80027 hours\n",
            "Validation at Epoch 71 , MSE: 1.62542 , Pearson Correlation: 0.21050 with p-value: 1.49326 , Concordance Index: 0.59657\n",
            "Training at Epoch 72 iteration 0 with loss 0.64099. Total time 4.80888 hours\n",
            "Training at Epoch 72 iteration 100 with loss 0.47116. Total time 4.81611 hours\n",
            "Training at Epoch 72 iteration 200 with loss 0.58043. Total time 4.82361 hours\n",
            "Training at Epoch 72 iteration 300 with loss 0.35490. Total time 4.83111 hours\n",
            "Training at Epoch 72 iteration 400 with loss 0.41362. Total time 4.83833 hours\n",
            "Training at Epoch 72 iteration 500 with loss 0.54681. Total time 4.84583 hours\n",
            "Training at Epoch 72 iteration 600 with loss 0.59157. Total time 4.85333 hours\n",
            "Training at Epoch 72 iteration 700 with loss 0.32439. Total time 4.86055 hours\n",
            "Training at Epoch 72 iteration 800 with loss 0.40649. Total time 4.86805 hours\n",
            "Validation at Epoch 72 , MSE: 1.67747 , Pearson Correlation: 0.21888 with p-value: 5.14659 , Concordance Index: 0.60152\n",
            "Training at Epoch 73 iteration 0 with loss 0.28683. Total time 4.87666 hours\n",
            "Training at Epoch 73 iteration 100 with loss 0.42020. Total time 4.88416 hours\n",
            "Training at Epoch 73 iteration 200 with loss 0.42921. Total time 4.89138 hours\n",
            "Training at Epoch 73 iteration 300 with loss 0.66588. Total time 4.89888 hours\n",
            "Training at Epoch 73 iteration 400 with loss 0.34491. Total time 4.90638 hours\n",
            "Training at Epoch 73 iteration 500 with loss 0.46505. Total time 4.91361 hours\n",
            "Training at Epoch 73 iteration 600 with loss 0.63050. Total time 4.92111 hours\n",
            "Training at Epoch 73 iteration 700 with loss 0.62259. Total time 4.92833 hours\n",
            "Training at Epoch 73 iteration 800 with loss 0.60115. Total time 4.93583 hours\n",
            "Validation at Epoch 73 , MSE: 1.65749 , Pearson Correlation: 0.22521 with p-value: 4.41439 , Concordance Index: 0.60550\n",
            "Training at Epoch 74 iteration 0 with loss 0.47014. Total time 4.94444 hours\n",
            "Training at Epoch 74 iteration 100 with loss 0.81250. Total time 4.95166 hours\n",
            "Training at Epoch 74 iteration 200 with loss 0.58867. Total time 4.95916 hours\n",
            "Training at Epoch 74 iteration 300 with loss 0.38196. Total time 4.96666 hours\n",
            "Training at Epoch 74 iteration 400 with loss 0.46254. Total time 4.97388 hours\n",
            "Training at Epoch 74 iteration 500 with loss 0.57721. Total time 4.98138 hours\n",
            "Training at Epoch 74 iteration 600 with loss 0.52390. Total time 4.98888 hours\n",
            "Training at Epoch 74 iteration 700 with loss 0.61921. Total time 4.99638 hours\n",
            "Training at Epoch 74 iteration 800 with loss 0.42741. Total time 5.00361 hours\n",
            "Validation at Epoch 74 , MSE: 1.67591 , Pearson Correlation: 0.21354 with p-value: 7.30667 , Concordance Index: 0.59902\n",
            "Training at Epoch 75 iteration 0 with loss 0.59969. Total time 5.01222 hours\n",
            "Training at Epoch 75 iteration 100 with loss 0.39949. Total time 5.01972 hours\n",
            "Training at Epoch 75 iteration 200 with loss 0.95031. Total time 5.02694 hours\n",
            "Training at Epoch 75 iteration 300 with loss 0.32927. Total time 5.03444 hours\n",
            "Training at Epoch 75 iteration 400 with loss 0.47940. Total time 5.04194 hours\n",
            "Training at Epoch 75 iteration 500 with loss 0.40163. Total time 5.04916 hours\n",
            "Training at Epoch 75 iteration 600 with loss 0.27195. Total time 5.05666 hours\n",
            "Training at Epoch 75 iteration 700 with loss 0.31375. Total time 5.06416 hours\n",
            "Training at Epoch 75 iteration 800 with loss 0.64561. Total time 5.07138 hours\n",
            "Validation at Epoch 75 , MSE: 1.62227 , Pearson Correlation: 0.23492 with p-value: 3.66359 , Concordance Index: 0.60387\n",
            "Training at Epoch 76 iteration 0 with loss 0.56258. Total time 5.08 hours\n",
            "Training at Epoch 76 iteration 100 with loss 0.34132. Total time 5.0875 hours\n",
            "Training at Epoch 76 iteration 200 with loss 0.57068. Total time 5.09472 hours\n",
            "Training at Epoch 76 iteration 300 with loss 0.59788. Total time 5.10222 hours\n",
            "Training at Epoch 76 iteration 400 with loss 0.49988. Total time 5.10972 hours\n",
            "Training at Epoch 76 iteration 500 with loss 0.72862. Total time 5.11694 hours\n",
            "Training at Epoch 76 iteration 600 with loss 0.63872. Total time 5.12444 hours\n",
            "Training at Epoch 76 iteration 700 with loss 0.49171. Total time 5.13194 hours\n",
            "Training at Epoch 76 iteration 800 with loss 0.63663. Total time 5.13944 hours\n",
            "Validation at Epoch 76 , MSE: 1.61756 , Pearson Correlation: 0.21831 with p-value: 1.42696 , Concordance Index: 0.59966\n",
            "Training at Epoch 77 iteration 0 with loss 0.35371. Total time 5.14777 hours\n",
            "Training at Epoch 77 iteration 100 with loss 0.51270. Total time 5.15527 hours\n",
            "Training at Epoch 77 iteration 200 with loss 0.42944. Total time 5.16277 hours\n",
            "Training at Epoch 77 iteration 300 with loss 0.50705. Total time 5.17027 hours\n",
            "Training at Epoch 77 iteration 400 with loss 0.57269. Total time 5.1775 hours\n",
            "Training at Epoch 77 iteration 500 with loss 0.57174. Total time 5.185 hours\n",
            "Training at Epoch 77 iteration 600 with loss 0.42687. Total time 5.19222 hours\n",
            "Training at Epoch 77 iteration 700 with loss 0.39655. Total time 5.19972 hours\n",
            "Training at Epoch 77 iteration 800 with loss 0.67261. Total time 5.20722 hours\n",
            "Validation at Epoch 77 , MSE: 1.64787 , Pearson Correlation: 0.21107 with p-value: 5.52127 , Concordance Index: 0.60026\n",
            "Training at Epoch 78 iteration 0 with loss 0.48686. Total time 5.21555 hours\n",
            "Training at Epoch 78 iteration 100 with loss 0.50858. Total time 5.22305 hours\n",
            "Training at Epoch 78 iteration 200 with loss 0.53489. Total time 5.23055 hours\n",
            "Training at Epoch 78 iteration 300 with loss 0.29718. Total time 5.23777 hours\n",
            "Training at Epoch 78 iteration 400 with loss 0.48241. Total time 5.24527 hours\n",
            "Training at Epoch 78 iteration 500 with loss 0.50761. Total time 5.25277 hours\n",
            "Training at Epoch 78 iteration 600 with loss 0.35136. Total time 5.26 hours\n",
            "Training at Epoch 78 iteration 700 with loss 0.32336. Total time 5.2675 hours\n",
            "Training at Epoch 78 iteration 800 with loss 0.53391. Total time 5.275 hours\n",
            "Validation at Epoch 78 , MSE: 1.71369 , Pearson Correlation: 0.22573 with p-value: 1.65042 , Concordance Index: 0.60777\n",
            "Training at Epoch 79 iteration 0 with loss 0.54395. Total time 5.28361 hours\n",
            "Training at Epoch 79 iteration 100 with loss 0.33293. Total time 5.29111 hours\n",
            "Training at Epoch 79 iteration 200 with loss 0.26868. Total time 5.29833 hours\n",
            "Training at Epoch 79 iteration 300 with loss 0.60643. Total time 5.30583 hours\n",
            "Training at Epoch 79 iteration 400 with loss 0.30932. Total time 5.31305 hours\n",
            "Training at Epoch 79 iteration 500 with loss 0.48410. Total time 5.32055 hours\n",
            "Training at Epoch 79 iteration 600 with loss 0.36190. Total time 5.32805 hours\n",
            "Training at Epoch 79 iteration 700 with loss 0.46033. Total time 5.33527 hours\n",
            "Training at Epoch 79 iteration 800 with loss 0.34388. Total time 5.34277 hours\n",
            "Validation at Epoch 79 , MSE: 1.63881 , Pearson Correlation: 0.23523 with p-value: 2.00396 , Concordance Index: 0.61211\n",
            "Training at Epoch 80 iteration 0 with loss 0.56445. Total time 5.35138 hours\n",
            "Training at Epoch 80 iteration 100 with loss 0.51711. Total time 5.35861 hours\n",
            "Training at Epoch 80 iteration 200 with loss 0.40677. Total time 5.36611 hours\n",
            "Training at Epoch 80 iteration 300 with loss 0.23522. Total time 5.37333 hours\n",
            "Training at Epoch 80 iteration 400 with loss 0.68635. Total time 5.38083 hours\n",
            "Training at Epoch 80 iteration 500 with loss 0.78306. Total time 5.38833 hours\n",
            "Training at Epoch 80 iteration 600 with loss 0.51939. Total time 5.39583 hours\n",
            "Training at Epoch 80 iteration 700 with loss 0.57912. Total time 5.40305 hours\n",
            "Training at Epoch 80 iteration 800 with loss 0.52769. Total time 5.41055 hours\n",
            "Validation at Epoch 80 , MSE: 1.66144 , Pearson Correlation: 0.23294 with p-value: 1.75552 , Concordance Index: 0.60925\n",
            "Training at Epoch 81 iteration 0 with loss 0.42827. Total time 5.41916 hours\n",
            "Training at Epoch 81 iteration 100 with loss 0.65674. Total time 5.42638 hours\n",
            "Training at Epoch 81 iteration 200 with loss 0.54720. Total time 5.43388 hours\n",
            "Training at Epoch 81 iteration 300 with loss 0.25802. Total time 5.44138 hours\n",
            "Training at Epoch 81 iteration 400 with loss 0.37702. Total time 5.44861 hours\n",
            "Training at Epoch 81 iteration 500 with loss 0.29009. Total time 5.45611 hours\n",
            "Training at Epoch 81 iteration 600 with loss 0.47146. Total time 5.46333 hours\n",
            "Training at Epoch 81 iteration 700 with loss 0.79253. Total time 5.47083 hours\n",
            "Training at Epoch 81 iteration 800 with loss 0.58681. Total time 5.47833 hours\n",
            "Validation at Epoch 81 , MSE: 1.64540 , Pearson Correlation: 0.22392 with p-value: 4.92024 , Concordance Index: 0.60822\n",
            "Training at Epoch 82 iteration 0 with loss 0.44856. Total time 5.48694 hours\n",
            "Training at Epoch 82 iteration 100 with loss 0.55503. Total time 5.49416 hours\n",
            "Training at Epoch 82 iteration 200 with loss 0.66650. Total time 5.50166 hours\n",
            "Training at Epoch 82 iteration 300 with loss 0.49871. Total time 5.50916 hours\n",
            "Training at Epoch 82 iteration 400 with loss 0.45809. Total time 5.51638 hours\n",
            "Training at Epoch 82 iteration 500 with loss 0.31686. Total time 5.52388 hours\n",
            "Training at Epoch 82 iteration 600 with loss 0.34495. Total time 5.53138 hours\n",
            "Training at Epoch 82 iteration 700 with loss 0.44320. Total time 5.53861 hours\n",
            "Training at Epoch 82 iteration 800 with loss 0.37525. Total time 5.54611 hours\n",
            "Validation at Epoch 82 , MSE: 1.69916 , Pearson Correlation: 0.19925 with p-value: 2.63045 , Concordance Index: 0.59452\n",
            "Training at Epoch 83 iteration 0 with loss 0.32184. Total time 5.55472 hours\n",
            "Training at Epoch 83 iteration 100 with loss 0.48047. Total time 5.56222 hours\n",
            "Training at Epoch 83 iteration 200 with loss 0.34220. Total time 5.56944 hours\n",
            "Training at Epoch 83 iteration 300 with loss 0.29428. Total time 5.57694 hours\n",
            "Training at Epoch 83 iteration 400 with loss 0.35650. Total time 5.58444 hours\n",
            "Training at Epoch 83 iteration 500 with loss 0.45898. Total time 5.59166 hours\n",
            "Training at Epoch 83 iteration 600 with loss 0.55457. Total time 5.59916 hours\n",
            "Training at Epoch 83 iteration 700 with loss 0.56883. Total time 5.60666 hours\n",
            "Training at Epoch 83 iteration 800 with loss 0.64253. Total time 5.61388 hours\n",
            "Validation at Epoch 83 , MSE: 1.67107 , Pearson Correlation: 0.20223 with p-value: 1.93087 , Concordance Index: 0.59501\n",
            "Training at Epoch 84 iteration 0 with loss 0.30831. Total time 5.6225 hours\n",
            "Training at Epoch 84 iteration 100 with loss 0.39578. Total time 5.63 hours\n",
            "Training at Epoch 84 iteration 200 with loss 0.24000. Total time 5.6375 hours\n",
            "Training at Epoch 84 iteration 300 with loss 0.23559. Total time 5.645 hours\n",
            "Training at Epoch 84 iteration 400 with loss 0.38182. Total time 5.65222 hours\n",
            "Training at Epoch 84 iteration 500 with loss 0.31467. Total time 5.65972 hours\n",
            "Training at Epoch 84 iteration 600 with loss 0.51863. Total time 5.66722 hours\n",
            "Training at Epoch 84 iteration 700 with loss 0.45844. Total time 5.67444 hours\n",
            "Training at Epoch 84 iteration 800 with loss 0.53154. Total time 5.68194 hours\n",
            "Validation at Epoch 84 , MSE: 1.60730 , Pearson Correlation: 0.21706 with p-value: 1.37199 , Concordance Index: 0.60394\n",
            "Training at Epoch 85 iteration 0 with loss 0.35400. Total time 5.69055 hours\n",
            "Training at Epoch 85 iteration 100 with loss 0.36196. Total time 5.69777 hours\n",
            "Training at Epoch 85 iteration 200 with loss 0.59950. Total time 5.70527 hours\n",
            "Training at Epoch 85 iteration 300 with loss 0.56310. Total time 5.71277 hours\n",
            "Training at Epoch 85 iteration 400 with loss 0.24417. Total time 5.72 hours\n",
            "Training at Epoch 85 iteration 500 with loss 0.38204. Total time 5.7275 hours\n",
            "Training at Epoch 85 iteration 600 with loss 0.46666. Total time 5.735 hours\n",
            "Training at Epoch 85 iteration 700 with loss 0.38030. Total time 5.74222 hours\n",
            "Training at Epoch 85 iteration 800 with loss 0.35221. Total time 5.74972 hours\n",
            "Validation at Epoch 85 , MSE: 1.69542 , Pearson Correlation: 0.20484 with p-value: 2.42455 , Concordance Index: 0.59748\n",
            "Training at Epoch 86 iteration 0 with loss 0.39563. Total time 5.75833 hours\n",
            "Training at Epoch 86 iteration 100 with loss 0.46443. Total time 5.76583 hours\n",
            "Training at Epoch 86 iteration 200 with loss 0.32490. Total time 5.77305 hours\n",
            "Training at Epoch 86 iteration 300 with loss 0.48002. Total time 5.78055 hours\n",
            "Training at Epoch 86 iteration 400 with loss 0.38897. Total time 5.78805 hours\n",
            "Training at Epoch 86 iteration 500 with loss 0.55532. Total time 5.79527 hours\n",
            "Training at Epoch 86 iteration 600 with loss 0.72703. Total time 5.80277 hours\n",
            "Training at Epoch 86 iteration 700 with loss 0.22352. Total time 5.81027 hours\n",
            "Training at Epoch 86 iteration 800 with loss 0.30221. Total time 5.81777 hours\n",
            "Validation at Epoch 86 , MSE: 1.65522 , Pearson Correlation: 0.21633 with p-value: 5.04918 , Concordance Index: 0.60314\n",
            "Training at Epoch 87 iteration 0 with loss 0.39930. Total time 5.82638 hours\n",
            "Training at Epoch 87 iteration 100 with loss 0.51980. Total time 5.83361 hours\n",
            "Training at Epoch 87 iteration 200 with loss 0.26151. Total time 5.84111 hours\n",
            "Training at Epoch 87 iteration 300 with loss 0.38405. Total time 5.84861 hours\n",
            "Training at Epoch 87 iteration 400 with loss 0.39275. Total time 5.85583 hours\n",
            "Training at Epoch 87 iteration 500 with loss 0.42176. Total time 5.86333 hours\n",
            "Training at Epoch 87 iteration 600 with loss 0.29804. Total time 5.87083 hours\n",
            "Training at Epoch 87 iteration 700 with loss 0.36831. Total time 5.87833 hours\n",
            "Training at Epoch 87 iteration 800 with loss 0.51817. Total time 5.88555 hours\n",
            "Validation at Epoch 87 , MSE: 1.69217 , Pearson Correlation: 0.21631 with p-value: 5.26268 , Concordance Index: 0.60380\n",
            "Training at Epoch 88 iteration 0 with loss 0.42098. Total time 5.89416 hours\n",
            "Training at Epoch 88 iteration 100 with loss 0.22435. Total time 5.90138 hours\n",
            "Training at Epoch 88 iteration 200 with loss 0.30839. Total time 5.90888 hours\n",
            "Training at Epoch 88 iteration 300 with loss 0.41627. Total time 5.91638 hours\n",
            "Training at Epoch 88 iteration 400 with loss 0.53309. Total time 5.92361 hours\n",
            "Training at Epoch 88 iteration 500 with loss 0.52033. Total time 5.93111 hours\n",
            "Training at Epoch 88 iteration 600 with loss 0.39516. Total time 5.93861 hours\n",
            "Training at Epoch 88 iteration 700 with loss 0.32717. Total time 5.94611 hours\n",
            "Training at Epoch 88 iteration 800 with loss 0.37277. Total time 5.95361 hours\n",
            "Validation at Epoch 88 , MSE: 1.68550 , Pearson Correlation: 0.20052 with p-value: 3.25438 , Concordance Index: 0.59960\n",
            "Training at Epoch 89 iteration 0 with loss 0.32384. Total time 5.96222 hours\n",
            "Training at Epoch 89 iteration 100 with loss 0.25326. Total time 5.96944 hours\n",
            "Training at Epoch 89 iteration 200 with loss 0.34809. Total time 5.97694 hours\n",
            "Training at Epoch 89 iteration 300 with loss 0.48941. Total time 5.98444 hours\n",
            "Training at Epoch 89 iteration 400 with loss 0.29761. Total time 5.99166 hours\n",
            "Training at Epoch 89 iteration 500 with loss 0.43232. Total time 5.99916 hours\n",
            "Training at Epoch 89 iteration 600 with loss 0.40728. Total time 6.00666 hours\n",
            "Training at Epoch 89 iteration 700 with loss 0.45503. Total time 6.01388 hours\n",
            "Training at Epoch 89 iteration 800 with loss 0.27711. Total time 6.02138 hours\n",
            "Validation at Epoch 89 , MSE: 1.65712 , Pearson Correlation: 0.22686 with p-value: 1.98649 , Concordance Index: 0.60967\n",
            "Training at Epoch 90 iteration 0 with loss 0.59111. Total time 6.03 hours\n",
            "Training at Epoch 90 iteration 100 with loss 0.38133. Total time 6.0375 hours\n",
            "Training at Epoch 90 iteration 200 with loss 0.29905. Total time 6.04472 hours\n",
            "Training at Epoch 90 iteration 300 with loss 0.49214. Total time 6.05222 hours\n",
            "Training at Epoch 90 iteration 400 with loss 0.42152. Total time 6.05972 hours\n",
            "Training at Epoch 90 iteration 500 with loss 0.32435. Total time 6.06694 hours\n",
            "Training at Epoch 90 iteration 600 with loss 0.42248. Total time 6.07444 hours\n",
            "Training at Epoch 90 iteration 700 with loss 0.39290. Total time 6.08194 hours\n",
            "Training at Epoch 90 iteration 800 with loss 0.45120. Total time 6.08944 hours\n",
            "Validation at Epoch 90 , MSE: 1.62173 , Pearson Correlation: 0.21906 with p-value: 3.71649 , Concordance Index: 0.60935\n",
            "Training at Epoch 91 iteration 0 with loss 0.21491. Total time 6.09777 hours\n",
            "Training at Epoch 91 iteration 100 with loss 0.37235. Total time 6.10527 hours\n",
            "Training at Epoch 91 iteration 200 with loss 0.29603. Total time 6.11277 hours\n",
            "Training at Epoch 91 iteration 300 with loss 0.36853. Total time 6.12 hours\n",
            "Training at Epoch 91 iteration 400 with loss 0.38181. Total time 6.1275 hours\n",
            "Training at Epoch 91 iteration 500 with loss 0.24852. Total time 6.135 hours\n",
            "Training at Epoch 91 iteration 600 with loss 0.41303. Total time 6.14222 hours\n",
            "Training at Epoch 91 iteration 700 with loss 0.37799. Total time 6.14972 hours\n",
            "Training at Epoch 91 iteration 800 with loss 0.23992. Total time 6.15722 hours\n",
            "Validation at Epoch 91 , MSE: 1.66538 , Pearson Correlation: 0.22646 with p-value: 4.17692 , Concordance Index: 0.60905\n",
            "Training at Epoch 92 iteration 0 with loss 0.30432. Total time 6.16583 hours\n",
            "Training at Epoch 92 iteration 100 with loss 0.38255. Total time 6.17305 hours\n",
            "Training at Epoch 92 iteration 200 with loss 0.32118. Total time 6.18055 hours\n",
            "Training at Epoch 92 iteration 300 with loss 0.45796. Total time 6.18777 hours\n",
            "Training at Epoch 92 iteration 400 with loss 0.45050. Total time 6.19527 hours\n",
            "Training at Epoch 92 iteration 500 with loss 0.64713. Total time 6.20277 hours\n",
            "Training at Epoch 92 iteration 600 with loss 0.39375. Total time 6.21 hours\n",
            "Training at Epoch 92 iteration 700 with loss 0.50165. Total time 6.2175 hours\n",
            "Training at Epoch 92 iteration 800 with loss 0.39322. Total time 6.225 hours\n",
            "Validation at Epoch 92 , MSE: 1.67318 , Pearson Correlation: 0.19508 with p-value: 2.26400 , Concordance Index: 0.59544\n",
            "Training at Epoch 93 iteration 0 with loss 0.31136. Total time 6.23333 hours\n",
            "Training at Epoch 93 iteration 100 with loss 0.53981. Total time 6.24083 hours\n",
            "Training at Epoch 93 iteration 200 with loss 0.29259. Total time 6.24833 hours\n",
            "Training at Epoch 93 iteration 300 with loss 0.50092. Total time 6.25555 hours\n",
            "Training at Epoch 93 iteration 400 with loss 0.47045. Total time 6.26305 hours\n",
            "Training at Epoch 93 iteration 500 with loss 0.22469. Total time 6.27055 hours\n",
            "Training at Epoch 93 iteration 600 with loss 0.42106. Total time 6.27777 hours\n",
            "Training at Epoch 93 iteration 700 with loss 0.36073. Total time 6.28527 hours\n",
            "Training at Epoch 93 iteration 800 with loss 0.40512. Total time 6.29277 hours\n",
            "Validation at Epoch 93 , MSE: 1.61609 , Pearson Correlation: 0.21242 with p-value: 5.22161 , Concordance Index: 0.60178\n",
            "Training at Epoch 94 iteration 0 with loss 0.44083. Total time 6.30138 hours\n",
            "Training at Epoch 94 iteration 100 with loss 0.36125. Total time 6.30861 hours\n",
            "Training at Epoch 94 iteration 200 with loss 0.31973. Total time 6.31611 hours\n",
            "Training at Epoch 94 iteration 300 with loss 0.34243. Total time 6.32361 hours\n",
            "Training at Epoch 94 iteration 400 with loss 0.31113. Total time 6.33083 hours\n",
            "Training at Epoch 94 iteration 500 with loss 0.44897. Total time 6.33833 hours\n",
            "Training at Epoch 94 iteration 600 with loss 0.40554. Total time 6.34555 hours\n",
            "Training at Epoch 94 iteration 700 with loss 0.47738. Total time 6.35305 hours\n",
            "Training at Epoch 94 iteration 800 with loss 0.33025. Total time 6.36055 hours\n",
            "Validation at Epoch 94 , MSE: 1.64626 , Pearson Correlation: 0.20841 with p-value: 5.50100 , Concordance Index: 0.59877\n",
            "Training at Epoch 95 iteration 0 with loss 0.36731. Total time 6.36916 hours\n",
            "Training at Epoch 95 iteration 100 with loss 0.32694. Total time 6.37638 hours\n",
            "Training at Epoch 95 iteration 200 with loss 0.19636. Total time 6.38388 hours\n",
            "Training at Epoch 95 iteration 300 with loss 0.16308. Total time 6.39138 hours\n",
            "Training at Epoch 95 iteration 400 with loss 0.34942. Total time 6.39861 hours\n",
            "Training at Epoch 95 iteration 500 with loss 0.59597. Total time 6.40611 hours\n",
            "Training at Epoch 95 iteration 600 with loss 0.29830. Total time 6.41361 hours\n",
            "Training at Epoch 95 iteration 700 with loss 0.30810. Total time 6.42083 hours\n",
            "Training at Epoch 95 iteration 800 with loss 0.31978. Total time 6.42833 hours\n",
            "Validation at Epoch 95 , MSE: 1.72423 , Pearson Correlation: 0.21029 with p-value: 2.16025 , Concordance Index: 0.60282\n",
            "Training at Epoch 96 iteration 0 with loss 0.31039. Total time 6.43666 hours\n",
            "Training at Epoch 96 iteration 100 with loss 0.57940. Total time 6.44416 hours\n",
            "Training at Epoch 96 iteration 200 with loss 0.26610. Total time 6.45166 hours\n",
            "Training at Epoch 96 iteration 300 with loss 0.36030. Total time 6.45916 hours\n",
            "Training at Epoch 96 iteration 400 with loss 0.39929. Total time 6.46638 hours\n",
            "Training at Epoch 96 iteration 500 with loss 0.51084. Total time 6.47388 hours\n",
            "Training at Epoch 96 iteration 600 with loss 0.45436. Total time 6.48111 hours\n",
            "Training at Epoch 96 iteration 700 with loss 0.32291. Total time 6.48861 hours\n",
            "Training at Epoch 96 iteration 800 with loss 0.41207. Total time 6.49611 hours\n",
            "Validation at Epoch 96 , MSE: 1.70848 , Pearson Correlation: 0.20655 with p-value: 1.34096 , Concordance Index: 0.59777\n",
            "Training at Epoch 97 iteration 0 with loss 0.27528. Total time 6.50472 hours\n",
            "Training at Epoch 97 iteration 100 with loss 0.34414. Total time 6.51194 hours\n",
            "Training at Epoch 97 iteration 200 with loss 0.50398. Total time 6.51944 hours\n",
            "Training at Epoch 97 iteration 300 with loss 0.46233. Total time 6.52694 hours\n",
            "Training at Epoch 97 iteration 400 with loss 0.37515. Total time 6.53416 hours\n",
            "Training at Epoch 97 iteration 500 with loss 0.39537. Total time 6.54166 hours\n",
            "Training at Epoch 97 iteration 600 with loss 0.32411. Total time 6.54916 hours\n",
            "Training at Epoch 97 iteration 700 with loss 0.36778. Total time 6.55638 hours\n",
            "Training at Epoch 97 iteration 800 with loss 0.18521. Total time 6.56388 hours\n",
            "Validation at Epoch 97 , MSE: 1.67914 , Pearson Correlation: 0.21332 with p-value: 1.07456 , Concordance Index: 0.59784\n",
            "Training at Epoch 98 iteration 0 with loss 0.24389. Total time 6.5725 hours\n",
            "Training at Epoch 98 iteration 100 with loss 0.36759. Total time 6.57972 hours\n",
            "Training at Epoch 98 iteration 200 with loss 0.45084. Total time 6.58722 hours\n",
            "Training at Epoch 98 iteration 300 with loss 0.29203. Total time 6.59444 hours\n",
            "Training at Epoch 98 iteration 400 with loss 0.23383. Total time 6.60194 hours\n",
            "Training at Epoch 98 iteration 500 with loss 0.32417. Total time 6.60944 hours\n",
            "Training at Epoch 98 iteration 600 with loss 0.29284. Total time 6.61666 hours\n",
            "Training at Epoch 98 iteration 700 with loss 0.50879. Total time 6.62416 hours\n",
            "Training at Epoch 98 iteration 800 with loss 0.32570. Total time 6.63166 hours\n",
            "Validation at Epoch 98 , MSE: 1.69920 , Pearson Correlation: 0.20689 with p-value: 7.45126 , Concordance Index: 0.60021\n",
            "Training at Epoch 99 iteration 0 with loss 0.34319. Total time 6.64 hours\n",
            "Training at Epoch 99 iteration 100 with loss 0.39269. Total time 6.6475 hours\n",
            "Training at Epoch 99 iteration 200 with loss 0.33479. Total time 6.655 hours\n",
            "Training at Epoch 99 iteration 300 with loss 0.48360. Total time 6.66222 hours\n",
            "Training at Epoch 99 iteration 400 with loss 0.45752. Total time 6.66972 hours\n",
            "Training at Epoch 99 iteration 500 with loss 0.45449. Total time 6.67722 hours\n",
            "Training at Epoch 99 iteration 600 with loss 0.27504. Total time 6.68444 hours\n",
            "Training at Epoch 99 iteration 700 with loss 0.36245. Total time 6.69194 hours\n",
            "Training at Epoch 99 iteration 800 with loss 0.27285. Total time 6.69944 hours\n",
            "Validation at Epoch 99 , MSE: 1.73096 , Pearson Correlation: 0.20317 with p-value: 3.96875 , Concordance Index: 0.59916\n",
            "Training at Epoch 100 iteration 0 with loss 0.36720. Total time 6.70777 hours\n",
            "Training at Epoch 100 iteration 100 with loss 0.31286. Total time 6.71527 hours\n",
            "Training at Epoch 100 iteration 200 with loss 0.35956. Total time 6.72277 hours\n",
            "Training at Epoch 100 iteration 300 with loss 0.35258. Total time 6.73 hours\n",
            "Training at Epoch 100 iteration 400 with loss 0.24482. Total time 6.7375 hours\n",
            "Training at Epoch 100 iteration 500 with loss 0.29929. Total time 6.745 hours\n",
            "Training at Epoch 100 iteration 600 with loss 0.35254. Total time 6.75222 hours\n",
            "Training at Epoch 100 iteration 700 with loss 0.34265. Total time 6.75972 hours\n",
            "Training at Epoch 100 iteration 800 with loss 0.29740. Total time 6.76722 hours\n",
            "Validation at Epoch 100 , MSE: 1.69666 , Pearson Correlation: 0.21681 with p-value: 2.14017 , Concordance Index: 0.60517\n",
            "--- Go for Testing ---\n",
            "Testing MSE: 1.3661350143682738 , Pearson Correlation: 0.2770051076485187 with p-value: 3.0867658935899862e-136 , Concordance Index: 0.6242840725491154\n",
            "--- Training Finished ---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgRfv4o06vZy"
      },
      "source": [
        "pickle_rick.dump(model, open(models_folder + \"model_first_colab.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDjDNWGQiY_W"
      },
      "source": [
        "The MSE obtained from the preliminary model was 1.3661350143682738."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC5kuBMijpog"
      },
      "source": [
        "# Model fine-tuning\n",
        "\n",
        "To finetune the model, we will perform random search and pick the best performant model on the validation set. The hyperparameter ranges are as follows:\n",
        "\n",
        "\n",
        "To perform the random search, we decided to do it randomly, using 10 separate jupyter notebooks to run the model with a different set of hyperparameters, in parallel, to save on computation time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVbQt8-Re5hw"
      },
      "source": [
        "## Create new embeddings\n",
        "\n",
        "One of the hyperparameters is to use the transformer protein encodings for the convolutional encoder. To do this, a new dataset needs to be preprocessed, as in the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPg1QTPDlEfH",
        "outputId": "9dff6bd4-8b20-4fb7-c1af-14419485957a"
      },
      "source": [
        " X_drug, X_target, y  = load_process_DAVIS(path = data_folder, binary = False, convert_to_log = True, threshold = 30)\n",
        "\n",
        "if not os.path.isfile(data_folder + 'BindingDB_All.tsv'):\n",
        "    bdb_path = download_BindingDB(path = data_folder)\n",
        "else:\n",
        "    bdb_path = data_folder + 'BindingDB_All.tsv'\n",
        "X_drug2, X_target2, y2  = process_BindingDB(path = bdb_path, df = None, y = 'Kd', binary = False, convert_to_log = True, threshold = 30)\n",
        "\n",
        "X_drug = np.concatenate((X_drug, X_drug2), axis=0)\n",
        "X_target = np.concatenate((X_target, X_target2), axis=0)\n",
        "y = np.concatenate((y, y2), axis=0)\n",
        "\n",
        "# ensure all uppercase for checking duplicates\n",
        "upper = np.vectorize(lambda x: x.upper())\n",
        "X_drug = upper(X_drug)\n",
        "X_target = upper(X_target)\n",
        "\n",
        "# Keep DAVIS version if duplicates exist\n",
        "all_data = pd.DataFrame(data=np.transpose(np.stack([X_drug, X_target, y])))\n",
        "all_data = all_data.drop_duplicates(subset=[0, 1], keep='first')\n",
        "\n",
        "drug_encoding, target_encoding = 'Transformer', 'CNN_inspire'\n",
        "#ensure none of test smiles appears in set\n",
        "train, val, test  = data_process(all_data[0].to_numpy(), all_data[1].to_numpy(), y, \n",
        "                      drug_encoding, target_encoding, \n",
        "                      split_method='cold_protein',frac=[0.8,0.1,0.1],\n",
        "                      cnn_inspire_use_transformer_embedding=True, # new embedding set\n",
        "                      random_seed = 42)\n",
        "# train.head()\n",
        "print(f\"First drug representation: \\n{train.drug_encoding.iloc[0]}\")\n",
        "print(f\"First target representation: \\n{train.target_encoding.iloc[0]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning Processing...\n",
            "Beginning to extract zip file...\n",
            "Default set to logspace (nM -> p) for easier regression\n",
            "Done!\n",
            "Loading Dataset from path...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 772572: expected 193 fields, saw 205\\nSkipping line 772598: expected 193 fields, saw 205\\n'\n",
            "b'Skipping line 805291: expected 193 fields, saw 205\\n'\n",
            "b'Skipping line 827961: expected 193 fields, saw 265\\n'\n",
            "b'Skipping line 1231688: expected 193 fields, saw 241\\n'\n",
            "b'Skipping line 1345591: expected 193 fields, saw 241\\nSkipping line 1345592: expected 193 fields, saw 241\\nSkipping line 1345593: expected 193 fields, saw 241\\nSkipping line 1345594: expected 193 fields, saw 241\\nSkipping line 1345595: expected 193 fields, saw 241\\nSkipping line 1345596: expected 193 fields, saw 241\\nSkipping line 1345597: expected 193 fields, saw 241\\nSkipping line 1345598: expected 193 fields, saw 241\\nSkipping line 1345599: expected 193 fields, saw 241\\n'\n",
            "b'Skipping line 1358864: expected 193 fields, saw 205\\n'\n",
            "b'Skipping line 1378087: expected 193 fields, saw 241\\nSkipping line 1378088: expected 193 fields, saw 241\\nSkipping line 1378089: expected 193 fields, saw 241\\nSkipping line 1378090: expected 193 fields, saw 241\\nSkipping line 1378091: expected 193 fields, saw 241\\nSkipping line 1378092: expected 193 fields, saw 241\\nSkipping line 1378093: expected 193 fields, saw 241\\nSkipping line 1378094: expected 193 fields, saw 241\\nSkipping line 1378095: expected 193 fields, saw 241\\n'\n",
            "b'Skipping line 1417264: expected 193 fields, saw 205\\n'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Beginning Processing...\n",
            "There are 66444 drug target pairs.\n",
            "Default set to logspace (nM -> p) for easier regression\n",
            "Drug Target Interaction Prediction Mode...\n",
            "in total: 71845 drug-target pairs\n",
            "encoding drug...\n",
            "unique drugs: 10685\n",
            "encoding protein...\n",
            "unique target sequence: 1449\n",
            "splitting dataset...\n",
            "Done.\n",
            "First drug representation: \n",
            "(array([ 800,  122,  248,  282,  623,  272, 1256, 2210,   91,   85,  109,\n",
            "        119,   80,    8,  282,  861,  209,   19,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "          0,    0,    0,    0,    0,    0]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0]))\n",
            "First target representation: \n",
            "[ 839  167  106   61  277  447  291 2515   32  447   94  291   45 1607\n",
            "  200   87   67 1491   69  495 2820   32 1251  677   84   94   81  227\n",
            " 1684  110  301   97  121 1429 3640  927  144 3875 3718 2624  558 1464\n",
            "   33   92  320   33  319 3157   28   38  171    3  328   32  869   55\n",
            "  202  581 1494 1412  121   28 1937    4  221  237   39  265  313  440\n",
            "  109  122  283   36  733  210   25  261   78  315   28  303  705   59\n",
            " 1942 2078   13  698   97  423   75  620  272  453  257  387  324   55\n",
            " 1187  140   79  181   49  369  251 1163 1357    4  262  616   87  189\n",
            "  253   46 2151 2604 1378  126 1923  156  421  142   95   12  818  128\n",
            "  128  110  128  157 1244 1722  239   25  532  246  101 2015 1818  101\n",
            "  285 1460   74  514   26  307 2004   15  265 1602  101 1021   94 1031\n",
            " 1284  280   41  205  163 1507  712  144  768  109 1009   59  120  115\n",
            "  282  236   33  288   63 2779   90  163  133 1577   76  991  282  172\n",
            "   53  163   13  872  109   98  921  172   72  412 3778  853  234 3813\n",
            "   56   15  638   98   13  715   72   98  983  147   12  638   72 1715\n",
            "  311   39  175   59 1938   46   72  178   42  276  447 3813 2686  204\n",
            " 2329  638  673   59 1485 3813  226   98  183   98  133  318  236 1875\n",
            "  163  163 2347  163   83   72  249   46 1577   84   98  280  120   85\n",
            "  715   39  105  196   58   31 1372  691  116  136   41 1661   38  165\n",
            "   69  158   39   87   90   50  103  172   25  522  332  248 1425  103\n",
            " 3990 1006  179 1369   98 2127 1383  146  238 3190 1261  235   99   40\n",
            " 3302  628  112    3 2711  146  211   65   40  447   43   31  857 1947\n",
            " 1440   21   80   59   94   99   41  606  110  720  205  672  412  323\n",
            "   64  177  232   47   13 1292  387 2937   29  103  128   94  194  128\n",
            "   25  399 1602  705 1145 3730  889  380    8   31  101  417  115   90\n",
            "  581   76   13 1092  646  242  127   31  442  106  348  203  323  143\n",
            "   85  478   34 1341    7  598   83  148   15  332  106   55  596  123\n",
            "    6  406  165  206   43   75 1501  101   28   79  111  282   32  158\n",
            "  162  349   33  189  471   55  707  194   62   51   68    9    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53FBKYFef8DA"
      },
      "source": [
        "Using the same random seed as before, the split should be the same, as can be confirmed by the first drug representation.\n",
        "\n",
        "If everything is in order, save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmlG7efVf6ut"
      },
      "source": [
        "train.to_pickle(data_folder + 'G35_data_train_t_emb.pkl')\n",
        "val.to_pickle(data_folder + 'G35_data_vali_t_emb.pkl')\n",
        "test.to_pickle(data_folder + 'G35_data_test_t_emb.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP5O29XhgBKe"
      },
      "source": [
        "## Hyperparameter selection and randomization\n",
        "\n",
        "The available hyperparameters are as in the text cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0IzZ6_lh40l",
        "outputId": "998b66b6-c243-4edf-8904-778c9e4e0644"
      },
      "source": [
        "hyperparameter_ranges = {\n",
        "                      'cls_hidden_n_layers': list(range(1, 5)),\n",
        "                      'cls_hidden_dims': [1024,512,256,128],  \n",
        "                      'LR': [ 0.001, 0.0001], \n",
        "                      'batch_size': [32, 64, 128],\n",
        "                      'cnn_inspire_use_transformer_embedding': [True, False],\n",
        "                      'inspire_activation': ['elu', 'relu', 'sigmoid'],\n",
        "                      'CNN_inspire_filters': [64, 128],\n",
        "                      'n_protein_strides': list(range(3, 7)),\n",
        "                      'protein_strides': [5, 10, 15, 20, 25, 30, 35, 40],\n",
        "                      'inspire_dropout':  [x / 100.0 for x in range(0, 22, 2)],\n",
        "                      'n_protein_layers': list(range(0,3)),\n",
        "                      'protein_layers':  [128, 64],\n",
        "                      'transformer_size': ['base', 'large'],\n",
        "                      'transformer_dropout_rate': [x / 100.0 for x in range(0, 22, 2)],\n",
        "                      'transformer_attention_probs_dropout': [x / 100.0 for x in range(0, 22, 2)],\n",
        "                      'transformer_hidden_dropout_rate': [x / 100.0 for x in range(0, 22, 2)],\n",
        "                      'decay': [0.001, 0.0001, 0]\n",
        "}\n",
        "\n",
        "hyperparameter_ranges"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CNN_inspire_filters': [64, 128],\n",
              " 'LR': [0.001, 0.0001],\n",
              " 'batch_size': [32, 64, 128],\n",
              " 'cls_hidden_dims': [1024, 512, 256, 128],\n",
              " 'cls_hidden_n_layers': [1, 2, 3, 4],\n",
              " 'cnn_inspire_use_transformer_embedding': [True, False],\n",
              " 'decay': [0.001, 0.0001, 0],\n",
              " 'inspire_activation': ['elu', 'relu', 'sigmoid'],\n",
              " 'inspire_dropout': [0.0,\n",
              "  0.02,\n",
              "  0.04,\n",
              "  0.06,\n",
              "  0.08,\n",
              "  0.1,\n",
              "  0.12,\n",
              "  0.14,\n",
              "  0.16,\n",
              "  0.18,\n",
              "  0.2],\n",
              " 'n_protein_layers': [0, 1, 2],\n",
              " 'n_protein_strides': [3, 4, 5, 6],\n",
              " 'protein_layers': [128, 64],\n",
              " 'protein_strides': [5, 10, 15, 20, 25, 30, 35, 40],\n",
              " 'transformer_attention_probs_dropout': [0.0,\n",
              "  0.02,\n",
              "  0.04,\n",
              "  0.06,\n",
              "  0.08,\n",
              "  0.1,\n",
              "  0.12,\n",
              "  0.14,\n",
              "  0.16,\n",
              "  0.18,\n",
              "  0.2],\n",
              " 'transformer_dropout_rate': [0.0,\n",
              "  0.02,\n",
              "  0.04,\n",
              "  0.06,\n",
              "  0.08,\n",
              "  0.1,\n",
              "  0.12,\n",
              "  0.14,\n",
              "  0.16,\n",
              "  0.18,\n",
              "  0.2],\n",
              " 'transformer_hidden_dropout_rate': [0.0,\n",
              "  0.02,\n",
              "  0.04,\n",
              "  0.06,\n",
              "  0.08,\n",
              "  0.1,\n",
              "  0.12,\n",
              "  0.14,\n",
              "  0.16,\n",
              "  0.18,\n",
              "  0.2],\n",
              " 'transformer_size': ['base', 'large']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAEpfaEPnZhx"
      },
      "source": [
        "Now pick random hyperparameters (each notebook uses a different seed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYDvpY2snRCg",
        "outputId": "19d45d39-724c-4283-eab9-5d169bcdf8fb"
      },
      "source": [
        "import random\n",
        "random_seed = 8\n",
        "random.seed(random_seed)\n",
        "\n",
        "hyperparameters = {}\n",
        "cls_hidden_n_layers = random.choice(hyperparameter_ranges['cls_hidden_n_layers'])\n",
        "# sample with replacement, size of layer from greatest to least\n",
        "hyperparameters['cls_hidden_dims'] = sorted(random.choices(hyperparameter_ranges['cls_hidden_dims'], k=cls_hidden_n_layers), reverse=True)\n",
        "hyperparameters['LR'] = random.choice(hyperparameter_ranges['LR'])\n",
        "hyperparameters['batch_size'] = random.choice(hyperparameter_ranges['batch_size'])\n",
        "hyperparameters['cnn_inspire_use_transformer_embedding'] = random.choice(hyperparameter_ranges['cnn_inspire_use_transformer_embedding'])\n",
        "hyperparameters['inspire_activation'] = random.choice(hyperparameter_ranges['inspire_activation'])\n",
        "hyperparameters['CNN_inspire_filters'] = random.choice(hyperparameter_ranges['CNN_inspire_filters'])\n",
        "n_protein_strides = random.choice(hyperparameter_ranges['n_protein_strides'])\n",
        "# sample without replacement and sort\n",
        "hyperparameters['protein_strides'] = sorted(random.sample(hyperparameter_ranges['protein_strides'], k=n_protein_strides))\n",
        "hyperparameters['inspire_dropout'] = random.choice(hyperparameter_ranges['inspire_dropout'])\n",
        "n_protein_layers = random.choice(hyperparameter_ranges['n_protein_layers'])\n",
        "# sample with replacement\n",
        "hyperparameters['protein_layers'] = random.choices(hyperparameter_ranges['protein_layers'], k=n_protein_layers)\n",
        "\n",
        "# Transformer sizes from MT-DTI\n",
        "\n",
        "hyperparameters['epochs'] = 40\n",
        "if random.choice(hyperparameter_ranges['transformer_size']) is 'large':\n",
        "    hyperparameters['transformer_emb_size_drug'] = 240\n",
        "    hyperparameters['transformer_intermediate_size_drug'] = 960\n",
        "    hyperparameters['transformer_num_attention_heads_drug'] = 16\n",
        "    hyperparameters['transformer_n_layer_drug'] = 24\n",
        "else:\n",
        "    hyperparameters['transformer_emb_size_drug'] = 128\n",
        "    hyperparameters['transformer_intermediate_size_drug'] = 512\n",
        "    hyperparameters['transformer_num_attention_heads_drug'] = 8\n",
        "    hyperparameters['transformer_n_layer_drug'] = 8\n",
        "\n",
        "hyperparameters['transformer_dropout_rate'] = random.choice(hyperparameter_ranges['transformer_dropout_rate'])\n",
        "hyperparameters['transformer_attention_probs_dropout'] = random.choice(hyperparameter_ranges['transformer_attention_probs_dropout'])\n",
        "hyperparameters['transformer_hidden_dropout_rate'] = random.choice(hyperparameter_ranges['transformer_hidden_dropout_rate'])\n",
        "hyperparameters['decay'] = random.choice(hyperparameter_ranges['decay'])\n",
        "\n",
        "hyperparameters"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CNN_inspire_filters': 64,\n",
              " 'LR': 0.001,\n",
              " 'batch_size': 128,\n",
              " 'cls_hidden_dims': [512, 512],\n",
              " 'cnn_inspire_use_transformer_embedding': True,\n",
              " 'decay': 0.001,\n",
              " 'epochs': 40,\n",
              " 'inspire_activation': 'elu',\n",
              " 'inspire_dropout': 0.14,\n",
              " 'protein_layers': [128],\n",
              " 'protein_strides': [5, 20, 30, 40],\n",
              " 'transformer_attention_probs_dropout': 0.06,\n",
              " 'transformer_dropout_rate': 0.18,\n",
              " 'transformer_emb_size_drug': 240,\n",
              " 'transformer_hidden_dropout_rate': 0.12,\n",
              " 'transformer_intermediate_size_drug': 960,\n",
              " 'transformer_n_layer_drug': 24,\n",
              " 'transformer_num_attention_heads_drug': 16}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxeEjjOtvwbV"
      },
      "source": [
        "## Train with random search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2nfU2L5v5b8"
      },
      "source": [
        "Select correct preprocessed data for encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF-JIuasv31V"
      },
      "source": [
        "if hyperparameters['cnn_inspire_use_transformer_embedding']:\n",
        "    train = pd.read_pickle(data_folder + 'G35_data_train_t_emb.pkl')\n",
        "    val = pd.read_pickle(data_folder + 'G35_data_vali_t_emb.pkl')\n",
        "    test = pd.read_pickle(data_folder + 'G35_data_test_t_emb.pkl')\n",
        "else:\n",
        "    train = pd.read_pickle(data_folder + 'G35_data_train.pkl')\n",
        "    val = pd.read_pickle(data_folder + 'G35_data_vali.pkl')\n",
        "    test = pd.read_pickle(data_folder + 'G35_data_test.pkl')\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKIDQz7uwbos"
      },
      "source": [
        "Start training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMYB8MuJwgvT"
      },
      "source": [
        "drug_encoding, target_encoding = 'Transformer', 'CNN_inspire'\n",
        "\n",
        "config = generate_config(drug_encoding = drug_encoding, \n",
        "                      target_encoding = target_encoding, \n",
        "                      result_folder = results_folder + str(random_seed),\n",
        "                      cls_hidden_dims = hyperparameters['cls_hidden_dims'],  \n",
        "                      train_epoch = hyperparameters['epochs'], \n",
        "                      LR = hyperparameters['LR'], \n",
        "                      batch_size = hyperparameters['batch_size'],\n",
        "                      cnn_inspire_use_transformer_embedding = hyperparameters['cnn_inspire_use_transformer_embedding'],\n",
        "                      inspire_activation = hyperparameters['inspire_activation'],\n",
        "                      CNN_inspire_filters = hyperparameters['CNN_inspire_filters'],\n",
        "                      protein_strides = hyperparameters['protein_strides'],\n",
        "                      inspire_dropout =  hyperparameters['inspire_dropout'],\n",
        "                      protein_layers =  hyperparameters['protein_layers'],\n",
        "                      transformer_emb_size_drug = hyperparameters['transformer_emb_size_drug'],\n",
        "                      transformer_intermediate_size_drug = hyperparameters['transformer_intermediate_size_drug'],\n",
        "                      transformer_num_attention_heads_drug = hyperparameters['transformer_num_attention_heads_drug'],\n",
        "                      transformer_n_layer_drug = hyperparameters['transformer_n_layer_drug'],\n",
        "                      transformer_dropout_rate = hyperparameters['transformer_dropout_rate'],\n",
        "                      transformer_attention_probs_dropout = hyperparameters['transformer_attention_probs_dropout'],\n",
        "                      transformer_hidden_dropout_rate = hyperparameters['transformer_hidden_dropout_rate'],\n",
        "                      num_workers = 0,\n",
        "                      decay=hyperparameters['decay']\n",
        "                    )"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6sR1q0HvycgB",
        "outputId": "e7fa58d3-efe6-48f1-e640-a38ff6bb6529"
      },
      "source": [
        "model = models.model_initialize(**config)\n",
        "model.train(train, val, val, drop_last = True, save_path = models_folder + str(random_seed))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's use 1 GPU!\n",
            "--- Data Preparation ---\n",
            "--- Go for Training ---\n",
            "Training at Epoch 1 iteration 0 with loss 32.0127. Total time 0.00027 hours\n",
            "Training at Epoch 1 iteration 100 with loss 1.08914. Total time 0.01333 hours\n",
            "Training at Epoch 1 iteration 200 with loss 1.39393. Total time 0.02694 hours\n",
            "Training at Epoch 1 iteration 300 with loss 1.92245. Total time 0.04083 hours\n",
            "Training at Epoch 1 iteration 400 with loss 1.47839. Total time 0.05472 hours\n",
            "Validation at Epoch 1 , MSE: 1.45437 , Pearson Correlation: -0.0255 with p-value: 0.02384 , Concordance Index: 0.48600\n",
            "Training at Epoch 2 iteration 0 with loss 1.24092. Total time 0.06305 hours\n",
            "Training at Epoch 2 iteration 100 with loss 1.10362. Total time 0.07694 hours\n",
            "Training at Epoch 2 iteration 200 with loss 1.53215. Total time 0.09083 hours\n",
            "Training at Epoch 2 iteration 300 with loss 1.44087. Total time 0.105 hours\n",
            "Training at Epoch 2 iteration 400 with loss 2.07211. Total time 0.11888 hours\n",
            "Validation at Epoch 2 , MSE: 1.47857 , Pearson Correlation: -0.0206 with p-value: 0.06785 , Concordance Index: 0.49332\n",
            "Training at Epoch 3 iteration 0 with loss 1.65065. Total time 0.12694 hours\n",
            "Training at Epoch 3 iteration 100 with loss 1.66600. Total time 0.14111 hours\n",
            "Training at Epoch 3 iteration 200 with loss 1.31855. Total time 0.155 hours\n",
            "Training at Epoch 3 iteration 300 with loss 1.13366. Total time 0.16888 hours\n",
            "Training at Epoch 3 iteration 400 with loss 1.36865. Total time 0.18277 hours\n",
            "Validation at Epoch 3 , MSE: 1.47812 , Pearson Correlation: 9.12362 with p-value: 0.99356 , Concordance Index: 0.50207\n",
            "Training at Epoch 4 iteration 0 with loss 1.50071. Total time 0.19111 hours\n",
            "Training at Epoch 4 iteration 100 with loss 1.68704. Total time 0.205 hours\n",
            "Training at Epoch 4 iteration 200 with loss 2.02228. Total time 0.21888 hours\n",
            "Training at Epoch 4 iteration 300 with loss 1.59796. Total time 0.23277 hours\n",
            "Training at Epoch 4 iteration 400 with loss 1.40856. Total time 0.24666 hours\n",
            "Validation at Epoch 4 , MSE: 1.44361 , Pearson Correlation: -0.0175 with p-value: 0.12030 , Concordance Index: 0.49639\n",
            "Training at Epoch 5 iteration 0 with loss 1.59741. Total time 0.255 hours\n",
            "Training at Epoch 5 iteration 100 with loss 0.93143. Total time 0.26888 hours\n",
            "Training at Epoch 5 iteration 200 with loss 1.26849. Total time 0.28277 hours\n",
            "Training at Epoch 5 iteration 300 with loss 1.42026. Total time 0.29666 hours\n",
            "Training at Epoch 5 iteration 400 with loss 1.38865. Total time 0.31083 hours\n",
            "Validation at Epoch 5 , MSE: 1.44510 , Pearson Correlation: 0.01870 with p-value: 0.09846 , Concordance Index: 0.50824\n",
            "Training at Epoch 6 iteration 0 with loss 1.42704. Total time 0.31888 hours\n",
            "Training at Epoch 6 iteration 100 with loss 1.33374. Total time 0.33305 hours\n",
            "Training at Epoch 6 iteration 200 with loss 1.53493. Total time 0.34694 hours\n",
            "Training at Epoch 6 iteration 300 with loss 1.77294. Total time 0.36083 hours\n",
            "Training at Epoch 6 iteration 400 with loss 1.13224. Total time 0.37472 hours\n",
            "Validation at Epoch 6 , MSE: 1.46965 , Pearson Correlation: -0.0295 with p-value: 0.00905 , Concordance Index: 0.48571\n",
            "Training at Epoch 7 iteration 0 with loss 1.60790. Total time 0.38305 hours\n",
            "Training at Epoch 7 iteration 100 with loss 1.39127. Total time 0.39694 hours\n",
            "Training at Epoch 7 iteration 200 with loss 2.00492. Total time 0.41083 hours\n",
            "Training at Epoch 7 iteration 300 with loss 1.72374. Total time 0.42472 hours\n",
            "Training at Epoch 7 iteration 400 with loss 1.35835. Total time 0.43888 hours\n",
            "Validation at Epoch 7 , MSE: 1.45198 , Pearson Correlation: 0.01816 with p-value: 0.10852 , Concordance Index: 0.50832\n",
            "Training at Epoch 8 iteration 0 with loss 1.64181. Total time 0.44694 hours\n",
            "Training at Epoch 8 iteration 100 with loss 1.39938. Total time 0.46111 hours\n",
            "Training at Epoch 8 iteration 200 with loss 1.89688. Total time 0.475 hours\n",
            "Training at Epoch 8 iteration 300 with loss 1.98629. Total time 0.48888 hours\n",
            "Training at Epoch 8 iteration 400 with loss 1.53086. Total time 0.50277 hours\n",
            "Validation at Epoch 8 , MSE: 1.53612 , Pearson Correlation: -0.0257 with p-value: 0.02307 , Concordance Index: 0.48322\n",
            "Training at Epoch 9 iteration 0 with loss 1.26611. Total time 0.51111 hours\n",
            "Training at Epoch 9 iteration 100 with loss 1.47279. Total time 0.525 hours\n",
            "Training at Epoch 9 iteration 200 with loss 1.11859. Total time 0.53888 hours\n",
            "Training at Epoch 9 iteration 300 with loss 1.50721. Total time 0.55277 hours\n",
            "Training at Epoch 9 iteration 400 with loss 1.50818. Total time 0.56666 hours\n",
            "Validation at Epoch 9 , MSE: 1.52317 , Pearson Correlation: -0.0334 with p-value: 0.00308 , Concordance Index: 0.48497\n",
            "Training at Epoch 10 iteration 0 with loss 0.95749. Total time 0.575 hours\n",
            "Training at Epoch 10 iteration 100 with loss 1.30483. Total time 0.58888 hours\n",
            "Training at Epoch 10 iteration 200 with loss 0.97714. Total time 0.60305 hours\n",
            "Training at Epoch 10 iteration 300 with loss 1.51785. Total time 0.61694 hours\n",
            "Training at Epoch 10 iteration 400 with loss 1.40233. Total time 0.63083 hours\n",
            "Validation at Epoch 10 , MSE: 1.49040 , Pearson Correlation: -0.0301 with p-value: 0.00766 , Concordance Index: 0.48132\n",
            "Training at Epoch 11 iteration 0 with loss 1.14454. Total time 0.63916 hours\n",
            "Training at Epoch 11 iteration 100 with loss 1.75885. Total time 0.65305 hours\n",
            "Training at Epoch 11 iteration 200 with loss 2.05586. Total time 0.66694 hours\n",
            "Training at Epoch 11 iteration 300 with loss 1.21023. Total time 0.68083 hours\n",
            "Training at Epoch 11 iteration 400 with loss 1.64177. Total time 0.69472 hours\n",
            "Validation at Epoch 11 , MSE: 1.45485 , Pearson Correlation: -0.0166 with p-value: 0.14062 , Concordance Index: 0.49395\n",
            "Training at Epoch 12 iteration 0 with loss 1.73327. Total time 0.70305 hours\n",
            "Training at Epoch 12 iteration 100 with loss 1.70344. Total time 0.71722 hours\n",
            "Training at Epoch 12 iteration 200 with loss 1.80253. Total time 0.73111 hours\n",
            "Training at Epoch 12 iteration 300 with loss 1.36632. Total time 0.745 hours\n",
            "Training at Epoch 12 iteration 400 with loss 1.72952. Total time 0.75888 hours\n",
            "Validation at Epoch 12 , MSE: 1.47714 , Pearson Correlation: 0.02117 with p-value: 0.06130 , Concordance Index: 0.51473\n",
            "Training at Epoch 13 iteration 0 with loss 1.28602. Total time 0.76722 hours\n",
            "Training at Epoch 13 iteration 100 with loss 1.36835. Total time 0.78111 hours\n",
            "Training at Epoch 13 iteration 200 with loss 1.34658. Total time 0.795 hours\n",
            "Training at Epoch 13 iteration 300 with loss 1.36716. Total time 0.80888 hours\n",
            "Training at Epoch 13 iteration 400 with loss 1.40103. Total time 0.82305 hours\n",
            "Validation at Epoch 13 , MSE: 1.53442 , Pearson Correlation: 0.01826 with p-value: 0.10654 , Concordance Index: 0.50617\n",
            "Training at Epoch 14 iteration 0 with loss 1.26647. Total time 0.83111 hours\n",
            "Training at Epoch 14 iteration 100 with loss 1.32874. Total time 0.845 hours\n",
            "Training at Epoch 14 iteration 200 with loss 1.42470. Total time 0.85916 hours\n",
            "Training at Epoch 14 iteration 300 with loss 1.42573. Total time 0.87305 hours\n",
            "Training at Epoch 14 iteration 400 with loss 1.78996. Total time 0.88694 hours\n",
            "Validation at Epoch 14 , MSE: 1.50540 , Pearson Correlation: -0.0185 with p-value: 0.10117 , Concordance Index: 0.49618\n",
            "Training at Epoch 15 iteration 0 with loss 1.31691. Total time 0.895 hours\n",
            "Training at Epoch 15 iteration 100 with loss 1.30674. Total time 0.90916 hours\n",
            "Training at Epoch 15 iteration 200 with loss 1.31697. Total time 0.92305 hours\n",
            "Training at Epoch 15 iteration 300 with loss 1.67189. Total time 0.93694 hours\n",
            "Training at Epoch 15 iteration 400 with loss 1.41756. Total time 0.95083 hours\n",
            "Validation at Epoch 15 , MSE: 1.49277 , Pearson Correlation: 0.00234 with p-value: 0.83579 , Concordance Index: 0.50295\n",
            "Training at Epoch 16 iteration 0 with loss 1.27048. Total time 0.95916 hours\n",
            "Training at Epoch 16 iteration 100 with loss 2.02899. Total time 0.97305 hours\n",
            "Training at Epoch 16 iteration 200 with loss 1.50530. Total time 0.98694 hours\n",
            "Training at Epoch 16 iteration 300 with loss 1.00817. Total time 1.00083 hours\n",
            "Training at Epoch 16 iteration 400 with loss 1.65468. Total time 1.01472 hours\n",
            "Validation at Epoch 16 , MSE: 1.44802 , Pearson Correlation: 0.01174 with p-value: 0.29942 , Concordance Index: 0.50811\n",
            "Training at Epoch 17 iteration 0 with loss 1.62885. Total time 1.02277 hours\n",
            "Training at Epoch 17 iteration 100 with loss 1.40535. Total time 1.03694 hours\n",
            "Training at Epoch 17 iteration 200 with loss 1.52430. Total time 1.05083 hours\n",
            "Training at Epoch 17 iteration 300 with loss 1.79837. Total time 1.06472 hours\n",
            "Training at Epoch 17 iteration 400 with loss 1.11077. Total time 1.07861 hours\n",
            "Validation at Epoch 17 , MSE: 1.61772 , Pearson Correlation: -0.0132 with p-value: 0.24337 , Concordance Index: 0.49844\n",
            "Training at Epoch 18 iteration 0 with loss 1.77381. Total time 1.08694 hours\n",
            "Training at Epoch 18 iteration 100 with loss 1.39902. Total time 1.10083 hours\n",
            "Training at Epoch 18 iteration 200 with loss 1.50730. Total time 1.11472 hours\n",
            "Training at Epoch 18 iteration 300 with loss 1.34246. Total time 1.12861 hours\n",
            "Training at Epoch 18 iteration 400 with loss 1.74962. Total time 1.1425 hours\n",
            "Validation at Epoch 18 , MSE: 1.55457 , Pearson Correlation: 0.00693 with p-value: 0.54021 , Concordance Index: 0.50303\n",
            "Training at Epoch 19 iteration 0 with loss 1.13940. Total time 1.15083 hours\n",
            "Training at Epoch 19 iteration 100 with loss 1.76290. Total time 1.16472 hours\n",
            "Training at Epoch 19 iteration 200 with loss 1.62926. Total time 1.17861 hours\n",
            "Training at Epoch 19 iteration 300 with loss 1.89395. Total time 1.19277 hours\n",
            "Training at Epoch 19 iteration 400 with loss 1.59638. Total time 1.20666 hours\n",
            "Validation at Epoch 19 , MSE: 1.45177 , Pearson Correlation: -0.0161 with p-value: 0.15394 , Concordance Index: 0.49434\n",
            "Training at Epoch 20 iteration 0 with loss 1.47355. Total time 1.21472 hours\n",
            "Training at Epoch 20 iteration 100 with loss 1.59637. Total time 1.22888 hours\n",
            "Training at Epoch 20 iteration 200 with loss 1.69669. Total time 1.24277 hours\n",
            "Training at Epoch 20 iteration 300 with loss 1.12417. Total time 1.25666 hours\n",
            "Training at Epoch 20 iteration 400 with loss 2.21859. Total time 1.27055 hours\n",
            "Validation at Epoch 20 , MSE: 1.49703 , Pearson Correlation: 0.01247 with p-value: 0.27031 , Concordance Index: 0.50819\n",
            "Training at Epoch 21 iteration 0 with loss 1.95884. Total time 1.27888 hours\n",
            "Training at Epoch 21 iteration 100 with loss 1.38658. Total time 1.29277 hours\n",
            "Training at Epoch 21 iteration 200 with loss 1.56021. Total time 1.30666 hours\n",
            "Training at Epoch 21 iteration 300 with loss 1.43170. Total time 1.32055 hours\n",
            "Training at Epoch 21 iteration 400 with loss 1.39678. Total time 1.33472 hours\n",
            "Validation at Epoch 21 , MSE: 1.52298 , Pearson Correlation: 0.01029 with p-value: 0.36287 , Concordance Index: 0.50954\n",
            "Training at Epoch 22 iteration 0 with loss 1.56987. Total time 1.34277 hours\n",
            "Training at Epoch 22 iteration 100 with loss 1.53185. Total time 1.35666 hours\n",
            "Training at Epoch 22 iteration 200 with loss 1.07992. Total time 1.37083 hours\n",
            "Training at Epoch 22 iteration 300 with loss 1.82930. Total time 1.38472 hours\n",
            "Training at Epoch 22 iteration 400 with loss 1.78365. Total time 1.39861 hours\n",
            "Validation at Epoch 22 , MSE: 1.51077 , Pearson Correlation: 0.00232 with p-value: 0.83735 , Concordance Index: 0.50511\n",
            "Training at Epoch 23 iteration 0 with loss 1.05637. Total time 1.40694 hours\n",
            "Training at Epoch 23 iteration 100 with loss 1.21669. Total time 1.42083 hours\n",
            "Training at Epoch 23 iteration 200 with loss 1.47314. Total time 1.43472 hours\n",
            "Training at Epoch 23 iteration 300 with loss 1.61655. Total time 1.44861 hours\n",
            "Training at Epoch 23 iteration 400 with loss 1.03955. Total time 1.4625 hours\n",
            "Validation at Epoch 23 , MSE: 1.46094 , Pearson Correlation: 0.00855 with p-value: 0.44999 , Concordance Index: 0.50806\n",
            "Training at Epoch 24 iteration 0 with loss 1.32407. Total time 1.47083 hours\n",
            "Training at Epoch 24 iteration 100 with loss 1.56674. Total time 1.48472 hours\n",
            "Training at Epoch 24 iteration 200 with loss 1.15175. Total time 1.49888 hours\n",
            "Training at Epoch 24 iteration 300 with loss 1.49060. Total time 1.51277 hours\n",
            "Training at Epoch 24 iteration 400 with loss 1.15318. Total time 1.52666 hours\n",
            "Validation at Epoch 24 , MSE: 1.52503 , Pearson Correlation: -0.0044 with p-value: 0.69723 , Concordance Index: 0.49714\n",
            "Training at Epoch 25 iteration 0 with loss 1.63696. Total time 1.535 hours\n",
            "Training at Epoch 25 iteration 100 with loss 1.68938. Total time 1.54888 hours\n",
            "Training at Epoch 25 iteration 200 with loss 1.50686. Total time 1.56277 hours\n",
            "Training at Epoch 25 iteration 300 with loss 1.96091. Total time 1.57666 hours\n",
            "Training at Epoch 25 iteration 400 with loss 1.19125. Total time 1.59055 hours\n",
            "Validation at Epoch 25 , MSE: 1.45706 , Pearson Correlation: 0.01441 with p-value: 0.20291 , Concordance Index: 0.50707\n",
            "Training at Epoch 26 iteration 0 with loss 1.64243. Total time 1.59888 hours\n",
            "Training at Epoch 26 iteration 100 with loss 1.53592. Total time 1.61277 hours\n",
            "Training at Epoch 26 iteration 200 with loss 1.47620. Total time 1.62666 hours\n",
            "Training at Epoch 26 iteration 300 with loss 1.42618. Total time 1.64055 hours\n",
            "Training at Epoch 26 iteration 400 with loss 1.63392. Total time 1.65444 hours\n",
            "Validation at Epoch 26 , MSE: 1.46113 , Pearson Correlation: 0.00447 with p-value: 0.69237 , Concordance Index: 0.50613\n",
            "Training at Epoch 27 iteration 0 with loss 1.68975. Total time 1.66277 hours\n",
            "Training at Epoch 27 iteration 100 with loss 2.05027. Total time 1.67666 hours\n",
            "Training at Epoch 27 iteration 200 with loss 1.71042. Total time 1.69083 hours\n",
            "Training at Epoch 27 iteration 300 with loss 1.31040. Total time 1.70472 hours\n",
            "Training at Epoch 27 iteration 400 with loss 1.37887. Total time 1.71861 hours\n",
            "Validation at Epoch 27 , MSE: 1.46273 , Pearson Correlation: 0.00570 with p-value: 0.61445 , Concordance Index: 0.50374\n",
            "Training at Epoch 28 iteration 0 with loss 1.64936. Total time 1.72666 hours\n",
            "Training at Epoch 28 iteration 100 with loss 1.39916. Total time 1.74083 hours\n",
            "Training at Epoch 28 iteration 200 with loss 1.54742. Total time 1.75472 hours\n",
            "Training at Epoch 28 iteration 300 with loss 1.41892. Total time 1.76861 hours\n",
            "Training at Epoch 28 iteration 400 with loss 1.82904. Total time 1.7825 hours\n",
            "Validation at Epoch 28 , MSE: 1.45777 , Pearson Correlation: 0.00818 with p-value: 0.46940 , Concordance Index: 0.50579\n",
            "Training at Epoch 29 iteration 0 with loss 1.41474. Total time 1.79083 hours\n",
            "Training at Epoch 29 iteration 100 with loss 0.94473. Total time 1.80472 hours\n",
            "Training at Epoch 29 iteration 200 with loss 1.75280. Total time 1.81861 hours\n",
            "Training at Epoch 29 iteration 300 with loss 1.66815. Total time 1.8325 hours\n",
            "Training at Epoch 29 iteration 400 with loss 1.35744. Total time 1.84638 hours\n",
            "Validation at Epoch 29 , MSE: 1.51200 , Pearson Correlation: -0.0033 with p-value: 0.76868 , Concordance Index: 0.49835\n",
            "Training at Epoch 30 iteration 0 with loss 1.44470. Total time 1.85472 hours\n",
            "Training at Epoch 30 iteration 100 with loss 1.31637. Total time 1.86861 hours\n",
            "Training at Epoch 30 iteration 200 with loss 1.78956. Total time 1.8825 hours\n",
            "Training at Epoch 30 iteration 300 with loss 1.38759. Total time 1.89666 hours\n",
            "Training at Epoch 30 iteration 400 with loss 1.31758. Total time 1.91055 hours\n",
            "Validation at Epoch 30 , MSE: 1.49956 , Pearson Correlation: 0.00979 with p-value: 0.38702 , Concordance Index: 0.50642\n",
            "Training at Epoch 31 iteration 0 with loss 1.58624. Total time 1.91888 hours\n",
            "Training at Epoch 31 iteration 100 with loss 2.07608. Total time 1.93277 hours\n",
            "Training at Epoch 31 iteration 200 with loss 1.67925. Total time 1.94666 hours\n",
            "Training at Epoch 31 iteration 300 with loss 1.39423. Total time 1.96055 hours\n",
            "Training at Epoch 31 iteration 400 with loss 1.40007. Total time 1.97472 hours\n",
            "Validation at Epoch 31 , MSE: 1.49781 , Pearson Correlation: 0.01041 with p-value: 0.35746 , Concordance Index: 0.50403\n",
            "Training at Epoch 32 iteration 0 with loss 1.27160. Total time 1.98277 hours\n",
            "Training at Epoch 32 iteration 100 with loss 1.83297. Total time 1.99666 hours\n",
            "Training at Epoch 32 iteration 200 with loss 1.21716. Total time 2.01055 hours\n",
            "Training at Epoch 32 iteration 300 with loss 1.62834. Total time 2.02444 hours\n",
            "Training at Epoch 32 iteration 400 with loss 1.47653. Total time 2.03861 hours\n",
            "Validation at Epoch 32 , MSE: 1.49981 , Pearson Correlation: 0.01175 with p-value: 0.29888 , Concordance Index: 0.50783\n",
            "Training at Epoch 33 iteration 0 with loss 1.43042. Total time 2.04694 hours\n",
            "Training at Epoch 33 iteration 100 with loss 1.37021. Total time 2.06083 hours\n",
            "Training at Epoch 33 iteration 200 with loss 1.67843. Total time 2.07472 hours\n",
            "Training at Epoch 33 iteration 300 with loss 1.34754. Total time 2.08888 hours\n",
            "Training at Epoch 33 iteration 400 with loss 1.24890. Total time 2.10277 hours\n",
            "Validation at Epoch 33 , MSE: 1.56862 , Pearson Correlation: 0.00310 with p-value: 0.78369 , Concordance Index: 0.50498\n",
            "Training at Epoch 34 iteration 0 with loss 1.79442. Total time 2.11111 hours\n",
            "Training at Epoch 34 iteration 100 with loss 1.04393. Total time 2.12527 hours\n",
            "Training at Epoch 34 iteration 200 with loss 1.69559. Total time 2.13944 hours\n",
            "Training at Epoch 34 iteration 300 with loss 1.51295. Total time 2.15333 hours\n",
            "Training at Epoch 34 iteration 400 with loss 1.13860. Total time 2.1675 hours\n",
            "Validation at Epoch 34 , MSE: 1.50409 , Pearson Correlation: 0.01650 with p-value: 0.14465 , Concordance Index: 0.51098\n",
            "Training at Epoch 35 iteration 0 with loss 1.26836. Total time 2.17583 hours\n",
            "Training at Epoch 35 iteration 100 with loss 1.49804. Total time 2.18972 hours\n",
            "Training at Epoch 35 iteration 200 with loss 1.23696. Total time 2.20388 hours\n",
            "Training at Epoch 35 iteration 300 with loss 1.35703. Total time 2.21777 hours\n",
            "Training at Epoch 35 iteration 400 with loss 1.93938. Total time 2.23194 hours\n",
            "Validation at Epoch 35 , MSE: 1.62011 , Pearson Correlation: 0.01185 with p-value: 0.29482 , Concordance Index: 0.50951\n",
            "Training at Epoch 36 iteration 0 with loss 1.08602. Total time 2.24027 hours\n",
            "Training at Epoch 36 iteration 100 with loss 1.28352. Total time 2.25416 hours\n",
            "Training at Epoch 36 iteration 200 with loss 1.68093. Total time 2.26833 hours\n",
            "Training at Epoch 36 iteration 300 with loss 1.49500. Total time 2.28222 hours\n",
            "Training at Epoch 36 iteration 400 with loss 1.24055. Total time 2.29638 hours\n",
            "Validation at Epoch 36 , MSE: 1.49583 , Pearson Correlation: 0.01416 with p-value: 0.21059 , Concordance Index: 0.50581\n",
            "Training at Epoch 37 iteration 0 with loss 1.33844. Total time 2.30472 hours\n",
            "Training at Epoch 37 iteration 100 with loss 1.75816. Total time 2.31861 hours\n",
            "Training at Epoch 37 iteration 200 with loss 1.20178. Total time 2.33277 hours\n",
            "Training at Epoch 37 iteration 300 with loss 1.85613. Total time 2.34666 hours\n",
            "Training at Epoch 37 iteration 400 with loss 1.48068. Total time 2.36083 hours\n",
            "Validation at Epoch 37 , MSE: 1.46414 , Pearson Correlation: 0.01131 with p-value: 0.31757 , Concordance Index: 0.50412\n",
            "Training at Epoch 38 iteration 0 with loss 1.44961. Total time 2.36916 hours\n",
            "Training at Epoch 38 iteration 100 with loss 1.30197. Total time 2.38305 hours\n",
            "Training at Epoch 38 iteration 200 with loss 1.00653. Total time 2.39722 hours\n",
            "Training at Epoch 38 iteration 300 with loss 2.13045. Total time 2.41138 hours\n",
            "Training at Epoch 38 iteration 400 with loss 1.42165. Total time 2.42555 hours\n",
            "Validation at Epoch 38 , MSE: 1.57265 , Pearson Correlation: 0.01425 with p-value: 0.20772 , Concordance Index: 0.50530\n",
            "Training at Epoch 39 iteration 0 with loss 1.76183. Total time 2.43388 hours\n",
            "Training at Epoch 39 iteration 100 with loss 1.19977. Total time 2.44777 hours\n",
            "Training at Epoch 39 iteration 200 with loss 1.76092. Total time 2.46194 hours\n",
            "Training at Epoch 39 iteration 300 with loss 1.37476. Total time 2.47611 hours\n",
            "Training at Epoch 39 iteration 400 with loss 1.53154. Total time 2.49 hours\n",
            "Validation at Epoch 39 , MSE: 1.48001 , Pearson Correlation: 0.00440 with p-value: 0.69701 , Concordance Index: 0.50154\n",
            "Training at Epoch 40 iteration 0 with loss 1.53537. Total time 2.49833 hours\n",
            "Training at Epoch 40 iteration 100 with loss 1.39358. Total time 2.5125 hours\n",
            "Training at Epoch 40 iteration 200 with loss 1.21375. Total time 2.52666 hours\n",
            "Training at Epoch 40 iteration 300 with loss 1.41187. Total time 2.54083 hours\n",
            "Training at Epoch 40 iteration 400 with loss 1.62404. Total time 2.555 hours\n",
            "Validation at Epoch 40 , MSE: 1.48920 , Pearson Correlation: -0.0031 with p-value: 0.78167 , Concordance Index: 0.50068\n",
            "--- Go for Testing ---\n",
            "Testing MSE: 1.4510069880934893 , Pearson Correlation: -0.0181725082535966 with p-value: 0.10587239467932323 , Concordance Index: 0.49593615457503115\n",
            "--- Training Finished ---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N5EIq6XywyL"
      },
      "source": [
        "pickle_rick.dump(model, open(models_folder + \"model_\" + str(random_seed) + \".pkl\", \"wb\"))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j83plo7QpNpC"
      },
      "source": [
        "# [Delete me] This should be your last code cell. Make sure to change the filename to your group id!\n",
        "pickle_rick.dump(final_model, open(models_folder + \"G35_model.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPpo_Mj-mPyc"
      },
      "source": [
        "\n",
        "---\n",
        "# That's all folks!"
      ]
    }
  ]
}